question,answer,retrieved_docs,retrieved_titles,retrieved_terms,similarities
What does deep reinforcement learning combine?,model-free and model-based deep reinforcement learning algorithms,"['Deep reinforcement learning enables algorithms to learn complex behavior,\ndeal with continuous action spaces and find good strategies in environments\nwith high dimensional state spaces. With deep reinforcement learning being an\nactive area of research and many concurrent inventions, we decided to focus on\na relatively simple robotic task to evaluate a set of ideas that might help to\nsolve recent reinforcement learning problems. We test a newly created\ncombination of two commonly used reinforcement learning methods, whether it is\nable to learn more effectively than a baseline. We also compare different ideas\nto preprocess information before it is fed to the reinforcement learning\nalgorithm. The goal of this strategy is to reduce training time and eventually\nhelp the algorithm to converge. The concluding evaluation proves the general\napplicability of the described concepts by testing them using a simulated\nenvironment. These concepts might be reused for future experiments.', 'Learning robust value functions given raw observations and rewards is now\npossible with model-free and model-based deep reinforcement learning\nalgorithms. There is a third alternative, called Successor Representations\n(SR), which decomposes the value function into two components -- a reward\npredictor and a successor map. The successor map represents the expected future\nstate occupancy from any given state and the reward predictor maps states to\nscalar rewards. The value function of a state can be computed as the inner\nproduct between the successor map and the reward weights. In this paper, we\npresent DSR, which generalizes SR within an end-to-end deep reinforcement\nlearning framework. DSR has several appealing properties including: increased\nsensitivity to distal reward changes due to factorization of reward and world\ndynamics, and the ability to extract bottleneck states (subgoals) given\nsuccessor maps trained under a random policy. We show the efficacy of our\napproach on two diverse environments given raw pixel observations -- simple\ngrid-world domains (MazeBase) and the Doom game engine.', ""The ability to transfer skills across tasks has the potential to scale up\nreinforcement learning (RL) agents to environments currently out of reach.\nRecently, a framework based on two ideas, successor features (SFs) and\ngeneralised policy improvement (GPI), has been introduced as a principled way\nof transferring skills. In this paper we extend the SFs & GPI framework in two\nways. One of the basic assumptions underlying the original formulation of SFs &\nGPI is that rewards for all tasks of interest can be computed as linear\ncombinations of a fixed set of features. We relax this constraint and show that\nthe theoretical guarantees supporting the framework can be extended to any set\nof tasks that only differ in the reward function. Our second contribution is to\nshow that one can use the reward functions themselves as features for future\ntasks, without any loss of expressiveness, thus removing the need to specify a\nset of features beforehand. This makes it possible to combine SFs & GPI with\ndeep learning in a more stable way. We empirically verify this claim on a\ncomplex 3D environment where observations are images from a first-person\nperspective. We show that the transfer promoted by SFs & GPI leads to very good\npolicies on unseen tasks almost instantaneously. We also describe how to learn\npolicies specialised to the new tasks in a way that allows them to be added to\nthe agent's set of skills, and thus be reused in the future.""]","['Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms', 'Deep Successor Reinforcement Learning', 'Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement']","[""['cs.LG', 'cs.RO', 'stat.ML']"", ""['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE']"", ""['cs.LG', 'cs.AI']""]","[0.883836567401886, 0.8750433921813965, 0.8743283152580261]"
What is the main focus of the research in this paper regarding Blockchain technologies?,Reverse Vending Machines,"['This work proposes DeepFolio, a new model for deep portfolio management based\non data from limit order books (LOB). DeepFolio solves problems found in the\nstate-of-the-art for LOB data to predict price movements. Our evaluation\nconsists of two scenarios using a large dataset of millions of time series. The\nimprovements deliver superior results both in cases of abundant as well as\nscarce data. The experiments show that DeepFolio outperforms the\nstate-of-the-art on the benchmark FI-2010 LOB. Further, we use DeepFolio for\noptimal portfolio allocation of crypto-assets with rebalancing. For this\npurpose, we use two loss-functions - Sharpe ratio loss and minimum volatility\nrisk. We show that DeepFolio outperforms widely used portfolio allocation\ntechniques in the literature.', 'Reverse Vending Machines (RVMs) are a proven instrument for facilitating\nclosed-loop plastic packaging recycling. A good customer experience at the RVM\nis crucial for a further proliferation of this technology. Bin full events are\nthe major reason for Reverse Vending Machine (RVM) downtime at the world leader\nin the RVM market. The paper at hand develops and evaluates an approach based\non machine learning and statistical approximation to foresee bin full events\nand, thus increase uptime of RVMs. Our approach relies on forecasting the\nhourly time series of returned beverage containers at a given RVM. We\ncontribute by developing and evaluating an approach for hourly forecasts in a\nretail setting - this combination of application domain and forecast\ngranularity is novel. A trace-driven simulation confirms that the\nforecasting-based approach leads to less downtime and costs than naive emptying\nstrategies.', ""Advances in renewable energy generation and introduction of the government\ntargets to improve energy efficiency gave rise to a concept of a Zero Energy\nBuilding (ZEB). A ZEB is a building whose net energy usage over a year is zero,\ni.e., its energy use is not larger than its overall renewables generation. A\ncollection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses\nthe problem of energy sharing in such a community. This is different from\npreviously addressed energy sharing between buildings as our focus is on the\nimprovement of community energy status, while traditionally research focused on\nreducing losses due to transmission and storage, or achieving economic gains.\nWe model this problem in a multi-agent environment and propose a Deep\nReinforcement Learning (DRL) based solution. Each building is represented by an\nintelligent agent that learns over time the appropriate behaviour to share\nenergy. We have evaluated the proposed solution in a multi-agent simulation\nbuilt using osBrain. Results indicate that with time agents learn to\ncollaborate and learn a policy comparable to the optimal policy, which in turn\nimproves the ZEC's energy status. Buildings with no renewables preferred to\nrequest energy from their neighbours rather than from the supply grid.""]","['DeepFolio: Convolutional Neural Networks for Portfolios with Limit Order Book Data', 'Half-empty or half-full? A Hybrid Approach to Predict Recycling Behavior of Consumers to Increase Reverse Vending Machine Uptime', 'Multi-agent Deep Reinforcement Learning for Zero Energy Communities']","[""['cs.LG', 'q-fin.TR', 'stat.AP', 'stat.ML']"", ""['cs.LG', 'stat.AP', 'stat.ML']"", ""['cs.LG', 'cs.AI', 'cs.MA', 'stat.ML', '97R40', 'I.2.11; I.2.6']""]","[0.8250531554222107, 0.8182892799377441, 0.8095580339431763]"
What does pose information represent in Capsule Networks?,symmetry,"['Autonomous part assembly is a challenging yet crucial task in 3D computer\nvision and robotics. Analogous to buying an IKEA furniture, given a set of 3D\nparts that can assemble a single shape, an intelligent agent needs to perceive\nthe 3D part geometry, reason to propose pose estimations for the input parts,\nand finally call robotic planning and control routines for actuation. In this\npaper, we focus on the pose estimation subproblem from the vision side\ninvolving geometric and relational reasoning over the input part geometry.\nEssentially, the task of generative 3D part assembly is to predict a 6-DoF part\npose, including a rigid rotation and translation, for each input part that\nassembles a single 3D shape as the final output. To tackle this problem, we\npropose an assembly-oriented dynamic graph learning framework that leverages an\niterative graph neural network as a backbone. It explicitly conducts sequential\npart assembly refinements in a coarse-to-fine manner, exploits a pair of part\nrelation reasoning module and part aggregation module for dynamically adjusting\nboth part features and their relations in the part graph. We conduct extensive\nexperiments and quantitative comparisons to three strong baseline methods,\ndemonstrating the effectiveness of the proposed approach.', 'The task of estimating the 6D pose of an object from RGB images can be broken\ndown into two main steps: an initial pose estimation step, followed by a\nrefinement procedure to correctly register the object and its observation. In\nthis paper, we propose a new method for 6D pose estimation refinement from RGB\nimages. To achieve high accuracy of the final estimate, the observation and a\nrendered model need to be aligned. Our main insight is that after the initial\npose estimate, it is important to pay attention to distinct spatial features of\nthe object in order to improve the estimation accuracy during alignment.\nFurthermore, parts of the object that are occluded in the image should be given\nless weight during the alignment process. Most state-of-the-art refinement\napproaches do not allow for this fine-grained reasoning and can not fully\nleverage the structure of the problem. In contrast, we propose a novel neural\nnetwork architecture built around a spatial attention mechanism that identifies\nand leverages information about spatial details during pose refinement. We\nexperimentally show that this approach learns to attend to salient spatial\nfeatures and learns to ignore occluded parts of the object, leading to better\npose estimation across datasets. We conduct experiments on standard benchmark\ndatasets for 6D pose estimation (LineMOD and Occlusion LineMOD) and outperform\nprevious state-of-the-art methods.', 'Autonomous assembly of objects is an essential task in robotics and 3D\ncomputer vision. It has been studied extensively in robotics as a problem of\nmotion planning, actuator control and obstacle avoidance. However, the task of\ndeveloping a generalized framework for assembly robust to structural variants\nremains relatively unexplored. In this work, we tackle this problem using a\nrecurrent graph learning framework considering inter-part relations and the\nprogressive update of the part pose. Our network can learn more plausible\npredictions of shape structure by accounting for priorly assembled parts.\nCompared to the current state-of-the-art, our network yields up to 10%\nimprovement in part accuracy and up to 15% improvement in connectivity accuracy\non the PartNet dataset. Moreover, our resulting latent space facilitates\nexciting applications such as shape recovery from the point-cloud components.\nWe conduct extensive experiments to justify our design choices and demonstrate\nthe effectiveness of the proposed framework.']","['Generative 3D Part Assembly via Dynamic Graph Learning', 'Spatial Attention Improves Iterative 6D Object Pose Estimation', 'RGL-NET: A Recurrent Graph Learning framework for Progressive Part Assembly']","[""['cs.CV']"", ""['cs.CV']"", ""['cs.CV']""]","[0.8771268129348755, 0.8765555620193481, 0.8759101629257202]"
What are the key components of VA-GAN?,generative adversarial networks,"['Evaluating generative adversarial networks (GANs) is inherently challenging.\nIn this paper, we revisit several representative sample-based evaluation\nmetrics for GANs, and address the problem of how to evaluate the evaluation\nmetrics. We start with a few necessary conditions for metrics to produce\nmeaningful scores, such as distinguishing real from generated samples,\nidentifying mode dropping and mode collapsing, and detecting overfitting. With\na series of carefully designed experiments, we comprehensively investigate\nexisting sample-based metrics and identify their strengths and limitations in\npractical settings. Based on these results, we observe that kernel Maximum Mean\nDiscrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to\nsatisfy most of the desirable properties, provided that the distances between\nsamples are computed in a suitable feature space. Our experiments also unveil\ninteresting properties about the behavior of several popular GAN models, such\nas whether they are memorizing training samples, and how far they are from\nlearning the target distribution.', 'One of the biggest challenges in the research of generative adversarial\nnetworks (GANs) is assessing the quality of generated samples and detecting\nvarious levels of mode collapse. In this work, we construct a novel measure of\nperformance of a GAN by comparing geometrical properties of the underlying data\nmanifold and the generated one, which provides both qualitative and\nquantitative means for evaluation. Our algorithm can be applied to datasets of\nan arbitrary nature and is not limited to visual data. We test the obtained\nmetric on various real-life models and datasets and demonstrate that our method\nprovides new insights into properties of GANs.', 'Inferring the latent variable generating a given test sample is a challenging\nproblem in Generative Adversarial Networks (GANs). In this paper, we propose\nInvGAN - a novel framework for solving the inference problem in GANs, which\ninvolves training an encoder network capable of inverting a pre-trained\ngenerator network without access to any training data. Under mild assumptions,\nwe theoretically show that using InvGAN, we can approximately invert the\ngenerations of any latent code of a trained GAN model. Furthermore, we\nempirically demonstrate the superiority of our inference scheme by quantitative\nand qualitative comparisons with other methods that perform a similar task. We\nalso show the effectiveness of our framework in the problem of adversarial\ndefenses where InvGAN can successfully be used as a projection-based defense\nmechanism. Additionally, we show how InvGAN can be used to implement\nreparameterization white-box attacks on projection-based defense mechanisms.\nExperimental validation on several benchmark datasets demonstrate the efficacy\nof our method in achieving improved performance on several white-box and\nblack-box attacks. Our code is available at\nhttps://github.com/yogeshbalaji/InvGAN.']","['An empirical study on evaluation metrics of generative adversarial networks', 'Geometry Score: A Method For Comparing Generative Adversarial Networks', 'Invert and Defend: Model-based Approximate Inversion of Generative Adversarial Networks for Secure Inference']","[""['cs.LG', 'cs.CV', 'stat.ML']"", ""['cs.LG', 'cs.CG', 'stat.ML']"", ""['cs.LG', 'cs.CV', 'stat.ML']""]","[0.8546628952026367, 0.8514755368232727, 0.8501787781715393]"
How does the paper contribute to understanding the network dynamics behind cryptocurrencies like Bitcoin?,network,"['This work proposes DeepFolio, a new model for deep portfolio management based\non data from limit order books (LOB). DeepFolio solves problems found in the\nstate-of-the-art for LOB data to predict price movements. Our evaluation\nconsists of two scenarios using a large dataset of millions of time series. The\nimprovements deliver superior results both in cases of abundant as well as\nscarce data. The experiments show that DeepFolio outperforms the\nstate-of-the-art on the benchmark FI-2010 LOB. Further, we use DeepFolio for\noptimal portfolio allocation of crypto-assets with rebalancing. For this\npurpose, we use two loss-functions - Sharpe ratio loss and minimum volatility\nrisk. We show that DeepFolio outperforms widely used portfolio allocation\ntechniques in the literature.', ""Several social, medical, engineering and biological challenges rely on\ndiscovering the functionality of networks from their structure and node\nmetadata, when it is available. For example, in chemoinformatics one might want\nto detect whether a molecule is toxic based on structure and atomic types, or\ndiscover the research field of a scientific collaboration network. Existing\ntechniques rely on counting or measuring structural patterns that are known to\nshow large variations from network to network, such as the number of triangles,\nor the assortativity of node metadata. We introduce the concept of multi-hop\nassortativity, that captures the similarity of the nodes situated at the\nextremities of a randomly selected path of a given length. We show that\nmulti-hop assortativity unifies various existing concepts and offers a\nversatile family of 'fingerprints' to characterize networks. These fingerprints\nallow in turn to recover the functionalities of a network, with the help of the\nmachine learning toolbox. Our method is evaluated empirically on established\nsocial and chemoinformatic network benchmarks. Results reveal that our\nassortativity based features are competitive providing highly accurate results\noften outperforming state of the art methods for the network classification\ntask."", 'This work derives closed-form expressions computing the expectation of\nco-presence and of number of co-occurrences of nodes on paths sampled from a\nnetwork according to general path weights (a bag of paths). The underlying idea\nis that two nodes are considered as similar when they often appear together on\n(preferably short) paths of the network. The different expressions are obtained\nfor both regular and hitting paths and serve as a basis for computing new\ncovariance and correlation measures between nodes, which are valid positive\nsemi-definite kernels on a graph. Experiments on semi-supervised classification\nproblems show that the introduced similarity measures provide competitive\nresults compared to other state-of-the-art distance and similarity measures\nbetween nodes.']","['DeepFolio: Convolutional Neural Networks for Portfolios with Limit Order Book Data', 'Multi-hop assortativities for networks classification', 'Covariance and Correlation Kernels on a Graph in the Generalized Bag-of-Paths Formalism']","[""['cs.LG', 'q-fin.TR', 'stat.AP', 'stat.ML']"", ""['cs.LG', 'cs.SI', 'stat.ML']"", ""['cs.LG', 'stat.ML']""]","[0.8535720705986023, 0.842045783996582, 0.8328179121017456]"
What specific image processing algorithms were used to transform RNA sequences into gene motifs for the CNN classification?,a multi-task learning method,"['Fully supervised deep neural networks for segmentation usually require a\nmassive amount of pixel-level labels which are manually expensive to create. In\nthis work, we develop a multi-task learning method to relax this constraint. We\nregard the segmentation problem as a sequence of approximation subproblems that\nare recursively defined and in increasing levels of approximation accuracy. The\nsubproblems are handled by a framework that consists of 1) a segmentation task\nthat learns from pixel-level ground truth segmentation masks of a small\nfraction of the images, 2) a recursive approximation task that conducts partial\nobject regions learning and data-driven mask evolution starting from partial\nmasks of each object instance, and 3) other problem oriented auxiliary tasks\nthat are trained with sparse annotations and promote the learning of dedicated\nfeatures. Most training images are only labeled by (rough) partial masks, which\ndo not contain exact object boundaries, rather than by their full segmentation\nmasks. During the training phase, the approximation task learns the statistics\nof these partial masks, and the partial regions are recursively increased\ntowards object boundaries aided by the learned information from the\nsegmentation task in a fully data-driven fashion. The network is trained on an\nextremely small amount of precisely segmented images and a large set of coarse\nlabels. Annotations can thus be obtained in a cheap way. We demonstrate the\nefficiency of our approach in three applications with microscopy images and\nultrasound images.', 'We introduce language-driven image generation, the task of generating an\nimage visualizing the semantic contents of a word embedding, e.g., given the\nword embedding of grasshopper, we generate a natural image of a grasshopper. We\nimplement a simple method based on two mapping functions. The first takes as\ninput a word embedding (as produced, e.g., by the word2vec toolkit) and maps it\nonto a high-level visual space (e.g., the space defined by one of the top\nlayers of a Convolutional Neural Network). The second function maps this\nabstract visual representation to pixel space, in order to generate the target\nimage. Several user studies suggest that the current system produces images\nthat capture general visual properties of the concepts encoded in the word\nembedding, such as color or typical environment, and are sufficient to\ndiscriminate between general categories of objects.', 'Here we introduce a new model of natural textures based on the feature spaces\nof convolutional neural networks optimised for object recognition. Samples from\nthe model are of high perceptual quality demonstrating the generative power of\nneural networks trained in a purely discriminative fashion. Within the model,\ntextures are represented by the correlations between feature maps in several\nlayers of the network. We show that across layers the texture representations\nincreasingly capture the statistical properties of natural images while making\nobject information more and more explicit. The model provides a new tool to\ngenerate stimuli for neuroscience and might offer insights into the deep\nrepresentations learned by convolutional neural networks.']","['Multi-task deep learning for image segmentation using recursive approximation tasks', 'Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation', 'Texture Synthesis Using Convolutional Neural Networks']","[""['cs.CV', 'cs.LG', 'eess.IV']"", ""['cs.CV', 'cs.CL']"", ""['cs.CV', 'cs.NE', 'q-bio.NC']""]","[0.8818787336349487, 0.8809809684753418, 0.8807879686355591]"
What is the role of camera-aware domain adaptation in the proposed method?,uncertainty-aware domain adaptation network,"['Although various image-based domain adaptation (DA) techniques have been\nproposed in recent years, domain shift in videos is still not well-explored.\nMost previous works only evaluate performance on small-scale datasets which are\nsaturated. Therefore, we first propose a larger-scale dataset with larger\ndomain discrepancy: UCF-HMDB_full. Second, we investigate different DA\nintegration methods for videos, and show that simultaneously aligning and\nlearning temporal dynamics achieves effective alignment even without\nsophisticated DA methods. Finally, we propose Temporal Attentive Adversarial\nAdaptation Network (TA3N), which explicitly attends to the temporal dynamics\nusing domain discrepancy for more effective domain alignment, achieving\nstate-of-the-art performance on three video DA datasets. The code and data are\nreleased at http://github.com/cmhungsteve/TA3N.', 'Unsupervised domain adaptive object detection aims to adapt detectors from a\nlabelled source domain to an unlabelled target domain. Most existing works take\na two-stage strategy that first generates region proposals and then detects\nobjects of interest, where adversarial learning is widely adopted to mitigate\nthe inter-domain discrepancy in both stages. However, adversarial learning may\nimpair the alignment of well-aligned samples as it merely aligns the global\ndistributions across domains. To address this issue, we design an\nuncertainty-aware domain adaptation network (UaDAN) that introduces conditional\nadversarial learning to align well-aligned and poorly-aligned samples\nseparately in different manners. Specifically, we design an uncertainty metric\nthat assesses the alignment of each sample and adjusts the strength of\nadversarial learning for well-aligned and poorly-aligned samples adaptively. In\naddition, we exploit the uncertainty metric to achieve curriculum learning that\nfirst performs easier image-level alignment and then more difficult\ninstance-level alignment progressively. Extensive experiments over four\nchallenging domain adaptive object detection datasets show that UaDAN achieves\nsuperior performance as compared with state-of-the-art methods.', 'Deep networks have been successfully applied to learn transferable features\nfor adapting models from a source domain to a different target domain. In this\npaper, we present joint adaptation networks (JAN), which learn a transfer\nnetwork by aligning the joint distributions of multiple domain-specific layers\nacross domains based on a joint maximum mean discrepancy (JMMD) criterion.\nAdversarial training strategy is adopted to maximize JMMD such that the\ndistributions of the source and target domains are made more distinguishable.\nLearning can be performed by stochastic gradient descent with the gradients\ncomputed by back-propagation in linear-time. Experiments testify that our model\nyields state of the art results on standard datasets.']","['Temporal Attentive Alignment for Video Domain Adaptation', 'Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection', 'Deep Transfer Learning with Joint Adaptation Networks']","[""['cs.CV', 'cs.LG', 'cs.MM']"", ""['cs.CV']"", ""['cs.LG', 'stat.ML']""]","[0.9129963517189026, 0.9045946598052979, 0.9000077247619629]"
What is the primary goal of the Structure Transfer Machine (STM) method?,bin full events,"['Autonomous assembly of objects is an essential task in robotics and 3D\ncomputer vision. It has been studied extensively in robotics as a problem of\nmotion planning, actuator control and obstacle avoidance. However, the task of\ndeveloping a generalized framework for assembly robust to structural variants\nremains relatively unexplored. In this work, we tackle this problem using a\nrecurrent graph learning framework considering inter-part relations and the\nprogressive update of the part pose. Our network can learn more plausible\npredictions of shape structure by accounting for priorly assembled parts.\nCompared to the current state-of-the-art, our network yields up to 10%\nimprovement in part accuracy and up to 15% improvement in connectivity accuracy\non the PartNet dataset. Moreover, our resulting latent space facilitates\nexciting applications such as shape recovery from the point-cloud components.\nWe conduct extensive experiments to justify our design choices and demonstrate\nthe effectiveness of the proposed framework.', 'Reverse Vending Machines (RVMs) are a proven instrument for facilitating\nclosed-loop plastic packaging recycling. A good customer experience at the RVM\nis crucial for a further proliferation of this technology. Bin full events are\nthe major reason for Reverse Vending Machine (RVM) downtime at the world leader\nin the RVM market. The paper at hand develops and evaluates an approach based\non machine learning and statistical approximation to foresee bin full events\nand, thus increase uptime of RVMs. Our approach relies on forecasting the\nhourly time series of returned beverage containers at a given RVM. We\ncontribute by developing and evaluating an approach for hourly forecasts in a\nretail setting - this combination of application domain and forecast\ngranularity is novel. A trace-driven simulation confirms that the\nforecasting-based approach leads to less downtime and costs than naive emptying\nstrategies.', ""In this work, we study the transfer learning problem under high-dimensional\ngeneralized linear models (GLMs), which aim to improve the fit on target data\nby borrowing information from useful source data. Given which sources to\ntransfer, we propose an oracle algorithm and derive its $\\ell_2$-estimation\nerror bounds. The theoretical analysis shows that under certain conditions,\nwhen the target and source are sufficiently close to each other, the estimation\nerror bound could be improved over that of the classical penalized estimator\nusing only target data. When we don't know which sources to transfer, an\nalgorithm-free transferable source detection approach is introduced to detect\ninformative sources. The detection consistency is proved under the\nhigh-dimensional GLM transfer learning setting. Extensive simulations and a\nreal-data experiment verify the effectiveness of our algorithms.""]","['RGL-NET: A Recurrent Graph Learning framework for Progressive Part Assembly', 'Half-empty or half-full? A Hybrid Approach to Predict Recycling Behavior of Consumers to Increase Reverse Vending Machine Uptime', 'Transfer Learning under High-dimensional Generalized Linear Models']","[""['cs.CV']"", ""['cs.LG', 'stat.AP', 'stat.ML']"", ""['stat.ML', 'cs.LG', 'stat.ME']""]","[0.8291782140731812, 0.828155517578125, 0.8259705305099487]"
What is the role of the manifold structure in STM?,tangent plane,"['Density estimation is an important technique for characterizing distributions\ngiven observations. Much existing research on density estimation has focused on\ncases wherein the data lies in a Euclidean space. However, some kinds of data\nare not well-modeled by supposing that their underlying geometry is Euclidean.\nInstead, it can be useful to model such data as lying on a {\\it manifold} with\nsome known structure. For instance, some kinds of data may be known to lie on\nthe surface of a sphere. We study the problem of estimating densities on\nmanifolds. We propose a method, inspired by the literature on ""dequantization,""\nwhich we interpret through the lens of a coordinate transformation of an\nambient Euclidean space and a smooth manifold of interest. Using methods from\nnormalizing flows, we apply this method to the dequantization of smooth\nmanifold structures in order to model densities on the sphere, tori, and the\northogonal group.', 'We take the novel perspective to view data not as a probability distribution\nbut rather as a current. Primarily studied in the field of geometric measure\ntheory, $k$-currents are continuous linear functionals acting on compactly\nsupported smooth differential forms and can be understood as a generalized\nnotion of oriented $k$-dimensional manifold. By moving from distributions\n(which are $0$-currents) to $k$-currents, we can explicitly orient the data by\nattaching a $k$-dimensional tangent plane to each sample point. Based on the\nflat metric which is a fundamental distance between currents, we derive\nFlatGAN, a formulation in the spirit of generative adversarial networks but\ngeneralized to $k$-currents. In our theoretical contribution we prove that the\nflat metric between a parametrized current and a reference current is Lipschitz\ncontinuous in the parameters. In experiments, we show that the proposed shift\nto $k>0$ leads to interpretable and disentangled latent representations which\nbehave equivariantly to the specified oriented tangent planes.', 'We introduce an approach based on the Givens representation for posterior\ninference in statistical models with orthogonal matrix parameters, such as\nfactor models and probabilistic principal component analysis (PPCA). We show\nhow the Givens representation can be used to develop practical methods for\ntransforming densities over the Stiefel manifold into densities over subsets of\nEuclidean space. We show how to deal with issues arising from the topology of\nthe Stiefel manifold and how to inexpensively compute the change-of-measure\nterms. We introduce an auxiliary parameter approach that limits the impact of\ntopological issues. We provide both analysis of our methods and numerical\nexamples demonstrating the effectiveness of the approach. We also discuss how\nour Givens representation can be used to define general classes of\ndistributions over the space of orthogonal matrices. We then give\ndemonstrations on several examples showing how the Givens approach performs in\npractice in comparison with other methods.']","['Manifold Density Estimation via Generalized Dequantization', 'Flat Metric Minimization with Applications in Generative Modeling', 'Bayesian Inference over the Stiefel Manifold via the Givens Representation']","[""['stat.ML', 'cs.LG']"", ""['cs.LG', 'cs.CV', 'stat.ML']"", ""['stat.ML']""]","[0.8662106990814209, 0.8611139059066772, 0.8600803017616272]"
Where can the source code for STM be accessed?,a neural program synthesizer,"[""Program synthesis of general-purpose source code from natural language\nspecifications is challenging due to the need to reason about high-level\npatterns in the target program and low-level implementation details at the same\ntime. In this work, we present PATOIS, a system that allows a neural program\nsynthesizer to explicitly interleave high-level and low-level reasoning at\nevery generation step. It accomplishes this by automatically mining common code\nidioms from a given corpus, incorporating them into the underlying language for\nneural synthesis, and training a tree-based neural synthesizer to use these\nidioms during code generation. We evaluate PATOIS on two complex semantic\nparsing datasets and show that using learned code idioms improves the\nsynthesizer's accuracy."", 'We present basic notions of Gold\'s ""learnability in the limit"" paradigm,\nfirst presented in 1967, a formalization of the cognitive process by which a\nnative speaker gets to grasp the underlying grammar of his/her own native\nlanguage by being exposed to well formed sentences generated by that grammar.\nThen we present Lambek grammars, a formalism issued from categorial grammars\nwhich, although not as expressive as needed for a full formalization of natural\nlanguages, is particularly suited to easily implement a natural interface\nbetween syntax and semantics. In the last part of this work, we present a\nlearnability result for Rigid Lambek grammars from structured examples.', 'The success of machine learning algorithms often relies on a large amount of\nhigh-quality data to train well-performed models. However, data is a valuable\nresource and are always held by different parties in reality. An effective\nsolution to such a data isolation problem is to employ federated learning,\nwhich allows multiple parties to collaboratively train a model. In this paper,\nwe propose a Secure version of the widely used Maximum Mean Discrepancy (SMMD)\nbased on homomorphic encryption to enable effective knowledge transfer under\nthe data federation setting without compromising the data privacy. The proposed\nSMMD is able to avoid the potential information leakage in transfer learning\nwhen aligning the source and target data distribution. As a result, both the\nsource domain and target domain can fully utilize their data to build more\nscalable models. Experimental results demonstrate that our proposed SMMD is\nsecure and effective.']","['Program Synthesis and Semantic Parsing with Learned Code Idioms', 'A Study on Learnability for Rigid Lambek Grammars', 'Privacy-preserving Transfer Learning via Secure Maximum Mean Discrepancy']","[""['cs.LG', 'cs.AI', 'cs.CL', 'cs.PL', 'stat.ML']"", ""['cs.LG']"", ""['cs.LG', 'stat.ML']""]","[0.8277249336242676, 0.7881818413734436, 0.7863757610321045]"
What are the three main challenges in multi-task reinforcement learning?,Deep reinforcement learning,"['Deep reinforcement learning enables algorithms to learn complex behavior,\ndeal with continuous action spaces and find good strategies in environments\nwith high dimensional state spaces. With deep reinforcement learning being an\nactive area of research and many concurrent inventions, we decided to focus on\na relatively simple robotic task to evaluate a set of ideas that might help to\nsolve recent reinforcement learning problems. We test a newly created\ncombination of two commonly used reinforcement learning methods, whether it is\nable to learn more effectively than a baseline. We also compare different ideas\nto preprocess information before it is fed to the reinforcement learning\nalgorithm. The goal of this strategy is to reduce training time and eventually\nhelp the algorithm to converge. The concluding evaluation proves the general\napplicability of the described concepts by testing them using a simulated\nenvironment. These concepts might be reused for future experiments.', 'Despite significant progress, deep reinforcement learning (RL) suffers from\ndata-inefficiency and limited generalization. Recent efforts apply\nmeta-learning to learn a meta-learner from a set of RL tasks such that a novel\nbut related task could be solved quickly. Though specific in some ways,\ndifferent tasks in meta-RL are generally similar at a high level. However, most\nmeta-RL methods do not explicitly and adequately model the specific and shared\ninformation among different tasks, which limits their ability to learn training\ntasks and to generalize to novel tasks. In this paper, we propose to capture\nthe shared information on the one hand and meta-learn how to quickly abstract\nthe specific information about a task on the other hand. Methodologically, we\ntrain an SGD meta-learner to quickly optimize a task encoder for each task,\nwhich generates a task embedding based on past experience. Meanwhile, we learn\na policy which is shared across all tasks and conditioned on task embeddings.\nEmpirical results on four simulated tasks demonstrate that our method has\nbetter learning capacity on both training and novel tasks and attains up to 3\nto 4 times higher returns compared to baselines.', 'In standard reinforcement learning (RL), a learning agent seeks to optimize\nthe overall reward. However, many key aspects of a desired behavior are more\nnaturally expressed as constraints. For instance, the designer may want to\nlimit the use of unsafe actions, increase the diversity of trajectories to\nenable exploration, or approximate expert trajectories when rewards are sparse.\nIn this paper, we propose an algorithmic scheme that can handle a wide class of\nconstraints in RL tasks: specifically, any constraints that require expected\nvalues of some vector measurements (such as the use of an action) to lie in a\nconvex set. This captures previously studied constraints (such as safety and\nproximity to an expert), but also enables new classes of constraints (such as\ndiversity). Our approach comes with rigorous theoretical guarantees and only\nrelies on the ability to approximately solve standard RL tasks. As a result, it\ncan be easily adapted to work with any model-free or model-based RL. In our\nexperiments, we show that it matches previous algorithms that enforce safety\nvia constraints, but can also enforce new properties that these algorithms do\nnot incorporate, such as diversity.']","['Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms', 'Meta Reinforcement Learning with Task Embedding and Shared Policy', 'Reinforcement Learning with Convex Constraints']","[""['cs.LG', 'cs.RO', 'stat.ML']"", ""['cs.LG', 'cs.AI', 'stat.ML']"", ""['cs.LG', 'cs.AI', 'cs.GT', 'stat.ML']""]","[0.880189061164856, 0.8771612048149109, 0.8758977055549622]"
What is the main goal of self-supervised methods in reinforcement learning?,contextual bandit learning,"[""Contextual bandits are a common problem faced by machine learning\npractitioners in domains as diverse as hypothesis testing to product\nrecommendations. There have been a lot of approaches in exploiting rich data\nrepresentations for contextual bandit problems with varying degree of success.\nSelf-supervised learning is a promising approach to find rich data\nrepresentations without explicit labels. In a typical self-supervised learning\nscheme, the primary task is defined by the problem objective (e.g. clustering,\nclassification, embedding generation etc.) and the secondary task is defined by\nthe self-supervision objective (e.g. rotation prediction, words in\nneighborhood, colorization, etc.). In the usual self-supervision, we learn\nimplicit labels from the training data for a secondary task. However, in the\ncontextual bandit setting, we don't have the advantage of getting implicit\nlabels due to lack of data in the initial phase of learning. We provide a novel\napproach to tackle this issue by combining a contextual bandit objective with a\nself supervision objective. By augmenting contextual bandit learning with\nself-supervision we get a better cumulative reward. Our results on eight\npopular computer vision datasets show substantial gains in cumulative reward.\nWe provide cases where the proposed scheme doesn't perform optimally and give\nalternative methods for better learning in these cases."", ""We propose a generic reward shaping approach for improving the rate of\nconvergence in reinforcement learning (RL), called Self Improvement Based\nREwards, or SIBRE. The approach is designed for use in conjunction with any\nexisting RL algorithm, and consists of rewarding improvement over the agent's\nown past performance. We prove that SIBRE converges in expectation under the\nsame conditions as the original RL algorithm. The reshaped rewards help\ndiscriminate between policies when the original rewards are weakly\ndiscriminated or sparse. Experiments on several well-known benchmark\nenvironments with different RL algorithms show that SIBRE converges to the\noptimal policy faster and more stably. We also perform sensitivity analysis\nwith respect to hyper-parameters, in comparison with baseline RL algorithms."", 'We present a modular neural network architecture Main that learns algorithms\ngiven a set of input-output examples. Main consists of a neural controller that\ninteracts with a variable-length input tape and learns to compose modules\ntogether with their corresponding argument choices. Unlike previous approaches,\nMain uses a general domain-agnostic mechanism for selection of modules and\ntheir arguments. It uses a general input tape layout together with a parallel\nhistory tape to indicate most recently used locations. Finally, it uses a\nmemoryless controller with a length-invariant self-attention based input tape\nencoding to allow for random access to tape locations. The Main architecture is\ntrained end-to-end using reinforcement learning from a set of input-output\nexamples. We evaluate Main on five algorithmic tasks and show that it can learn\npolicies that generalizes perfectly to inputs of much longer lengths than the\nones used for training.']","['Self-Supervised Contextual Bandits in Computer Vision', 'SIBRE: Self Improvement Based REwards for Adaptive Feedback in Reinforcement Learning', 'Towards Modular Algorithm Induction']","[""['cs.CV', 'cs.LG', 'stat.ML']"", ""['cs.LG', 'cs.AI', 'stat.ML']"", ""['cs.LG', 'cs.AI']""]","[0.8870627284049988, 0.8858052492141724, 0.8824372291564941]"
How does the proposed Local and Global Diffusion (LGD) framework address this limitation?,We present a novel neural network architecture that learns the local and global representations in parallel.,"['Manifold learning techniques for dynamical systems and time series have shown\ntheir utility for a broad spectrum of applications in recent years. While these\nmethods are effective at learning a low-dimensional representation, they are\noften insufficient for visualizing the global and local structure of the data.\nIn this paper, we present DIG (Dynamical Information Geometry), a visualization\nmethod for multivariate time series data that extracts an information geometry\nfrom a diffusion framework. Specifically, we implement a novel group of\ndistances in the context of diffusion operators, which may be useful to reveal\nstructure in the data that may not be accessible by the commonly used diffusion\ndistances. Finally, we present a case study applying our visualization tool to\nEEG data to visualize sleep stages.', 'Convolutional Neural Networks (CNN) have been regarded as a powerful class of\nmodels for visual recognition problems. Nevertheless, the convolutional filters\nin these networks are local operations while ignoring the large-range\ndependency. Such drawback becomes even worse particularly for video\nrecognition, since video is an information-intensive media with complex\ntemporal variations. In this paper, we present a novel framework to boost the\nspatio-temporal representation learning by Local and Global Diffusion (LGD).\nSpecifically, we construct a novel neural network architecture that learns the\nlocal and global representations in parallel. The architecture is composed of\nLGD blocks, where each block updates local and global features by modeling the\ndiffusions between these two representations. Diffusions effectively interact\ntwo aspects of information, i.e., localized and holistic, for more powerful way\nof representation learning. Furthermore, a kernelized classifier is introduced\nto combine the representations from two aspects for video recognition. Our LGD\nnetworks achieve clear improvements on the large-scale Kinetics-400 and\nKinetics-600 video classification datasets against the best competitors by 3.5%\nand 0.7%. We further examine the generalization of both the global and local\nrepresentations produced by our pre-trained LGD networks on four different\nbenchmarks for video action recognition and spatio-temporal action detection\ntasks. Superior performances over several state-of-the-art techniques on these\nbenchmarks are reported. Code is available at:\nhttps://github.com/ZhaofanQiu/local-and-global-diffusion-networks.', 'We address the estimation of conditional average treatment effects (CATEs)\nwhen treatments are graph-structured (e.g., molecular graphs of drugs). Given a\nweak condition on the effect, we propose a plug-in estimator that decomposes\nCATE estimation into separate, simpler optimization problems. Our estimator (a)\nisolates the causal estimands (reducing regularization bias), and (b) allows\none to plug in arbitrary models for learning. In experiments with small-world\nand molecular graphs, we show that our approach outperforms prior approaches\nand is robust to varying selection biases. Our implementation is online.']","['Visualizing High Dimensional Dynamical Processes', 'Learning Spatio-Temporal Representation with Local and Global Diffusion', 'Graph Intervention Networks for Causal Effect Estimation']","[""['stat.ML', 'cs.LG', 'eess.SP']"", ""['cs.CV']"", ""['cs.LG', 'stat.ML']""]","[0.8832004070281982, 0.8781702518463135, 0.8705368638038635]"
What is the role of diffusions in the LGD network?,Local and Global Diffusion,"['Convolutional Neural Networks (CNN) have been regarded as a powerful class of\nmodels for visual recognition problems. Nevertheless, the convolutional filters\nin these networks are local operations while ignoring the large-range\ndependency. Such drawback becomes even worse particularly for video\nrecognition, since video is an information-intensive media with complex\ntemporal variations. In this paper, we present a novel framework to boost the\nspatio-temporal representation learning by Local and Global Diffusion (LGD).\nSpecifically, we construct a novel neural network architecture that learns the\nlocal and global representations in parallel. The architecture is composed of\nLGD blocks, where each block updates local and global features by modeling the\ndiffusions between these two representations. Diffusions effectively interact\ntwo aspects of information, i.e., localized and holistic, for more powerful way\nof representation learning. Furthermore, a kernelized classifier is introduced\nto combine the representations from two aspects for video recognition. Our LGD\nnetworks achieve clear improvements on the large-scale Kinetics-400 and\nKinetics-600 video classification datasets against the best competitors by 3.5%\nand 0.7%. We further examine the generalization of both the global and local\nrepresentations produced by our pre-trained LGD networks on four different\nbenchmarks for video action recognition and spatio-temporal action detection\ntasks. Superior performances over several state-of-the-art techniques on these\nbenchmarks are reported. Code is available at:\nhttps://github.com/ZhaofanQiu/local-and-global-diffusion-networks.', 'Manifold learning techniques for dynamical systems and time series have shown\ntheir utility for a broad spectrum of applications in recent years. While these\nmethods are effective at learning a low-dimensional representation, they are\noften insufficient for visualizing the global and local structure of the data.\nIn this paper, we present DIG (Dynamical Information Geometry), a visualization\nmethod for multivariate time series data that extracts an information geometry\nfrom a diffusion framework. Specifically, we implement a novel group of\ndistances in the context of diffusion operators, which may be useful to reveal\nstructure in the data that may not be accessible by the commonly used diffusion\ndistances. Finally, we present a case study applying our visualization tool to\nEEG data to visualize sleep stages.', 'We take the novel perspective to view data not as a probability distribution\nbut rather as a current. Primarily studied in the field of geometric measure\ntheory, $k$-currents are continuous linear functionals acting on compactly\nsupported smooth differential forms and can be understood as a generalized\nnotion of oriented $k$-dimensional manifold. By moving from distributions\n(which are $0$-currents) to $k$-currents, we can explicitly orient the data by\nattaching a $k$-dimensional tangent plane to each sample point. Based on the\nflat metric which is a fundamental distance between currents, we derive\nFlatGAN, a formulation in the spirit of generative adversarial networks but\ngeneralized to $k$-currents. In our theoretical contribution we prove that the\nflat metric between a parametrized current and a reference current is Lipschitz\ncontinuous in the parameters. In experiments, we show that the proposed shift\nto $k>0$ leads to interpretable and disentangled latent representations which\nbehave equivariantly to the specified oriented tangent planes.']","['Learning Spatio-Temporal Representation with Local and Global Diffusion', 'Visualizing High Dimensional Dynamical Processes', 'Flat Metric Minimization with Applications in Generative Modeling']","[""['cs.CV']"", ""['stat.ML', 'cs.LG', 'eess.SP']"", ""['cs.LG', 'cs.CV', 'stat.ML']""]","[0.8600984811782837, 0.8576536178588867, 0.8522980213165283]"
How many categories and images are included in the UDD dataset?,eleven,"['Learning to transfer visual attributes requires supervision dataset.\nCorresponding images with varying attribute values with the same identity are\nrequired for learning the transfer function. This largely limits their\napplications, because capturing them is often a difficult task. To address the\nissue, we propose an unsupervised method to learn to transfer visual attribute.\nThe proposed method can learn the transfer function without any corresponding\nimages. Inspecting visualization results from various unsupervised attribute\ntransfer tasks, we verify the effectiveness of the proposed method.', 'We consider the unsupervised learning problem of assigning labels to\nunlabeled data. A naive approach is to use clustering methods, but this works\nwell only when data is properly clustered and each cluster corresponds to an\nunderlying class. In this paper, we first show that this unsupervised labeling\nproblem in balanced binary cases can be solved if two unlabeled datasets having\ndifferent class balances are available. More specifically, estimation of the\nsign of the difference between probability densities of two unlabeled datasets\ngives the solution. We then introduce a new method to directly estimate the\nsign of the density difference without density estimation. Finally, we\ndemonstrate the usefulness of the proposed method against several clustering\nmethods on various toy problems and real-world datasets.', 'The majority of existing color naming methods focuses on the eleven basic\ncolor terms of the English language. However, in many applications, different\nsets of color names are used for the accurate description of objects. Labeling\ndata to learn these domain-specific color names is an expensive and laborious\ntask. Therefore, in this article we aim to learn color names from weakly\nlabeled data. For this purpose, we add an attention branch to the color naming\nnetwork. The attention branch is used to modulate the pixel-wise color naming\npredictions of the network. In experiments, we illustrate that the attention\nbranch correctly identifies the relevant regions. Furthermore, we show that our\nmethod obtains state-of-the-art results for pixel-wise and image-wise\nclassification on the EBAY dataset and is able to learn color names for various\ndomains.']","['Unsupervised Visual Attribute Transfer with Reconfigurable Generative Adversarial Networks', 'Clustering Unclustered Data: Unsupervised Binary Labeling of Two Datasets Having Different Class Balances', 'Weakly Supervised Domain-Specific Color Naming Based on Attention']","[""['cs.CV']"", ""['cs.LG']"", ""['cs.CV']""]","[0.8311766386032104, 0.8294934034347534, 0.8279050588607788]"
What are the recent advancements in graph neural networks?,graph analysis tasks,"['Recently, graph neural networks have been adopted in a wide variety of\napplications ranging from relational representations to modeling irregular data\ndomains such as point clouds and social graphs. However, the space of graph\nneural network architectures remains highly fragmented impeding the development\nof optimized implementations similar to what is available for convolutional\nneural networks. In this work, we present BiGraphNet, a graph neural network\narchitecture that generalizes many popular graph neural network models and\nenables new efficient operations similar to those supported by ConvNets. By\nexplicitly separating the input and output nodes, BiGraphNet: (i) generalizes\nthe graph convolution to support new efficient operations such as coarsened\ngraph convolutions (similar to strided convolution in convnets), multiple input\ngraphs convolution and graph expansions (unpooling) which can be used to\nimplement various graph architectures such as graph autoencoders, and graph\nresidual nets; and (ii) accelerates and scales the computations and memory\nrequirements in hierarchical networks by performing computations only at\nspecified output nodes.', 'Graphs play an important role in many applications. Recently, Graph Neural\nNetworks (GNNs) have achieved promising results in graph analysis tasks. Some\nstate-of-the-art GNN models have been proposed, e.g., Graph Convolutional\nNetworks (GCNs), Graph Attention Networks (GATs), etc. Despite these successes,\nmost of the GNNs only have shallow structure. This causes the low expressive\npower of the GNNs. To fully utilize the power of the deep neural network, some\ndeep GNNs have been proposed recently. However, the design of deep GNNs\nrequires significant architecture engineering. In this work, we propose a\nmethod to automate the deep GNNs design. In our proposed method, we add a new\ntype of skip connection to the GNNs search space to encourage feature reuse and\nalleviate the vanishing gradient problem. We also allow our evolutionary\nalgorithm to increase the layers of GNNs during the evolution to generate\ndeeper networks. We evaluate our method in the graph node classification task.\nThe experiments show that the GNNs generated by our method can obtain\nstate-of-the-art results in Cora, Citeseer, Pubmed and PPI datasets.', 'The performance of graph neural nets (GNNs) is known to gradually decrease\nwith increasing number of layers. This decay is partly attributed to\noversmoothing, where repeated graph convolutions eventually make node\nembeddings indistinguishable. We take a closer look at two different\ninterpretations, aiming to quantify oversmoothing. Our main contribution is\nPairNorm, a novel normalization layer that is based on a careful analysis of\nthe graph convolution operator, which prevents all node embeddings from\nbecoming too similar. What is more, PairNorm is fast, easy to implement without\nany change to network architecture nor any additional parameters, and is\nbroadly applicable to any GNN. Experiments on real-world graphs demonstrate\nthat PairNorm makes deeper GCN, GAT, and SGC models more robust against\noversmoothing, and significantly boosts performance for a new problem setting\nthat benefits from deeper GNNs. Code is available at\nhttps://github.com/LingxiaoShawn/PairNorm.']","['Hierarchical Bipartite Graph Convolution Networks', 'AutoGraph: Automated Graph Neural Network', 'PairNorm: Tackling Oversmoothing in GNNs']","[""['cs.LG', 'cs.CV', 'stat.ML']"", ""['cs.LG', 'cs.AI', 'cs.SI']"", ""['cs.LG', 'stat.ML']""]","[0.9177868366241455, 0.8976446390151978, 0.8961672186851501]"
How is contrastive learning used in computer vision applications?,augmentation pipeline to vary sensor effects,"['This paper introduces Relative Predictive Coding (RPC), a new contrastive\nrepresentation learning objective that maintains a good balance among training\nstability, minibatch size sensitivity, and downstream task performance. The key\nto the success of RPC is two-fold. First, RPC introduces the relative\nparameters to regularize the objective for boundedness and low variance.\nSecond, RPC contains no logarithm and exponential score functions, which are\nthe main cause of training instability in prior contrastive objectives. We\nempirically verify the effectiveness of RPC on benchmark vision and speech\nself-supervised learning tasks. Lastly, we relate RPC with mutual information\n(MI) estimation, showing RPC can be used to estimate MI with low variance.', 'Recent work has focused on generating synthetic imagery to increase the size\nand variability of training data for learning visual tasks in urban scenes.\nThis includes increasing the occurrence of occlusions or varying environmental\nand weather effects. However, few have addressed modeling variation in the\nsensor domain. Sensor effects can degrade real images, limiting\ngeneralizability of network performance on visual tasks trained on synthetic\ndata and tested in real environments. This paper proposes an efficient,\nautomatic, physically-based augmentation pipeline to vary sensor effects\n--chromatic aberration, blur, exposure, noise, and color cast-- for synthetic\nimagery. In particular, this paper illustrates that augmenting synthetic\ntraining datasets with the proposed pipeline reduces the domain gap between\nsynthetic and real domains for the task of object detection in urban driving\nscenes.', 'Automatic color enhancement is aimed to adaptively adjust photos to expected\nstyles and tones. For current learned methods in this field, global harmonious\nperception and local details are hard to be well-considered in a single model\nsimultaneously. To address this problem, we propose a coarse-to-fine framework\nwith non-local attention for color enhancement in this paper. Within our\nframework, we propose to divide enhancement process into channel-wise\nenhancement and pixel-wise refinement performed by two cascaded Convolutional\nNeural Networks (CNNs). In channel-wise enhancement, our model predicts a\nglobal linear mapping for RGB channels of input images to perform global style\nadjustment. In pixel-wise refinement, we learn a refining mapping using\nresidual learning for local adjustment. Further, we adopt a non-local attention\nblock to capture the long-range dependencies from global information for\nsubsequent fine-grained local refinement. We evaluate our proposed framework on\nthe commonly using benchmark and conduct sufficient experiments to demonstrate\neach technical component within it.']","['Self-supervised Representation Learning with Relative Predictive Coding', 'Modeling Camera Effects to Improve Visual Learning from Synthetic Data', 'A Coarse-to-Fine Framework for Learned Color Enhancement with Non-Local Attention']","[""['cs.LG', 'cs.IT', 'math.IT']"", ""['cs.CV']"", ""['cs.CV']""]","[0.8825443983078003, 0.877284824848175, 0.8750636577606201]"
What role does self-supervised learning play in improving model performance?,Learning is a key component of self-supervised learning,"['Self-supervised learning, which learns by constructing artificial labels\ngiven only the input signals, has recently gained considerable attention for\nlearning representations with unlabeled datasets, i.e., learning without any\nhuman-annotated supervision. In this paper, we show that such a technique can\nbe used to significantly improve the model accuracy even under fully-labeled\ndatasets. Our scheme trains the model to learn both original and\nself-supervised tasks, but is different from conventional multi-task learning\nframeworks that optimize the summation of their corresponding losses. Our main\nidea is to learn a single unified task with respect to the joint distribution\nof the original and self-supervised labels, i.e., we augment original labels\nvia self-supervision of input transformation. This simple, yet effective\napproach allows to train models easier by relaxing a certain invariant\nconstraint during learning the original and self-supervised tasks\nsimultaneously. It also enables an aggregated inference which combines the\npredictions from different augmentations to improve the prediction accuracy.\nFurthermore, we propose a novel knowledge transfer technique, which we refer to\nas self-distillation, that has the effect of the aggregated inference in a\nsingle (faster) inference. We demonstrate the large accuracy improvement and\nwide applicability of our framework on various fully-supervised settings, e.g.,\nthe few-shot and imbalanced classification scenarios.', 'Self-supervision has demonstrated to be an effective learning strategy when\ntraining target tasks on small annotated data-sets. While current research\nfocuses on creating novel pretext tasks to learn meaningful and reusable\nrepresentations for the target task, these efforts obtain marginal performance\ngains compared to fully-supervised learning. Meanwhile, little attention has\nbeen given to study the robustness of networks trained in a self-supervised\nmanner. In this work, we demonstrate that networks trained via self-supervised\nlearning have superior robustness and generalizability compared to\nfully-supervised learning in the context of medical imaging. Our experiments on\npneumonia detection in X-rays and multi-organ segmentation in CT yield\nconsistent results exposing the hidden benefits of self-supervision for\nlearning robust feature representations.', ""Contextual bandits are a common problem faced by machine learning\npractitioners in domains as diverse as hypothesis testing to product\nrecommendations. There have been a lot of approaches in exploiting rich data\nrepresentations for contextual bandit problems with varying degree of success.\nSelf-supervised learning is a promising approach to find rich data\nrepresentations without explicit labels. In a typical self-supervised learning\nscheme, the primary task is defined by the problem objective (e.g. clustering,\nclassification, embedding generation etc.) and the secondary task is defined by\nthe self-supervision objective (e.g. rotation prediction, words in\nneighborhood, colorization, etc.). In the usual self-supervision, we learn\nimplicit labels from the training data for a secondary task. However, in the\ncontextual bandit setting, we don't have the advantage of getting implicit\nlabels due to lack of data in the initial phase of learning. We provide a novel\napproach to tackle this issue by combining a contextual bandit objective with a\nself supervision objective. By augmenting contextual bandit learning with\nself-supervision we get a better cumulative reward. Our results on eight\npopular computer vision datasets show substantial gains in cumulative reward.\nWe provide cases where the proposed scheme doesn't perform optimally and give\nalternative methods for better learning in these cases.""]","['Self-supervised Label Augmentation via Input Transformations', 'Evaluating the Robustness of Self-Supervised Learning in Medical Imaging', 'Self-Supervised Contextual Bandits in Computer Vision']","[""['cs.LG', 'cs.CV', 'stat.ML']"", ""['cs.CV']"", ""['cs.CV', 'cs.LG', 'stat.ML']""]","[0.9206073880195618, 0.9045630693435669, 0.9001454710960388]"
How does knowledge distillation contribute to deep learning efficiency?,Knowledge distillation transfers knowledge from a teacher network to a student by enforcing the student to mimic the outputs of the pretrained teacher on training data,"['This paper presents a method to interpret the success of knowledge\ndistillation by quantifying and analyzing task-relevant and task-irrelevant\nvisual concepts that are encoded in intermediate layers of a deep neural\nnetwork (DNN). More specifically, three hypotheses are proposed as follows. 1.\nKnowledge distillation makes the DNN learn more visual concepts than learning\nfrom raw data. 2. Knowledge distillation ensures that the DNN is prone to\nlearning various visual concepts simultaneously. Whereas, in the scenario of\nlearning from raw data, the DNN learns visual concepts sequentially. 3.\nKnowledge distillation yields more stable optimization directions than learning\nfrom raw data. Accordingly, we design three types of mathematical metrics to\nevaluate feature representations of the DNN. In experiments, we diagnosed\nvarious DNNs, and above hypotheses were verified.', 'Knowledge distillation (KD) transfers knowledge from a teacher network to a\nstudent by enforcing the student to mimic the outputs of the pretrained teacher\non training data. However, data samples are not always accessible in many cases\ndue to large data sizes, privacy, or confidentiality. Many efforts have been\nmade on addressing this problem for convolutional neural networks (CNNs) whose\ninputs lie in a grid domain within a continuous space such as images and\nvideos, but largely overlook graph neural networks (GNNs) that handle non-grid\ndata with different topology structures within a discrete space. The inherent\ndifferences between their inputs make these CNN-based approaches not applicable\nto GNNs. In this paper, we propose to our best knowledge the first dedicated\napproach to distilling knowledge from a GNN without graph data. The proposed\ngraph-free KD (GFKD) learns graph topology structures for knowledge transfer by\nmodeling them with multinomial distribution. We then introduce a gradient\nestimator to optimize this framework. Essentially, the gradients w.r.t. graph\nstructures are obtained by only using GNN forward-propagation without\nback-propagation, which means that GFKD is compatible with modern GNN libraries\nsuch as DGL and Geometric. Moreover, we provide the strategies for handling\ndifferent types of prior knowledge in the graph data or the GNNs. Extensive\nexperiments demonstrate that GFKD achieves the state-of-the-art performance for\ndistilling knowledge from GNNs without training data.', 'Knowledge distillation is a generalized logits matching technique for model\ncompression. Their equivalence is previously established on the condition of\n$\\textit{infinity temperature}$ and $\\textit{zero-mean normalization}$. In this\npaper, we prove that with only $\\textit{infinity temperature}$, the effect of\nknowledge distillation equals to logits matching with an extra regularization.\nFurthermore, we reveal that an additional weaker condition --\n$\\textit{equal-mean initialization}$ rather than the original\n$\\textit{zero-mean normalization}$ already suffices to set up the equivalence.\nThe key to our proof is we realize that in modern neural networks with the\ncross-entropy loss and softmax activation, the mean of back-propagated gradient\non logits always keeps zero.']","['Explaining Knowledge Distillation by Quantifying the Knowledge', 'Graph-Free Knowledge Distillation for Graph Neural Networks', 'Exploring the Connection between Knowledge Distillation and Logits Matching']","[""['cs.LG', 'cs.CV', 'stat.ML']"", ""['cs.LG', 'cs.AI']"", ""['cs.LG', 'cs.AI']""]","[0.9247924089431763, 0.8913745880126953, 0.8896060585975647]"
What are the key differences between traditional and quantum machine learning?,high bias or high variance,"['In the paper, we focus on complexity of C5.0 algorithm for constructing\ndecision tree classifier that is the models for the classification problem from\nmachine learning. In classical case the decision tree is constructed in\n$O(hd(NM+N \\log N))$ running time, where $M$ is a number of classes, $N$ is the\nsize of a training data set, $d$ is a number of attributes of each element, $h$\nis a tree height. Firstly, we improved the classical version, the running time\nof the new version is $O(h\\cdot d\\cdot N\\log N)$. Secondly, we suggest a\nquantum version of this algorithm, which uses quantum subroutines like the\namplitude amplification and the D{\\""u}rr-H{\\o}yer minimum search algorithms\nthat are based on Grover\'s algorithm. The running time of the quantum algorithm\nis $O\\big(h\\cdot \\sqrt{d}\\log d \\cdot N \\log N\\big)$ that is better than\ncomplexity of the classical algorithm.', 'Estimating and optimizing Mutual Information (MI) is core to many problems in\nmachine learning; however, bounding MI in high dimensions is challenging. To\nestablish tractable and scalable objectives, recent work has turned to\nvariational bounds parameterized by neural networks, but the relationships and\ntradeoffs between these bounds remains unclear. In this work, we unify these\nrecent developments in a single framework. We find that the existing\nvariational lower bounds degrade when the MI is large, exhibiting either high\nbias or high variance. To address this problem, we introduce a continuum of\nlower bounds that encompasses previous bounds and flexibly trades off bias and\nvariance. On high-dimensional, controlled problems, we empirically characterize\nthe bias and variance of the bounds and their gradients and demonstrate the\neffectiveness of our new bounds for estimation and representation learning.', 'We take the novel perspective to view data not as a probability distribution\nbut rather as a current. Primarily studied in the field of geometric measure\ntheory, $k$-currents are continuous linear functionals acting on compactly\nsupported smooth differential forms and can be understood as a generalized\nnotion of oriented $k$-dimensional manifold. By moving from distributions\n(which are $0$-currents) to $k$-currents, we can explicitly orient the data by\nattaching a $k$-dimensional tangent plane to each sample point. Based on the\nflat metric which is a fundamental distance between currents, we derive\nFlatGAN, a formulation in the spirit of generative adversarial networks but\ngeneralized to $k$-currents. In our theoretical contribution we prove that the\nflat metric between a parametrized current and a reference current is Lipschitz\ncontinuous in the parameters. In experiments, we show that the proposed shift\nto $k>0$ leads to interpretable and disentangled latent representations which\nbehave equivariantly to the specified oriented tangent planes.']","['The Quantum Version Of Classification Decision Tree Constructing Algorithm C5.0', 'On Variational Bounds of Mutual Information', 'Flat Metric Minimization with Applications in Generative Modeling']","[""['cs.LG', 'quant-ph', 'stat.ML']"", ""['cs.LG', 'stat.ML']"", ""['cs.LG', 'cs.CV', 'stat.ML']""]","[0.8773800134658813, 0.8353546857833862, 0.8311080932617188]"
What were the primary causes of the fall of the Roman Empire?,a lack of large-scale benchmarking or missing state-of-the-art methods,"['Carbon capture and storage (CCS) can aid decarbonization of the atmosphere to\nlimit further global temperature increases. A framework utilizing unsupervised\nlearning is used to generate a range of subsurface geologic volumes to\ninvestigate potential sites for long-term storage of carbon dioxide. Generative\nadversarial networks are used to create geologic volumes, with a further neural\nnetwork used to sample the posterior distribution of a trained Generator\nconditional to sparsely sampled physical measurements. These generative models\nare further conditioned to historic dynamic fluid flow data through Bayesian\ninversion to improve the resolution of the forecast of the storage capacity of\ninjected carbon dioxide.', 'This work is devoted to a comprehensive analysis of topological data analysis\nfortime series classification. Previous works have significant shortcomings,\nsuch aslack of large-scale benchmarking or missing state-of-the-art methods. In\nthis work,we propose TOTOPO for extracting topological descriptors from\ndifferent types ofpersistence diagrams. The results suggest that TOTOPO\nsignificantly outperformsexisting baselines in terms of accuracy. TOTOPO is\nalso competitive with thestate-of-the-art, being the best on 20% of univariate\nand 40% of multivariate timeseries datasets. This work validates the hypothesis\nthat TDA-based approaches arerobust to small perturbations in data and are\nuseful for cases where periodicity andshape help discriminate between classes.', 'Visual illusions allow researchers to devise and test new models of visual\nperception. Here we show that artificial neural networks trained for basic\nvisual tasks in natural images are deceived by brightness and color illusions,\nhaving a response that is qualitatively very similar to the human achromatic\nand chromatic contrast sensitivity functions, and consistent with natural image\nstatistics. We also show that, while these artificial networks are deceived by\nillusions, their response might be significantly different to that of humans.\nOur results suggest that low-level illusions appear in any system that has to\nperform basic visual tasks in natural environments, in line with error\nminimization explanations of visual function, and they also imply a word of\ncaution on using artificial networks to study human vision, as previously\nsuggested in other contexts in the vision science literature.']","['Bayesian Inversion Of Generative Models For Geologic Storage Of Carbon Dioxide', 'TOTOPO: Classifying univariate and multivariate time series with Topological Data Analysis', 'Visual Illusions Also Deceive Convolutional Neural Networks: Analysis and Implications']","[""['stat.ML', 'cs.LG']"", ""['cs.LG']"", ""['cs.CV']""]","[0.7401406764984131, 0.7295398712158203, 0.7261519432067871]"
How did Renaissance art influence modern painting techniques?,a multi-layer representation for novel view synthesis,"[""While automatic computational techniques appear to reveal novel insights in\ndigital art history, a complementary approach seems to get less attention: that\nof human annotation. We argue and exemplify that a 'human in the loop' can\nreveal insights that may be difficult to detect automatically. Specifically, we\nfocussed on perceptual aspects within pictorial art. Using rather simple\nannotation tasks (e.g. delineate human lengths, indicate highlights and\nclassify gaze direction) we could both replicate earlier findings and reveal\nnovel insights into pictorial conventions. We found that Canaletto depicted\nhuman figures in rather accurate perspective, varied viewpoint elevation\nbetween approximately 3 and 9 meters and highly preferred light directions\nparallel to the projection plane. Furthermore, we found that taking the\naveraged images of leftward looking faces reveals a woman, and for rightward\nlooking faces showed a male, confirming earlier accounts on lateral gender bias\nin pictorial art. Lastly, we confirmed and refined the well-known\nlight-from-the-left bias. Together, the annotations, analyses and results\nexemplify how human annotation can contribute and complement to technical and\ndigital art history."", 'We propose a method for converting a single RGB-D input image into a 3D photo\n- a multi-layer representation for novel view synthesis that contains\nhallucinated color and depth structures in regions occluded in the original\nview. We use a Layered Depth Image with explicit pixel connectivity as\nunderlying representation, and present a learning-based inpainting model that\nsynthesizes new local color-and-depth content into the occluded region in a\nspatial context-aware manner. The resulting 3D photos can be efficiently\nrendered with motion parallax using standard graphics engines. We validate the\neffectiveness of our method on a wide range of challenging everyday scenes and\nshow fewer artifacts compared with the state of the arts.', 'In this paper, we propose a graph-based image-to-image translation framework\nfor generating images. We use rich data collected from the popular creativity\nplatform Artbreeder (http://artbreeder.com), where users interpolate multiple\nGAN-generated images to create artworks. This unique approach of creating new\nimages leads to a tree-like structure where one can track historical data about\nthe creation of a particular image. Inspired by this structure, we propose a\nnovel graph-to-image translation model called Graph2Pix, which takes a graph\nand corresponding images as input and generates a single image as output. Our\nexperiments show that Graph2Pix is able to outperform several image-to-image\ntranslation frameworks on benchmark metrics, including LPIPS (with a 25%\nimprovement) and human perception studies (n=60), where users preferred the\nimages generated by our method 81.5% of the time. Our source code and dataset\nare publicly available at https://github.com/catlab-team/graph2pix.']","[""Annotating shadows, highlights and faces: the contribution of a 'human in the loop' for digital art history"", '3D Photography using Context-aware Layered Depth Inpainting', 'Graph2Pix: A Graph-Based Image to Image Translation Framework']","[""['cs.CV', 'cs.HC']"", ""['cs.CV', 'eess.IV']"", ""['cs.CV']""]","[0.8236390948295593, 0.7994264960289001, 0.7925318479537964]"
What are the philosophical implications of time travel?,TD-VAE,"['This note aims to provide a basic intuition on the concept of filtrations as\nused in the context of reinforcement learning (RL). Filtrations are often used\nto formally define RL problems, yet their implications might not be eminent for\nthose without a background in measure theory. Essentially, a filtration is a\nconstruct that captures partial knowledge up to time $t$, without revealing any\nfuture information that has already been simulated, yet not revealed to the\ndecision-maker. We illustrate this with simple examples from the finance domain\non both discrete and continuous outcome spaces. Furthermore, we show that the\nnotion of filtration is not needed, as basing decisions solely on the current\nproblem state (which is possible due to the Markovian property) suffices to\neliminate future knowledge from the decision-making process.', 'To act and plan in complex environments, we posit that agents should have a\nmental simulator of the world with three characteristics: (a) it should build\nan abstract state representing the condition of the world; (b) it should form a\nbelief which represents uncertainty on the world; (c) it should go beyond\nsimple step-by-step simulation, and exhibit temporal abstraction. Motivated by\nthe absence of a model satisfying all these requirements, we propose TD-VAE, a\ngenerative sequence model that learns representations containing explicit\nbeliefs about states several steps into the future, and that can be rolled out\ndirectly without single-step transitions. TD-VAE is trained on pairs of\ntemporally separated time points, using an analogue of temporal difference\nlearning used in reinforcement learning.', 'Literary critics often attempt to uncover meaning in a single work of\nliterature through careful reading and analysis. Applying natural language\nprocessing methods to aid in such literary analyses remains a challenge in\ndigital humanities. While most previous work focuses on ""distant reading"" by\nalgorithmically discovering high-level patterns from large collections of\nliterary works, here we sharpen the focus of our methods to a single literary\ntheory about Italo Calvino\'s postmodern novel Invisible Cities, which consists\nof 55 short descriptions of imaginary cities. Calvino has provided a\nclassification of these cities into eleven thematic groups, but literary\nscholars disagree as to how trustworthy his categorization is. Due to the\nunique structure of this novel, we can computationally weigh in on this debate:\nwe leverage pretrained contextualized representations to embed each city\'s\ndescription and use unsupervised methods to cluster these embeddings.\nAdditionally, we compare results of our computational approach to similarity\njudgments generated by human readers. Our work is a first step towards\nincorporating natural language processing into literary criticism.']","['A Gentle Lecture Note on Filtrations in Reinforcement Learning', 'Temporal Difference Variational Auto-Encoder', 'Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism']","[""['cs.LG', 'cs.AI']"", ""['cs.LG', 'stat.ML']"", ""['cs.LG', 'cs.CL', 'stat.ML']""]","[0.7817533016204834, 0.7779459953308105, 0.7632167339324951]"
How does astrology determine personality traits?,elliptical distributions are increasingly used to generalise symmetric distributions,"[""Emotions play an important role in people's life. Understanding and\nrecognising is not only important for interpersonal communication, but also has\npromising applications in Human-Computer Interaction, automobile safety and\nmedical research. This project focuses on extending the emotion recognition\ndatabase, and training the CNN + RNN emotion recognition neural networks with\nemotion category representation and valence \\& arousal representation. The\ncombined models are constructed by training the two representations\nsimultaneously. The comparison and analysis between the three types of model\nare discussed. The inner-relationship between two emotion representations and\nthe interpretability of the neural networks are investigated. The findings\nsuggest that categorical emotion recognition performance can benefit from\ntraining with a combined model. And the mapping of emotion category and valence\n\\& arousal values can explain this phenomenon."", 'A large class of modern probabilistic learning systems assumes symmetric\ndistributions, however, real-world data tend to obey skewed distributions and\nare thus not always adequately modelled through symmetric distributions. To\naddress this issue, elliptical distributions are increasingly used to\ngeneralise symmetric distributions, and further improvements to skewed\nelliptical distributions have recently attracted much attention. However,\nexisting approaches are either hard to estimate or have complicated and\nabstract representations. To this end, we propose to employ the\nvon-Mises-Fisher (vMF) distribution to obtain an explicit and simple\nprobability representation of the skewed elliptical distribution. This is shown\nnot only to allow us to deal with non-symmetric learning systems, but also to\nprovide a physically meaningful way of generalising skewed distributions. For\nrigour, our extension is proved to share important and desirable properties\nwith its symmetric counterpart. We also demonstrate that the proposed vMF\ndistribution is both easy to generate and stable to estimate, both\ntheoretically and through examples.', ""Facial analysis models are increasingly used in applications that have\nserious impacts on people's lives, ranging from authentication to surveillance\ntracking. It is therefore critical to develop techniques that can reveal\nunintended biases in facial classifiers to help guide the ethical use of facial\nanalysis technology. This work proposes a framework called \\textit{image\ncounterfactual sensitivity analysis}, which we explore as a proof-of-concept in\nanalyzing a smiling attribute classifier trained on faces of celebrities. The\nframework utilizes counterfactuals to examine how a classifier's prediction\nchanges if a face characteristic slightly changes. We leverage recent advances\nin generative adversarial networks to build a realistic generative model of\nface images that affords controlled manipulation of specific image\ncharacteristics. We then introduce a set of metrics that measure the effect of\nmanipulating a specific property on the output of the trained classifier.\nEmpirically, we find several different factors of variation that affect the\npredictions of the smiling classifier. This proof-of-concept demonstrates\npotential ways generative models can be leveraged for fine-grained analysis of\nbias and fairness.""]","['Interpretable Deep Neural Networks for Dimensional and Categorical Emotion Recognition in-the-wild', 'Von Mises-Fisher Elliptical Distribution', 'Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias']","[""['cs.LG', 'cs.CV', 'stat.ML']"", ""['stat.ML', 'cs.LG']"", ""['cs.CV', 'cs.LG', 'stat.ML']""]","[0.7827245593070984, 0.7791645526885986, 0.7776768803596497]"
What are the traditional methods of winemaking in France?,PINN,"['We present a Physics-Informed Neural Network (PINN) to simulate the\nthermochemical evolution of a composite material on a tool undergoing cure in\nan autoclave. In particular, we solve the governing coupled system of\ndifferential equations -- including conductive heat transfer and resin cure\nkinetics -- by optimizing the parameters of a deep neural network (DNN) using a\nphysics-based loss function. To account for the vastly different behaviour of\nthermal conduction and resin cure, we design a PINN consisting of two\ndisconnected subnetworks, and develop a sequential training algorithm that\nmitigates instability present in traditional training methods. Further, we\nincorporate explicit discontinuities into the DNN at the composite-tool\ninterface and enforce known physical behaviour directly in the loss function to\nimprove the solution near the interface. We train the PINN with a technique\nthat automatically adapts the weights on the loss terms corresponding to PDE,\nboundary, interface, and initial conditions. Finally, we demonstrate that one\ncan include problem parameters as an input to the model -- resulting in a\nsurrogate that provides real-time simulation for a range of problem settings --\nand that one can use transfer learning to significantly reduce the training\ntime for problem settings similar to that of an initial trained model. The\nperformance of the proposed PINN is demonstrated in multiple scenarios with\ndifferent material thicknesses and thermal boundary conditions.', 'Carbon capture and storage (CCS) can aid decarbonization of the atmosphere to\nlimit further global temperature increases. A framework utilizing unsupervised\nlearning is used to generate a range of subsurface geologic volumes to\ninvestigate potential sites for long-term storage of carbon dioxide. Generative\nadversarial networks are used to create geologic volumes, with a further neural\nnetwork used to sample the posterior distribution of a trained Generator\nconditional to sparsely sampled physical measurements. These generative models\nare further conditioned to historic dynamic fluid flow data through Bayesian\ninversion to improve the resolution of the forecast of the storage capacity of\ninjected carbon dioxide.', 'This paper introduces a fast and efficient segmentation technique for 2D\nimages and 3D point clouds of building facades. Facades of buildings are highly\nstructured and consequently most methods that have been proposed for this\nproblem aim to make use of this strong prior information. Contrary to most\nprior work, we are describing a system that is almost domain independent and\nconsists of standard segmentation methods. We train a sequence of boosted\ndecision trees using auto-context features. This is learned using stacked\ngeneralization. We find that this technique performs better, or comparable with\nall previous published methods and present empirical results on all available\n2D and 3D facade benchmark datasets. The proposed method is simple to\nimplement, easy to extend, and very efficient at test-time inference.']","['Physics-Informed Neural Network for Modelling the Thermochemical Curing Process of Composite-Tool Systems During Manufacture', 'Bayesian Inversion Of Generative Models For Geologic Storage Of Carbon Dioxide', 'Efficient 2D and 3D Facade Segmentation using Auto-Context']","[""['cs.LG', '35Q79, 74A40, 68T07, 65M22,', 'J.2']"", ""['stat.ML', 'cs.LG']"", ""['cs.CV']""]","[0.7374398708343506, 0.7245824933052063, 0.7227652668952942]"
What role did mythology play in ancient Greek society?,digital humanities,"['Literary critics often attempt to uncover meaning in a single work of\nliterature through careful reading and analysis. Applying natural language\nprocessing methods to aid in such literary analyses remains a challenge in\ndigital humanities. While most previous work focuses on ""distant reading"" by\nalgorithmically discovering high-level patterns from large collections of\nliterary works, here we sharpen the focus of our methods to a single literary\ntheory about Italo Calvino\'s postmodern novel Invisible Cities, which consists\nof 55 short descriptions of imaginary cities. Calvino has provided a\nclassification of these cities into eleven thematic groups, but literary\nscholars disagree as to how trustworthy his categorization is. Due to the\nunique structure of this novel, we can computationally weigh in on this debate:\nwe leverage pretrained contextualized representations to embed each city\'s\ndescription and use unsupervised methods to cluster these embeddings.\nAdditionally, we compare results of our computational approach to similarity\njudgments generated by human readers. Our work is a first step towards\nincorporating natural language processing into literary criticism.', ""While automatic computational techniques appear to reveal novel insights in\ndigital art history, a complementary approach seems to get less attention: that\nof human annotation. We argue and exemplify that a 'human in the loop' can\nreveal insights that may be difficult to detect automatically. Specifically, we\nfocussed on perceptual aspects within pictorial art. Using rather simple\nannotation tasks (e.g. delineate human lengths, indicate highlights and\nclassify gaze direction) we could both replicate earlier findings and reveal\nnovel insights into pictorial conventions. We found that Canaletto depicted\nhuman figures in rather accurate perspective, varied viewpoint elevation\nbetween approximately 3 and 9 meters and highly preferred light directions\nparallel to the projection plane. Furthermore, we found that taking the\naveraged images of leftward looking faces reveals a woman, and for rightward\nlooking faces showed a male, confirming earlier accounts on lateral gender bias\nin pictorial art. Lastly, we confirmed and refined the well-known\nlight-from-the-left bias. Together, the annotations, analyses and results\nexemplify how human annotation can contribute and complement to technical and\ndigital art history."", 'We present basic notions of Gold\'s ""learnability in the limit"" paradigm,\nfirst presented in 1967, a formalization of the cognitive process by which a\nnative speaker gets to grasp the underlying grammar of his/her own native\nlanguage by being exposed to well formed sentences generated by that grammar.\nThen we present Lambek grammars, a formalism issued from categorial grammars\nwhich, although not as expressive as needed for a full formalization of natural\nlanguages, is particularly suited to easily implement a natural interface\nbetween syntax and semantics. In the last part of this work, we present a\nlearnability result for Rigid Lambek grammars from structured examples.']","['Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism', ""Annotating shadows, highlights and faces: the contribution of a 'human in the loop' for digital art history"", 'A Study on Learnability for Rigid Lambek Grammars']","[""['cs.LG', 'cs.CL', 'stat.ML']"", ""['cs.CV', 'cs.HC']"", ""['cs.LG']""]","[0.7417377829551697, 0.7384332418441772, 0.7341868877410889]"
What are the main techniques used in composing classical symphonies?,Learning idioms,"['We present basic notions of Gold\'s ""learnability in the limit"" paradigm,\nfirst presented in 1967, a formalization of the cognitive process by which a\nnative speaker gets to grasp the underlying grammar of his/her own native\nlanguage by being exposed to well formed sentences generated by that grammar.\nThen we present Lambek grammars, a formalism issued from categorial grammars\nwhich, although not as expressive as needed for a full formalization of natural\nlanguages, is particularly suited to easily implement a natural interface\nbetween syntax and semantics. In the last part of this work, we present a\nlearnability result for Rigid Lambek grammars from structured examples.', ""Program synthesis of general-purpose source code from natural language\nspecifications is challenging due to the need to reason about high-level\npatterns in the target program and low-level implementation details at the same\ntime. In this work, we present PATOIS, a system that allows a neural program\nsynthesizer to explicitly interleave high-level and low-level reasoning at\nevery generation step. It accomplishes this by automatically mining common code\nidioms from a given corpus, incorporating them into the underlying language for\nneural synthesis, and training a tree-based neural synthesizer to use these\nidioms during code generation. We evaluate PATOIS on two complex semantic\nparsing datasets and show that using learned code idioms improves the\nsynthesizer's accuracy."", 'This paper presents the beginnings of an automatic statistician, focusing on\nregression problems. Our system explores an open-ended space of statistical\nmodels to discover a good explanation of a data set, and then produces a\ndetailed report with figures and natural-language text. Our approach treats\nunknown regression functions nonparametrically using Gaussian processes, which\nhas two important consequences. First, Gaussian processes can model functions\nin terms of high-level properties (e.g. smoothness, trends, periodicity,\nchangepoints). Taken together with the compositional structure of our language\nof models this allows us to automatically describe functions in simple terms.\nSecond, the use of flexible nonparametric models and a rich language for\ncomposing them in an open-ended manner also results in state-of-the-art\nextrapolation performance evaluated over 13 real time series data sets from\nvarious domains.']","['A Study on Learnability for Rigid Lambek Grammars', 'Program Synthesis and Semantic Parsing with Learned Code Idioms', 'Automatic Construction and Natural-Language Description of Nonparametric Regression Models']","[""['cs.LG']"", ""['cs.LG', 'cs.AI', 'cs.CL', 'cs.PL', 'stat.ML']"", ""['stat.ML', 'cs.LG']""]","[0.7735093832015991, 0.7629592418670654, 0.7606574296951294]"
How has urban planning evolved in the last century?,generative sequence model,"['This paper introduces a fast and efficient segmentation technique for 2D\nimages and 3D point clouds of building facades. Facades of buildings are highly\nstructured and consequently most methods that have been proposed for this\nproblem aim to make use of this strong prior information. Contrary to most\nprior work, we are describing a system that is almost domain independent and\nconsists of standard segmentation methods. We train a sequence of boosted\ndecision trees using auto-context features. This is learned using stacked\ngeneralization. We find that this technique performs better, or comparable with\nall previous published methods and present empirical results on all available\n2D and 3D facade benchmark datasets. The proposed method is simple to\nimplement, easy to extend, and very efficient at test-time inference.', 'To act and plan in complex environments, we posit that agents should have a\nmental simulator of the world with three characteristics: (a) it should build\nan abstract state representing the condition of the world; (b) it should form a\nbelief which represents uncertainty on the world; (c) it should go beyond\nsimple step-by-step simulation, and exhibit temporal abstraction. Motivated by\nthe absence of a model satisfying all these requirements, we propose TD-VAE, a\ngenerative sequence model that learns representations containing explicit\nbeliefs about states several steps into the future, and that can be rolled out\ndirectly without single-step transitions. TD-VAE is trained on pairs of\ntemporally separated time points, using an analogue of temporal difference\nlearning used in reinforcement learning.', 'This paper gives a detailed review of reinforcement learning in combinatorial\noptimization, introduces the history of combinatorial optimization starting in\nthe 1960s, and compares it with the reinforcement learning algorithms in recent\nyears. We explicitly look at a famous combinatorial problem known as the\nTraveling Salesperson Problem (TSP). We compare the approach of the modern\nreinforcement learning algorithms on TSP with an approach published in 1970.\nThen, we discuss the similarities between these algorithms and how the approach\nof reinforcement learning changes due to the evolution of machine learning\ntechniques and computing power. We also mention the deep learning approach on\nthe TSP, which is named Deep Reinforcement Learning. We argue that deep\nlearning is a generic approach that can be integrated with traditional\nreinforcement learning algorithms and optimize the outcomes of the TSP.']","['Efficient 2D and 3D Facade Segmentation using Auto-Context', 'Temporal Difference Variational Auto-Encoder', 'A Survey on Reinforcement Learning for Combinatorial Optimization']","[""['cs.CV']"", ""['cs.LG', 'stat.ML']"", ""['cs.LG', 'stat.ML']""]","[0.8003250360488892, 0.7982640862464905, 0.7947434186935425]"
What are the core principles of homeopathy?,a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of his/her own native language by being exposed to well formed sentences generated by that grammar,"['Autodock is a widely used molecular modeling tool which predicts how small\nmolecules bind to a receptor of known 3D structure. The current version of\nAutoDock uses meta-heuristic algorithms in combination with local search\nmethods for doing the conformation search. Appropriate settings of\nhyperparameters in these algorithms are important, particularly for novice\nusers who often find it hard to identify the best configuration. In this work,\nwe design a surrogate based multi-objective algorithm to help such users by\nautomatically tuning hyperparameter settings. The proposed method iteratively\nuses a radial basis function model and non-dominated sorting to evaluate the\nsampled configurations during the search phase. Our experimental results using\nAutodock show that the introduced component is practical and effective.', 'We present basic notions of Gold\'s ""learnability in the limit"" paradigm,\nfirst presented in 1967, a formalization of the cognitive process by which a\nnative speaker gets to grasp the underlying grammar of his/her own native\nlanguage by being exposed to well formed sentences generated by that grammar.\nThen we present Lambek grammars, a formalism issued from categorial grammars\nwhich, although not as expressive as needed for a full formalization of natural\nlanguages, is particularly suited to easily implement a natural interface\nbetween syntax and semantics. In the last part of this work, we present a\nlearnability result for Rigid Lambek grammars from structured examples.', 'Learning to take actions based on observations is a core requirement for\nartificial agents to be able to be successful and robust at their task.\nReinforcement Learning (RL) is a well-known technique for learning such\npolicies. However, current RL algorithms often have to deal with reward\nshaping, have difficulties generalizing to other environments and are most\noften sample inefficient. In this paper, we explore active inference and the\nfree energy principle, a normative theory from neuroscience that explains how\nself-organizing biological systems operate by maintaining a model of the world\nand casting action selection as an inference problem. We apply this concept to\na typical problem known to the RL community, the mountain car problem, and show\nhow active inference encompasses both RL and learning from demonstrations.']","['Automatic hyperparameter selection in Autodock', 'A Study on Learnability for Rigid Lambek Grammars', 'Bayesian policy selection using active inference']","[""['stat.ML', 'cs.LG']"", ""['cs.LG']"", ""['cs.LG', 'cs.AI', 'cs.NE']""]","[0.7457058429718018, 0.7445895075798035, 0.7369412183761597]"
What impact does climate change have on deep-sea marine ecosystems?,carbon capture and storage (CCS) can aid decarbonization of the atmosphere to limit further global temperature increases,"[""In this paper we test the use of a deep learning approach to automatically\ncount Wandering Albatrosses in Very High Resolution (VHR) satellite imagery. We\nuse a dataset of manually labelled imagery provided by the British Antarctic\nSurvey to train and develop our methods. We employ a U-Net architecture,\ndesigned for image segmentation, to simultaneously classify and localise\npotential albatrosses. We aid training with the use of the Focal Loss\ncriterion, to deal with extreme class imbalance in the dataset. Initial results\nachieve peak precision and recall values of approximately 80%. Finally we\nassess the model's performance in relation to inter-observer variation, by\ncomparing errors against an image labelled by multiple observers. We conclude\nmodel accuracy falls within the range of human counters. We hope that the\nmethods will streamline the analysis of VHR satellite images, enabling more\nfrequent monitoring of a species which is of high conservation concern."", 'Carbon capture and storage (CCS) can aid decarbonization of the atmosphere to\nlimit further global temperature increases. A framework utilizing unsupervised\nlearning is used to generate a range of subsurface geologic volumes to\ninvestigate potential sites for long-term storage of carbon dioxide. Generative\nadversarial networks are used to create geologic volumes, with a further neural\nnetwork used to sample the posterior distribution of a trained Generator\nconditional to sparsely sampled physical measurements. These generative models\nare further conditioned to historic dynamic fluid flow data through Bayesian\ninversion to improve the resolution of the forecast of the storage capacity of\ninjected carbon dioxide.', 'As the role played by statistical and computational sciences in climate and\nenvironmental modelling and prediction becomes more important, Machine Learning\nresearchers are becoming more aware of the relevance of their work to help\ntackle the climate crisis. Indeed, being universal nonlinear function\napproximation tools, Machine Learning algorithms are efficient in analysing and\nmodelling spatially and temporally variable environmental data. While Deep\nLearning models have proved to be able to capture spatial, temporal, and\nspatio-temporal dependencies through their automatic feature representation\nlearning, the problem of the interpolation of continuous spatio-temporal fields\nmeasured on a set of irregular points in space is still under-investigated. To\nfill this gap, we introduce here a framework for spatio-temporal prediction of\nclimate and environmental data using deep learning. Specifically, we show how\nspatio-temporal processes can be decomposed in terms of a sum of products of\ntemporally referenced basis functions, and of stochastic spatial coefficients\nwhich can be spatially modelled and mapped on a regular grid, allowing the\nreconstruction of the complete spatio-temporal signal. Applications on two case\nstudies based on simulated and real-world data will show the effectiveness of\nthe proposed framework in modelling coherent spatio-temporal fields.']","['Using Deep Learning to Count Albatrosses from Space', 'Bayesian Inversion Of Generative Models For Geologic Storage Of Carbon Dioxide', 'A Novel Framework for Spatio-Temporal Prediction of Environmental Data Using Deep Learning']","[""['cs.CV']"", ""['stat.ML', 'cs.LG']"", ""['stat.ML', 'cs.LG', 'physics.ao-ph', 'physics.data-an', '62P12', 'I.2; J.2']""]","[0.8453024625778198, 0.8351677656173706, 0.8300192952156067]"
How do historians verify ancient manuscripts?,a 'human in the loop',"['In this paper, we face the problem of offline handwritten text recognition\n(HTR) in historical documents when few labeled samples are available and some\nof them contain errors in the train set. Three main contributions are\ndeveloped. First we analyze how to perform transfer learning (TL) from a\nmassive database to a smaller historical database, analyzing which layers of\nthe model need a fine-tuning process. Second, we analyze methods to efficiently\ncombine TL and data augmentation (DA). Finally, an algorithm to mitigate the\neffects of incorrect labelings in the training set is proposed. The methods are\nanalyzed over the ICFHR 2018 competition database, Washington and Parzival.\nCombining all these techniques, we demonstrate a remarkable reduction of CER\n(up to 6% in some cases) in the test set with little complexity overhead.', ""While automatic computational techniques appear to reveal novel insights in\ndigital art history, a complementary approach seems to get less attention: that\nof human annotation. We argue and exemplify that a 'human in the loop' can\nreveal insights that may be difficult to detect automatically. Specifically, we\nfocussed on perceptual aspects within pictorial art. Using rather simple\nannotation tasks (e.g. delineate human lengths, indicate highlights and\nclassify gaze direction) we could both replicate earlier findings and reveal\nnovel insights into pictorial conventions. We found that Canaletto depicted\nhuman figures in rather accurate perspective, varied viewpoint elevation\nbetween approximately 3 and 9 meters and highly preferred light directions\nparallel to the projection plane. Furthermore, we found that taking the\naveraged images of leftward looking faces reveals a woman, and for rightward\nlooking faces showed a male, confirming earlier accounts on lateral gender bias\nin pictorial art. Lastly, we confirmed and refined the well-known\nlight-from-the-left bias. Together, the annotations, analyses and results\nexemplify how human annotation can contribute and complement to technical and\ndigital art history."", 'Literary critics often attempt to uncover meaning in a single work of\nliterature through careful reading and analysis. Applying natural language\nprocessing methods to aid in such literary analyses remains a challenge in\ndigital humanities. While most previous work focuses on ""distant reading"" by\nalgorithmically discovering high-level patterns from large collections of\nliterary works, here we sharpen the focus of our methods to a single literary\ntheory about Italo Calvino\'s postmodern novel Invisible Cities, which consists\nof 55 short descriptions of imaginary cities. Calvino has provided a\nclassification of these cities into eleven thematic groups, but literary\nscholars disagree as to how trustworthy his categorization is. Due to the\nunique structure of this novel, we can computationally weigh in on this debate:\nwe leverage pretrained contextualized representations to embed each city\'s\ndescription and use unsupervised methods to cluster these embeddings.\nAdditionally, we compare results of our computational approach to similarity\njudgments generated by human readers. Our work is a first step towards\nincorporating natural language processing into literary criticism.']","['Boosting offline handwritten text recognition in historical documents with few labeled lines', ""Annotating shadows, highlights and faces: the contribution of a 'human in the loop' for digital art history"", 'Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism']","[""['cs.CV']"", ""['cs.CV', 'cs.HC']"", ""['cs.LG', 'cs.CL', 'stat.ML']""]","[0.8167415261268616, 0.8048121929168701, 0.7996010780334473]"
What are the main psychological theories explaining human creativity?,DNNs,"['Deep neural networks (DNNs) have achieved unprecedented performance on a wide\nrange of complex tasks, rapidly outpacing our understanding of the nature of\ntheir solutions. This has caused a recent surge of interest in methods for\nrendering modern neural systems more interpretable. In this work, we propose to\naddress the interpretability problem in modern DNNs using the rich history of\nproblem descriptions, theories and experimental methods developed by cognitive\npsychologists to study the human mind. To explore the potential value of these\ntools, we chose a well-established analysis from developmental psychology that\nexplains how children learn word labels for objects, and applied that analysis\nto DNNs. Using datasets of stimuli inspired by the original cognitive\npsychology experiments, we find that state-of-the-art one shot learning models\ntrained on ImageNet exhibit a similar bias to that observed in humans: they\nprefer to categorize objects according to shape rather than color. The\nmagnitude of this shape bias varies greatly among architecturally identical,\nbut differently seeded models, and even fluctuates within seeds throughout\ntraining, despite nearly equivalent classification performance. These results\ndemonstrate the capability of tools from cognitive psychology for exposing\nhidden computational properties of DNNs, while concurrently providing us with a\ncomputational model for human word learning.', ""While automatic computational techniques appear to reveal novel insights in\ndigital art history, a complementary approach seems to get less attention: that\nof human annotation. We argue and exemplify that a 'human in the loop' can\nreveal insights that may be difficult to detect automatically. Specifically, we\nfocussed on perceptual aspects within pictorial art. Using rather simple\nannotation tasks (e.g. delineate human lengths, indicate highlights and\nclassify gaze direction) we could both replicate earlier findings and reveal\nnovel insights into pictorial conventions. We found that Canaletto depicted\nhuman figures in rather accurate perspective, varied viewpoint elevation\nbetween approximately 3 and 9 meters and highly preferred light directions\nparallel to the projection plane. Furthermore, we found that taking the\naveraged images of leftward looking faces reveals a woman, and for rightward\nlooking faces showed a male, confirming earlier accounts on lateral gender bias\nin pictorial art. Lastly, we confirmed and refined the well-known\nlight-from-the-left bias. Together, the annotations, analyses and results\nexemplify how human annotation can contribute and complement to technical and\ndigital art history."", 'Visual illusions allow researchers to devise and test new models of visual\nperception. Here we show that artificial neural networks trained for basic\nvisual tasks in natural images are deceived by brightness and color illusions,\nhaving a response that is qualitatively very similar to the human achromatic\nand chromatic contrast sensitivity functions, and consistent with natural image\nstatistics. We also show that, while these artificial networks are deceived by\nillusions, their response might be significantly different to that of humans.\nOur results suggest that low-level illusions appear in any system that has to\nperform basic visual tasks in natural environments, in line with error\nminimization explanations of visual function, and they also imply a word of\ncaution on using artificial networks to study human vision, as previously\nsuggested in other contexts in the vision science literature.']","['Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study', ""Annotating shadows, highlights and faces: the contribution of a 'human in the loop' for digital art history"", 'Visual Illusions Also Deceive Convolutional Neural Networks: Analysis and Implications']","[""['stat.ML', 'cs.CV', 'cs.LG']"", ""['cs.CV', 'cs.HC']"", ""['cs.CV']""]","[0.7891461849212646, 0.7856591939926147, 0.7822584509849548]"
