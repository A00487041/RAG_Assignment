titles,summaries,terms
Survey on Semantic Stereo Matching / Semantic Depth Estimation,"Stereo matching is one of the widely used techniques for inferring depth from
stereo images owing to its robustness and speed. It has become one of the major
topics of research since it finds its applications in autonomous driving,
robotic navigation, 3D reconstruction, and many other fields. Finding pixel
correspondences in non-textured, occluded and reflective areas is the major
challenge in stereo matching. Recent developments have shown that semantic cues
from image segmentation can be used to improve the results of stereo matching.
Many deep neural network architectures have been proposed to leverage the
advantages of semantic segmentation in stereo matching. This paper aims to give
a comparison among the state of art networks both in terms of accuracy and in
terms of speed which are of higher importance in real-time applications.","['cs.CV', 'cs.LG']"
FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging,"The recent advancements in artificial intelligence (AI) combined with the
extensive amount of data generated by today's clinical systems, has led to the
development of imaging AI solutions across the whole value chain of medical
imaging, including image reconstruction, medical image segmentation,
image-based diagnosis and treatment planning. Notwithstanding the successes and
future potential of AI in medical imaging, many stakeholders are concerned of
the potential risks and ethical implications of imaging AI solutions, which are
perceived as complex, opaque, and difficult to comprehend, utilise, and trust
in critical clinical applications. Despite these concerns and risks, there are
currently no concrete guidelines and best practices for guiding future AI
developments in medical imaging towards increased trust, safety and adoption.
To bridge this gap, this paper introduces a careful selection of guiding
principles drawn from the accumulated experiences, consensus, and best
practices from five large European projects on AI in Health Imaging. These
guiding principles are named FUTURE-AI and its building blocks consist of (i)
Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness
and (vi) Explainability. In a step-by-step approach, these guidelines are
further translated into a framework of concrete recommendations for specifying,
developing, evaluating, and deploying technically, clinically and ethically
trustworthy AI solutions into clinical practice.","['cs.CV', 'cs.AI', 'cs.LG']"
Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation,"In this paper, we proposed a novel mutual consistency network (MC-Net+) to
effectively exploit the unlabeled hard regions for semi-supervised medical
image segmentation. The MC-Net+ model is motivated by the observation that deep
models trained with limited annotations are prone to output highly uncertain
and easily mis-classified predictions in the ambiguous regions (e.g. adhesive
edges or thin branches) for the image segmentation task. Leveraging these
region-level challenging samples can make the semi-supervised segmentation
model training more effective. Therefore, our proposed MC-Net+ model consists
of two new designs. First, the model contains one shared encoder and multiple
sightly different decoders (i.e. using different up-sampling strategies). The
statistical discrepancy of multiple decoders' outputs is computed to denote the
model's uncertainty, which indicates the unlabeled hard regions. Second, a new
mutual consistency constraint is enforced between one decoder's probability
output and other decoders' soft pseudo labels. In this way, we minimize the
model's uncertainty during training and force the model to generate invariant
and low-entropy results in such challenging areas of unlabeled data, in order
to learn a generalized feature representation. We compared the segmentation
results of the MC-Net+ with five state-of-the-art semi-supervised approaches on
three public medical datasets. Extension experiments with two common
semi-supervised settings demonstrate the superior performance of our model over
other existing methods, which sets a new state of the art for semi-supervised
medical image segmentation.","['cs.CV', 'cs.AI']"
Parameter Decoupling Strategy for Semi-supervised 3D Left Atrium Segmentation,"Consistency training has proven to be an advanced semi-supervised framework
and achieved promising results in medical image segmentation tasks through
enforcing an invariance of the predictions over different views of the inputs.
However, with the iterative updating of model parameters, the models would tend
to reach a coupled state and eventually lose the ability to exploit unlabeled
data. To address the issue, we present a novel semi-supervised segmentation
model based on parameter decoupling strategy to encourage consistent
predictions from diverse views. Specifically, we first adopt a two-branch
network to simultaneously produce predictions for each image. During the
training process, we decouple the two prediction branch parameters by quadratic
cosine distance to construct different views in latent space. Based on this,
the feature extractor is constrained to encourage the consistency of
probability maps generated by classifiers under diversified features. In the
overall training process, the parameters of feature extractor and classifiers
are updated alternately by consistency regularization operation and decoupling
operation to gradually improve the generalization performance of the model. Our
method has achieved a competitive result over the state-of-the-art
semi-supervised methods on the Atrial Segmentation Challenge dataset,
demonstrating the effectiveness of our framework. Code is available at
https://github.com/BX0903/PDC.",['cs.CV']
Background-Foreground Segmentation for Interior Sensing in Automotive Industry,"To ensure safety in automated driving, the correct perception of the
situation inside the car is as important as its environment. Thus, seat
occupancy detection and classification of detected instances play an important
role in interior sensing. By the knowledge of the seat occupancy status, it is
possible to, e.g., automate the airbag deployment control. Furthermore, the
presence of a driver, which is necessary for partially automated driving cars
at the automation levels two to four can be verified. In this work, we compare
different statistical methods from the field of image segmentation to approach
the problem of background-foreground segmentation in camera based interior
sensing. In the recent years, several methods based on different techniques
have been developed and applied to images or videos from different
applications. The peculiarity of the given scenarios of interior sensing is,
that the foreground instances and the background both contain static as well as
dynamic elements. In data considered in this work, even the camera position is
not completely fixed. We review and benchmark three different methods ranging,
i.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural
network, namely a Mask R-CNN. In particular, the limitations of the classical
methods, GMM and Morphological Snakes, for interior sensing are shown.
Furthermore, it turns, that it is possible to overcome these limitations by
deep learning, e.g.\ using a Mask R-CNN. Although only a small amount of ground
truth data was available for training, we enabled the Mask R-CNN to produce
high quality background-foreground masks via transfer learning. Moreover, we
demonstrate that certain augmentation as well as pre- and post-processing
methods further enhance the performance of the investigated methods.","['cs.CV', 'cs.LG']"
EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow,"High-quality training data play a key role in image segmentation tasks.
Usually, pixel-level annotations are expensive, laborious and time-consuming
for the large volume of training data. To reduce labelling cost and improve
segmentation quality, interactive segmentation methods have been proposed,
which provide the result with just a few clicks. However, their performance
does not meet the requirements of practical segmentation tasks in terms of
speed and accuracy. In this work, we propose EdgeFlow, a novel architecture
that fully utilizes interactive information of user clicks with edge-guided
flow. Our method achieves state-of-the-art performance without any
post-processing or iterative optimization scheme. Comprehensive experiments on
benchmarks also demonstrate the superiority of our method. In addition, with
the proposed method, we develop an efficient interactive segmentation tool for
practical data annotation tasks. The source code and tool is avaliable at
https://github.com/PaddlePaddle/PaddleSeg.","['cs.CV', 'cs.HC']"
Efficient Hybrid Transformer: Learning Global-local Context for Urban Sence Segmentation,"Semantic segmentation of fine-resolution urban scene images plays a vital
role in extensive practical applications, such as land cover mapping, urban
change detection, environmental protection and economic assessment. Driven by
rapid developments in deep learning technologies, convolutional neural networks
(CNNs) have dominated the semantic segmentation task for many years.
Convolutional neural networks adopt hierarchical feature representation and
have strong local context extraction. However, the local property of the
convolution layer limits the network from capturing global information that is
crucial for improving fine-resolution image segmentation. Recently, Transformer
comprise a hot topic in the computer vision domain. Vision Transformer
demonstrates the great capability of global information modelling, boosting
many vision tasks, such as image classification, object detection and
especially semantic segmentation. In this paper, we propose an efficient hybrid
Transformer (EHT) for semantic segmentation of urban scene images. EHT takes
advantage of CNNs and Transformer, learning global-local context to strengthen
the feature representation. Extensive experiments demonstrate that EHT has
higher efficiency with competitive accuracy compared with state-of-the-art
benchmark methods. Specifically, the proposed EHT achieves a 67.0% mIoU on the
UAVid test set and outperforms other lightweight models significantly. The code
will be available soon.",['cs.CV']
Towards to Robust and Generalized Medical Image Segmentation Framework,"To mitigate the radiologist's workload, computer-aided diagnosis with the
capability to review and analyze medical images is gradually deployed. Deep
learning-based region of interest segmentation is among the most exciting use
cases. However, this paradigm is restricted in real-world clinical applications
due to poor robustness and generalization. The issue is more sinister with a
lack of training data. In this paper, we address the challenge from the
representation learning point of view. We investigate that the collapsed
representations, as one of the main reasons which caused poor robustness and
generalization, could be avoided through transfer learning. Therefore, we
propose a novel two-stage framework for robust generalized segmentation. In
particular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining
architecture is coined to learn meaningful representation for improving the
generalization and robustness of the downstream tasks. Furthermore, the learned
knowledge is transferred to the segmentation benchmark. Coupled with an image
reconstruction network, the representation keeps to be decoded, encouraging the
model to capture more semantic features. Experiments of lung segmentation on
multi chest X-ray datasets are conducted. Empirically, the related experimental
results demonstrate the superior generalization capability of the proposed
framework on unseen domains in terms of high performance and robustness to
corruption, especially under the scenario of the limited training data.","['cs.CV', 'cs.AI']"
Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation,"Generalising deep models to new data from new centres (termed here domains)
remains a challenge. This is largely attributed to shifts in data statistics
(domain shifts) between source and unseen domains. Recently, gradient-based
meta-learning approaches where the training data are split into meta-train and
meta-test sets to simulate and handle the domain shifts during training have
shown improved generalisation performance. However, the current fully
supervised meta-learning approaches are not scalable for medical image
segmentation, where large effort is required to create pixel-wise annotations.
Meanwhile, in a low data regime, the simulated domain shifts may not
approximate the true domain shifts well across source and unseen domains. To
address this problem, we propose a novel semi-supervised meta-learning
framework with disentanglement. We explicitly model the representations related
to domain shifts. Disentangling the representations and combining them to
reconstruct the input image allows unlabeled data to be used to better
approximate the true domain shifts for meta-learning. Hence, the model can
achieve better generalisation performance, especially when there is a limited
amount of labeled data. Experiments show that the proposed method is robust on
different segmentation tasks and achieves state-of-the-art generalisation
performance on two public benchmarks.",['cs.CV']
Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation,"The success of deep learning methods in medical image segmentation tasks
heavily depends on a large amount of labeled data to supervise the training. On
the other hand, the annotation of biomedical images requires domain knowledge
and can be laborious. Recently, contrastive learning has demonstrated great
potential in learning latent representation of images even without any label.
Existing works have explored its application to biomedical image segmentation
where only a small portion of data is labeled, through a pre-training phase
based on self-supervised contrastive learning without using any labels followed
by a supervised fine-tuning phase on the labeled portion of data only. In this
paper, we establish that by including the limited label in formation in the
pre-training phase, it is possible to boost the performance of contrastive
learning. We propose a supervised local contrastive loss that leverages limited
pixel-wise annotation to force pixels with the same label to gather around in
the embedding space. Such loss needs pixel-wise computation which can be
expensive for large images, and we further propose two strategies, downsampling
and block division, to address the issue. We evaluate our methods on two public
biomedical image datasets of different modalities. With different amounts of
labeled data, our methods consistently outperform the state-of-the-art
contrast-based methods and other semi-supervised learning techniques.",['cs.CV']
Direct Estimation of Appearance Models for Segmentation,"Image segmentation algorithms often depend on appearance models that
characterize the distribution of pixel values in different image regions. We
describe a new approach for estimating appearance models directly from an
image, without explicit consideration of the pixels that make up each region.
Our approach is based on novel algebraic expressions that relate local image
statistics to the appearance of spatially coherent regions. We describe two
algorithms that can use the aforementioned algebraic expressions to estimate
appearance models directly from an image. The first algorithm solves a system
of linear and quadratic equations using a least squares formulation. The second
algorithm is a spectral method based on an eigenvector computation. We present
experimental results that demonstrate the proposed methods work well in
practice and lead to effective image segmentation algorithms.","['cs.CV', '68U10, 62M05, 62H30, 65C20']"
MISSFormer: An Effective Medical Image Segmentation Transformer,"The CNN-based methods have achieved impressive results in medical image
segmentation, but it failed to capture the long-range dependencies due to the
inherent locality of convolution operation. Transformer-based methods are
popular in vision tasks recently because of its capacity of long-range
dependencies and get a promising performance. However, it lacks in modeling
local context, although some works attempted to embed convolutional layer to
overcome this problem and achieved some improvement, but it makes the feature
inconsistent and fails to leverage the natural multi-scale features of
hierarchical transformer, which limit the performance of models. In this paper,
taking medical image segmentation as an example, we present MISSFormer, an
effective and powerful Medical Image Segmentation tranSFormer. MISSFormer is a
hierarchical encoder-decoder network and has two appealing designs: 1) A feed
forward network is redesigned with the proposed Enhanced Transformer Block,
which makes features aligned adaptively and enhances the long-range
dependencies and local context. 2) We proposed Enhanced Transformer Context
Bridge, a context bridge with the enhanced transformer block to model the
long-range dependencies and local context of multi-scale features generated by
our hierarchical transformer encoder. Driven by these two designs, the
MISSFormer shows strong capacity to capture more valuable dependencies and
context in medical image segmentation. The experiments on multi-organ and
cardiac segmentation tasks demonstrate the superiority, effectiveness and
robustness of our MISSFormer, the exprimental results of MISSFormer trained
from scratch even outperforms state-of-the-art methods pretrained on ImageNet,
and the core designs can be generalized to other visual segmentation tasks. The
code will be released in Github.",['cs.CV']
Neural Architecture Search in operational context: a remote sensing case-study,"Deep learning has become in recent years a cornerstone tool fueling key
innovations in the industry, such as autonomous driving. To attain good
performances, the neural network architecture used for a given application must
be chosen with care. These architectures are often handcrafted and therefore
prone to human biases and sub-optimal selection. Neural Architecture Search
(NAS) is a framework introduced to mitigate such risks by jointly optimizing
the network architectures and its weights. Albeit its novelty, it was applied
on complex tasks with significant results - e.g. semantic image segmentation.
In this technical paper, we aim to evaluate its ability to tackle a challenging
operational task: semantic segmentation of objects of interest in satellite
imagery. Designing a NAS framework is not trivial and has strong dependencies
to hardware constraints. We therefore motivate our NAS approach selection and
provide corresponding implementation details. We also present novel ideas to
carry out other such use-case studies.","['cs.CV', 'cs.NE']"
Patch-based medical image segmentation using Quantum Tensor Networks,"Tensor networks are efficient factorisations of high dimensional tensors into
a network of lower order tensors. They have been most commonly used to model
entanglement in quantum many-body systems and more recently are witnessing
increased applications in supervised machine learning. In this work, we
formulate image segmentation in a supervised setting with tensor networks. The
key idea is to first lift the pixels in image patches to exponentially high
dimensional feature spaces and using a linear decision hyper-plane to classify
the input pixels into foreground and background classes. The high dimensional
linear model itself is approximated using the matrix product state (MPS) tensor
network. The MPS is weight-shared between the non-overlapping image patches
resulting in our strided tensor network model. The performance of the proposed
model is evaluated on three 2D- and one 3D- biomedical imaging datasets. The
performance of the proposed tensor network segmentation model is compared with
relevant baseline methods. In the 2D experiments, the tensor network model
yeilds competitive performance compared to the baseline methods while being
more resource efficient.",['cs.CV']
Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation,"Simultaneous segmentation of multiple organs from different medical imaging
modalities is a crucial task as it can be utilized for computer-aided
diagnosis, computer-assisted surgery, and therapy planning. Thanks to the
recent advances in deep learning, several deep neural networks for medical
image segmentation have been introduced successfully for this purpose. In this
paper, we focus on learning a deep multi-organ segmentation network that labels
voxels. In particular, we examine the critical choice of a loss function in
order to handle the notorious imbalance problem that plagues both the input and
output of a learning model. The input imbalance refers to the class-imbalance
in the input training samples (i.e., small foreground objects embedded in an
abundance of background voxels, as well as organs of varying sizes). The output
imbalance refers to the imbalance between the false positives and false
negatives of the inference model. In order to tackle both types of imbalance
during training and inference, we introduce a new curriculum learning based
loss function. Specifically, we leverage Dice similarity coefficient to deter
model parameters from being held at bad local minima and at the same time
gradually learn better model parameters by penalizing for false
positives/negatives using a cross entropy term. We evaluated the proposed loss
function on three datasets: whole body positron emission tomography (PET) scans
with 5 target organs, magnetic resonance imaging (MRI) prostate scans, and
ultrasound echocardigraphy images with a single target organ i.e., left
ventricular. We show that a simple network architecture with the proposed
integrative loss function can outperform state-of-the-art methods and results
of the competing methods can be improved when our proposed loss is used.",['cs.CV']
POPCORN: Progressive Pseudo-labeling with Consistency Regularization and Neighboring,"Semi-supervised learning (SSL) uses unlabeled data to compensate for the
scarcity of annotated images and the lack of method generalization to unseen
domains, two usual problems in medical segmentation tasks. In this work, we
propose POPCORN, a novel method combining consistency regularization and
pseudo-labeling designed for image segmentation. The proposed framework uses
high-level regularization to constrain our segmentation model to use similar
latent features for images with similar segmentations. POPCORN estimates a
proximity graph to select data from easiest ones to more difficult ones, in
order to ensure accurate pseudo-labeling and to limit confirmation bias.
Applied to multiple sclerosis lesion segmentation, our method demonstrates
competitive results compared to other state-of-the-art SSL strategies.",['cs.CV']
HCDG: A Hierarchical Consistency Framework for Domain Generalization on Medical Image Segmentation,"Modern deep neural networks struggle to transfer knowledge and generalize
across domains when deploying to real-world applications. Domain generalization
(DG) aims to learn a universal representation from multiple source domains to
improve the network generalization ability on unseen target domains. Previous
DG methods mostly focus on the data-level consistency scheme to advance the
generalization capability of deep networks, without considering the synergistic
regularization of different consistency schemes. In this paper, we present a
novel Hierarchical Consistency framework for Domain Generalization (HCDG) by
ensembling Extrinsic Consistency and Intrinsic Consistency. Particularly, for
Extrinsic Consistency, we leverage the knowledge across multiple source domains
to enforce data-level consistency. Also, we design a novel Amplitude
Gaussian-mixing strategy for Fourier-based data augmentation to enhance such
consistency. For Intrinsic Consistency, we perform task-level consistency for
the same instance under the dual-task form. We evaluate the proposed HCDG
framework on two medical image segmentation tasks, i.e., optic cup/disc
segmentation on fundus images and prostate MRI segmentation. Extensive
experimental results manifest the effectiveness and versatility of our HCDG
framework. Code will be available once accept.",['cs.CV']
Pyramid Medical Transformer for Medical Image Segmentation,"Deep neural networks have been a prevailing technique in the field of medical
image processing. However, the most popular convolutional neural networks
(CNNs) based methods for medical image segmentation are imperfect because they
model long-range dependencies by stacking layers or enlarging filters.
Transformers and the self-attention mechanism are recently proposed to
effectively learn long-range dependencies by modeling all pairs of word-to-word
attention regardless of their positions. The idea has also been extended to the
computer vision field by creating and treating image patches as embeddings.
Considering the computation complexity for whole image self-attention, current
transformer-based models settle for a rigid partitioning scheme that
potentially loses informative relations. Besides, current medical transformers
model global context on full resolution images, leading to unnecessary
computation costs. To address these issues, we developed a novel method to
integrate multi-scale attention and CNN feature extraction using a pyramidal
network architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans
captured multi-range relations by working on multi-resolution images. An
adaptive partitioning scheme was implemented to retain informative relations
and to access different receptive fields efficiently. Experimental results on
three medical image datasets (gland segmentation, MoNuSeg, and HECKTOR
datasets) showed that PMTrans outperformed the latest CNN-based and
transformer-based models for medical image segmentation.",['cs.CV']
Temporally Coherent Person Matting Trained on Fake-Motion Dataset,"We propose a novel neural-network-based method to perform matting of videos
depicting people that does not require additional user input such as trimaps.
Our architecture achieves temporal stability of the resulting alpha mattes by
using motion-estimation-based smoothing of image-segmentation algorithm
outputs, combined with convolutional-LSTM modules on U-Net skip connections.
  We also propose a fake-motion algorithm that generates training clips for the
video-matting network given photos with ground-truth alpha mattes and
background videos. We apply random motion to photos and their mattes to
simulate movement one would find in real videos and composite the result with
the background clips. It lets us train a deep neural network operating on
videos in an absence of a large annotated video dataset and provides
ground-truth training-clip foreground optical flow for use in loss functions.",['cs.CV']
MEAL: Manifold Embedding-based Active Learning,"Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.","['cs.CV', 'cs.LG']"
UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer,"Most recent semantic segmentation methods adopt a U-Net framework with an
encoder-decoder architecture. It is still challenging for U-Net with a simple
skip connection scheme to model the global multi-scale context: 1) Not each
skip connection setting is effective due to the issue of incompatible feature
sets of encoder and decoder stage, even some skip connection negatively
influence the segmentation performance; 2) The original U-Net is worse than the
one without any skip connection on some datasets. Based on our findings, we
propose a new segmentation framework, named UCTransNet (with a proposed CTrans
module in U-Net), from the channel perspective with attention mechanism.
Specifically, the CTrans module is an alternate of the U-Net skip connections,
which consists of a sub-module to conduct the multi-scale Channel Cross fusion
with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention
(named CCA) to guide the fused multi-scale channel-wise information to
effectively connect to the decoder features for eliminating the ambiguity.
Hence, the proposed connection consisting of the CCT and CCA is able to replace
the original skip connection to solve the semantic gaps for an accurate
automatic medical image segmentation. The experimental results suggest that our
UCTransNet produces more precise segmentation performance and achieves
consistent improvements over the state-of-the-art for semantic segmentation
across different datasets and conventional architectures involving transformer
or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.","['cs.CV', 'cs.LG', 'eess.IV']"
Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels,"Detection faults in seismic data is a crucial step for seismic structural
interpretation, reservoir characterization and well placement. Some recent
works regard it as an image segmentation task. The task of image segmentation
requires huge labels, especially 3D seismic data, which has a complex structure
and lots of noise. Therefore, its annotation requires expert experience and a
huge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to
effectively train 3D-CNN by some slices from 3D seismic data, so that the model
can learn the segmentation of 3D seismic data from a few 2D slices. In order to
fully extract information from limited data and suppress seismic noise, we
propose an attention module that can be used for active supervision training
and embedded in the network. The attention heatmap label is generated by the
original label, and letting it supervise the attention module using the
lambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss
function, the method can extract 3D seismic features from a few 2D slice
labels. And it also shows the advanced performance of the attention module,
which can significantly suppress the noise in the seismic data while increasing
the model's sensitivity to the foreground. Finally, on the public test set, we
only use the 2D slice labels training that accounts for 3.3% of the 3D volume
label, and achieve similar performance to the 3D volume label training.","['cs.CV', 'physics.geo-ph']"
"A Survey on Machine Learning Techniques for Auto Labeling of Video, Audio, and Text Data","Machine learning has been utilized to perform tasks in many different domains
such as classification, object detection, image segmentation and natural
language analysis. Data labeling has always been one of the most important
tasks in machine learning. However, labeling large amounts of data increases
the monetary cost in machine learning. As a result, researchers started to
focus on reducing data annotation and labeling costs. Transfer learning was
designed and widely used as an efficient approach that can reasonably reduce
the negative impact of limited data, which in turn, reduces the data
preparation cost. Even transferring previous knowledge from a source domain
reduces the amount of data needed in a target domain. However, large amounts of
annotated data are still demanded to build robust models and improve the
prediction accuracy of the model. Therefore, researchers started to pay more
attention on auto annotation and labeling. In this survey paper, we provide a
review of previous techniques that focuses on optimized data annotation and
labeling for video, audio, and text data.",['cs.LG']
From Contexts to Locality: Ultra-high Resolution Image Segmentation via Locality-aware Contextual Correlation,"Ultra-high resolution image segmentation has raised increasing interests in
recent years due to its realistic applications. In this paper, we innovate the
widely used high-resolution image segmentation pipeline, in which an ultra-high
resolution image is partitioned into regular patches for local segmentation and
then the local results are merged into a high-resolution semantic mask. In
particular, we introduce a novel locality-aware contextual correlation based
segmentation model to process local patches, where the relevance between local
patch and its various contexts are jointly and complementarily utilized to
handle the semantic regions with large variations. Additionally, we present a
contextual semantics refinement network that associates the local segmentation
result with its contextual semantics, and thus is endowed with the ability of
reducing boundary artifacts and refining mask contours during the generation of
final high-resolution mask. Furthermore, in comprehensive experiments, we
demonstrate that our model outperforms other state-of-the-art methods in public
benchmarks. Our released codes are available at
https://github.com/liqiokkk/FCtL.",['cs.CV']
"A Critical Connectivity Radius for Segmenting Randomly-Generated, High Dimensional Data Points","Motivated by a $2$-dimensional (unsupervised) image segmentation task whereby
local regions of pixels are clustered via edge detection methods, a more
general probabilistic mathematical framework is devised. Critical thresholds
are calculated that indicate strong correlation between randomly-generated,
high dimensional data points that have been projected into structures in a
partition of a bounded, $2$-dimensional area, of which, an image is a special
case. A neighbor concept for structures in the partition is defined and a
critical radius is uncovered. Measured from a central structure in localized
regions of the partition, the radius indicates strong, long and short range
correlation in the count of occupied structures. The size of a short interval
of radii is estimated upon which the transition from short-to-long range
correlation is virtually assured, which defines a demarcation of when an image
ceases to be ""interesting"".","['cs.LG', '60D05, 62C99']"
Personalized Image Semantic Segmentation,"Semantic segmentation models trained on public datasets have achieved great
success in recent years. However, these models didn't consider the
personalization issue of segmentation though it is important in practice. In
this paper, we address the problem of personalized image segmentation. The
objective is to generate more accurate segmentation results on unlabeled
personalized images by investigating the data's personalized traits. To open up
future research in this area, we collect a large dataset containing various
users' personalized images called PIS (Personalized Image Semantic
Segmentation). We also survey some recent researches related to this problem
and report their performance on our dataset. Furthermore, by observing the
correlation among a user's personalized images, we propose a baseline method
that incorporates the inter-image context when segmenting certain images.
Extensive experiments show that our method outperforms the existing methods on
the proposed dataset. The code and the PIS dataset will be made publicly
available.",['cs.CV']
Segmenter: Transformer for Semantic Segmentation,"Image segmentation is often ambiguous at the level of individual image
patches and requires contextual information to reach label consensus. In this
paper we introduce Segmenter, a transformer model for semantic segmentation. In
contrast to convolution-based methods, our approach allows to model global
context already at the first layer and throughout the network. We build on the
recent Vision Transformer (ViT) and extend it to semantic segmentation. To do
so, we rely on the output embeddings corresponding to image patches and obtain
class labels from these embeddings with a point-wise linear decoder or a mask
transformer decoder. We leverage models pre-trained for image classification
and show that we can fine-tune them on moderate sized datasets available for
semantic segmentation. The linear decoder allows to obtain excellent results
already, but the performance can be further improved by a mask transformer
generating class masks. We conduct an extensive ablation study to show the
impact of the different parameters, in particular the performance is better for
large models and small patch sizes. Segmenter attains excellent results for
semantic segmentation. It outperforms the state of the art on both ADE20K and
Pascal Context datasets and is competitive on Cityscapes.","['cs.CV', 'cs.AI', 'cs.LG']"
Autonomous Removal of Perspective Distortion of Elevator Button Images based on Corner Detection,"Elevator button recognition is a critical function to realize the autonomous
operation of elevators. However, challenging image conditions and various image
distortions make it difficult to recognize buttons accurately. To fill this
gap, we propose a novel deep learning-based approach, which aims to
autonomously correct perspective distortions of elevator button images based on
button corner detection results. First, we leverage a novel image segmentation
model and the Hough Transform method to obtain button segmentation and button
corner detection results. Then, pixel coordinates of standard button corners
are used as reference features to estimate camera motions for correcting
perspective distortions. Fifteen elevator button images are captured from
different angles of view as the dataset. The experimental results demonstrate
that our proposed approach is capable of estimating camera motions and removing
perspective distortions of elevator button images with high accuracy.","['cs.CV', 'cs.RO']"
Box-Adapt: Domain-Adaptive Medical Image Segmentation using Bounding BoxSupervision,"Deep learning has achieved remarkable success in medicalimage segmentation,
but it usually requires a large numberof images labeled with fine-grained
segmentation masks, andthe annotation of these masks can be very expensive
andtime-consuming. Therefore, recent methods try to use un-supervised domain
adaptation (UDA) methods to borrow in-formation from labeled data from other
datasets (source do-mains) to a new dataset (target domain). However, due tothe
absence of labels in the target domain, the performance ofUDA methods is much
worse than that of the fully supervisedmethod. In this paper, we propose a
weakly supervised do-main adaptation setting, in which we can partially label
newdatasets with bounding boxes, which are easier and cheaperto obtain than
segmentation masks. Accordingly, we proposea new weakly-supervised domain
adaptation method calledBox-Adapt, which fully explores the fine-grained
segmenta-tion mask in the source domain and the weak bounding boxin the target
domain. Our Box-Adapt is a two-stage methodthat first performs joint training
on the source and target do-mains, and then conducts self-training with the
pseudo-labelsof the target domain. We demonstrate the effectiveness of
ourmethod in the liver segmentation task. Weakly supervised do-main adaptation",['cs.CV']
ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation,"Co-occurrent visual pattern makes aggregating contextual information a common
paradigm to enhance the pixel representation for semantic image segmentation.
The existing approaches focus on modeling the context from the perspective of
the whole image, i.e., aggregating the image-level contextual information.
Despite impressive, these methods weaken the significance of the pixel
representations of the same category, i.e., the semantic-level contextual
information. To address this, this paper proposes to augment the pixel
representations by aggregating the image-level and semantic-level contextual
information, respectively. First, an image-level context module is designed to
capture the contextual information for each pixel in the whole image. Second,
we aggregate the representations of the same category for each pixel where the
category regions are learned under the supervision of the ground-truth
segmentation. Third, we compute the similarities between each pixel
representation and the image-level contextual information, the semantic-level
contextual information, respectively. At last, a pixel representation is
augmented by weighted aggregating both the image-level contextual information
and the semantic-level contextual information with the similarities as the
weights. Integrating the image-level and semantic-level context allows this
paper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K,
LIP, COCOStuff and Cityscapes.",['cs.CV']
Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts,"Thanks to their ability to learn flexible data-driven losses, Generative
Adversarial Networks (GANs) are an integral part of many semi- and
weakly-supervised methods for medical image segmentation. GANs jointly optimise
a generator and an adversarial discriminator on a set of training data. After
training has completed, the discriminator is usually discarded and only the
generator is used for inference. But should we discard discriminators? In this
work, we argue that training stable discriminators produces expressive loss
functions that we can re-use at inference to detect and correct segmentation
mistakes. First, we identify key challenges and suggest possible solutions to
make discriminators re-usable at inference. Then, we show that we can combine
discriminators with image reconstruction costs (via decoders) to further
improve the model. Our method is simple and improves the test-time performance
of pre-trained GANs. Moreover, we show that it is compatible with standard
post-processing techniques and it has potentials to be used for Online
Continual Learning. With our work, we open new research avenues for re-using
adversarial discriminators at inference.","['cs.CV', 'eess.IV']"
Mining Contextual Information Beyond Image for Semantic Segmentation,"This paper studies the context aggregation problem in semantic image
segmentation. The existing researches focus on improving the pixel
representations by aggregating the contextual information within individual
images. Though impressive, these methods neglect the significance of the
representations of the pixels of the corresponding class beyond the input
image. To address this, this paper proposes to mine the contextual information
beyond individual images to further augment the pixel representations. We first
set up a feature memory module, which is updated dynamically during training,
to store the dataset-level representations of various categories. Then, we
learn class probability distribution of each pixel representation under the
supervision of the ground-truth segmentation. At last, the representation of
each pixel is augmented by aggregating the dataset-level representations based
on the corresponding class probability distribution. Furthermore, by utilizing
the stored dataset-level representations, we also propose a representation
consistent learning strategy to make the classification head better address
intra-class compactness and inter-class dispersion. The proposed method could
be effortlessly incorporated into existing segmentation frameworks (e.g., FCN,
PSPNet, OCRNet and DeepLabV3) and brings consistent performance improvements.
Mining contextual information beyond image allows us to report state-of-the-art
performance on various benchmarks: ADE20K, LIP, Cityscapes and COCO-Stuff.",['cs.CV']
Fully Transformer Networks for Semantic Image Segmentation,"Transformers have shown impressive performance in various natural language
processing and computer vision tasks, due to the capability of modeling
long-range dependencies. Recent progress has demonstrated to combine such
transformers with CNN-based semantic image segmentation models is very
promising. However, it is not well studied yet on how well a pure transformer
based approach can achieve for image segmentation. In this work, we explore a
novel framework for semantic image segmentation, which is encoder-decoder based
Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid
Group Transformer (PGT) as the encoder for progressively learning hierarchical
features, while reducing the computation complexity of the standard visual
transformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse
semantic-level and spatial-level information from multiple levels of the PGT
encoder for semantic image segmentation. Surprisingly, this simple baseline can
achieve new state-of-the-art results on multiple challenging semantic
segmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The
source code will be released upon the publication of this work.",['cs.CV']
PoissonSeg: Semi-Supervised Few-Shot Medical Image Segmentation via Poisson Learning,"The application of deep learning to medical image segmentation has been
hampered due to the lack of abundant pixel-level annotated data. Few-shot
Semantic Segmentation (FSS) is a promising strategy for breaking the deadlock.
However, a high-performing FSS model still requires sufficient pixel-level
annotated classes for training to avoid overfitting, which leads to its
performance bottleneck in medical image segmentation due to the unmet need for
annotations. Thus, semi-supervised FSS for medical images is accordingly
proposed to utilize unlabeled data for further performance improvement.
Nevertheless, existing semi-supervised FSS methods has two obvious defects: (1)
neglecting the relationship between the labeled and unlabeled data; (2) using
unlabeled data directly for end-to-end training leads to degenerated
representation learning. To address these problems, we propose a novel
semi-supervised FSS framework for medical image segmentation. The proposed
framework employs Poisson learning for modeling data relationship and
propagating supervision signals, and Spatial Consistency Calibration for
encouraging the model to learn more coherent representations. In this process,
unlabeled samples do not involve in end-to-end training, but provide
supervisory information for query image segmentation through graph-based
learning. We conduct extensive experiments on three medical image segmentation
datasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for
MRI and abdominal organs segmentation for CT) to demonstrate the
state-of-the-art performance and broad applicability of the proposed framework.","['cs.CV', 'cs.LG']"
Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and
utilizes the powerful representation of deep neural networks. Recent works have
demonstrated the remarkable successes of deep reinforcement learning in various
domains including finance, medicine, healthcare, video games, robotics, and
computer vision. In this work, we provide a detailed review of recent and
state-of-the-art research advances of deep reinforcement learning in computer
vision. We start with comprehending the theories of deep learning,
reinforcement learning, and deep reinforcement learning. We then propose a
categorization of deep reinforcement learning methodologies and discuss their
advantages and limitations. In particular, we divide deep reinforcement
learning into seven main categories according to their applications in computer
vision, i.e. (i)landmark localization (ii) object detection; (iii) object
tracking; (iv) registration on both 2D image and 3D image volumetric data (v)
image segmentation; (vi) videos analysis; and (vii) other applications. Each of
these categories is further analyzed with reinforcement learning techniques,
network design, and performance. Moreover, we provide a comprehensive analysis
of the existing publicly available datasets and examine source code
availability. Finally, we present some open issues and discuss future research
directions on deep reinforcement learning in computer vision","['cs.CV', 'cs.AI']"
Duo-SegNet: Adversarial Dual-Views for Semi-Supervised Medical Image Segmentation,"Segmentation of images is a long-standing challenge in medical AI. This is
mainly due to the fact that training a neural network to perform image
segmentation requires a significant number of pixel-level annotated data, which
is often unavailable. To address this issue, we propose a semi-supervised image
segmentation technique based on the concept of multi-view learning. In contrast
to the previous art, we introduce an adversarial form of dual-view training and
employ a critic to formulate the learning problem in multi-view training as a
min-max problem. Thorough quantitative and qualitative evaluations on several
datasets indicate that our proposed method outperforms state-of-the-art medical
image segmentation algorithms consistently and comfortably. The code is
publicly available at https://github.com/himashi92/Duo-SegNet",['cs.CV']
Comprehensive Multi-Modal Interactions for Referring Image Segmentation,"We investigate Referring Image Segmentation (RIS), which outputs a
segmentation map corresponding to the given natural language description. To
solve RIS efficiently, we need to understand each word's relationship with
other words, each region in the image to other regions, and cross-modal
alignment between linguistic and visual domains. We argue that one of the
limiting factors in the recent methods is that they do not handle these
interactions simultaneously. To this end, we propose a novel architecture
called JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capture
the inter-modal and intra-modal interactions. The output of JRM is passed
through a novel Cross-Modal Multi-Level Fusion (CMMLF) module which further
refines the segmentation masks by exchanging contextual information across
visual hierarchy through linguistic features acting as a bridge. We present
thorough ablation studies and validate our approach's performance on four
benchmark datasets, showing considerable performance gains over the existing
state-of-the-art methods.",['cs.CV']
Self-Paced Contrastive Learning for Semi-supervised Medical Image Segmentation with Meta-labels,"Pre-training a recognition model with contrastive learning on a large dataset
of unlabeled data has shown great potential to boost the performance of a
downstream task, e.g., image classification. However, in domains such as
medical imaging, collecting unlabeled data can be challenging and expensive. In
this work, we propose to adapt contrastive learning to work with meta-label
annotations, for improving the model's performance in medical image
segmentation even when no additional unlabeled data is available. Meta-labels
such as the location of a 2D slice in a 3D MRI scan or the type of device used,
often come for free during the acquisition process. We use the meta-labels for
pre-training the image encoder as well as to regularize a semi-supervised
training, in which a reduced set of annotated data is used for training.
Finally, to fully exploit the weak annotations, a self-paced learning approach
is used to help the learning and discriminate useful labels from noise. Results
on three different medical image segmentation datasets show that our approach:
i) highly boosts the performance of a model trained on a few scans, ii)
outperforms previous contrastive and semi-supervised approaches, and iii)
reaches close to the performance of a model trained on the full data.",['cs.CV']
Multi-task Federated Learning for Heterogeneous Pancreas Segmentation,"Federated learning (FL) for medical image segmentation becomes more
challenging in multi-task settings where clients might have different
categories of labels represented in their data. For example, one client might
have patient data with ""healthy'' pancreases only while datasets from other
clients may contain cases with pancreatic tumors. The vanilla federated
averaging algorithm makes it possible to obtain more generalizable deep
learning-based segmentation models representing the training data from multiple
institutions without centralizing datasets. However, it might be sub-optimal
for the aforementioned multi-task scenarios. In this paper, we investigate
heterogeneous optimization methods that show improvements for the automated
segmentation of pancreas and pancreatic tumors in abdominal CT images with FL
settings.","['cs.CV', 'I.4.6']"
Membership Inference Attacks are Easier on Difficult Problems,"Membership inference attacks (MIA) try to detect if data samples were used to
train a neural network model, e.g. to detect copyright abuses. We show that
models with higher dimensional input and output are more vulnerable to MIA, and
address in more detail models for image translation and semantic segmentation,
including medical image segmentation. We show that reconstruction-errors can
lead to very effective MIA attacks as they are indicative of memorization.
Unfortunately, reconstruction error alone is less effective at discriminating
between non-predictable images used in training and easy to predict images that
were never seen before. To overcome this, we propose using a novel
predictability error that can be computed for each sample, and its computation
does not require a training set. Our membership error, obtained by subtracting
the predictability error from the reconstruction error, is shown to achieve
high MIA accuracy on an extensive number of benchmarks.","['cs.LG', 'cs.CR']"
Cross-Image Region Mining with Region Prototypical Network for Weakly Supervised Segmentation,"Weakly supervised image segmentation trained with image-level labels usually
suffers from inaccurate coverage of object areas during the generation of the
pseudo groundtruth. This is because the object activation maps are trained with
the classification objective and lack the ability to generalize. To improve the
generality of the objective activation maps, we propose a region prototypical
network RPNet to explore the cross-image object diversity of the training set.
Similar object parts across images are identified via region feature
comparison. Object confidence is propagated between regions to discover new
object areas while background regions are suppressed. Experiments show that the
proposed method generates more complete and accurate pseudo object masks, while
achieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In
addition, we investigate the robustness of the proposed method on reduced
training sets.",['cs.CV']
SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation,"Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, most existing
learning-based approaches usually suffer from limited manually annotated
medical data, which poses a major practical problem for accurate and robust
medical image segmentation. In addition, most existing semi-supervised
approaches are usually not robust compared with the supervised counterparts,
and also lack explicit modeling of geometric structure and semantic
information, both of which limit the segmentation accuracy. In this work, we
present SimCVD, a simple contrastive distillation framework that significantly
advances state-of-the-art voxel-wise representation learning. We first describe
an unsupervised training strategy, which takes two views of an input volume and
predicts their signed distance maps of object boundaries in a contrastive
objective, with only two independent dropout as mask. This simple approach
works surprisingly well, performing on the same level as previous fully
supervised methods with much less labeled data. We hypothesize that dropout can
be viewed as a minimal form of data augmentation and makes the network robust
to representation collapse. Then, we propose to perform structural distillation
by distilling pair-wise similarities. We evaluate SimCVD on two popular
datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT
dataset. The results on the LA dataset demonstrate that, in two types of
labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of
90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to
previous best results. Our method can be trained in an end-to-end fashion,
showing the promise of utilizing SimCVD as a general framework for downstream
tasks, such as medical image synthesis and registration.","['cs.CV', 'cs.AI', 'cs.LG']"
Multi-Slice Dense-Sparse Learning for Efficient Liver and Tumor Segmentation,"Accurate automatic liver and tumor segmentation plays a vital role in
treatment planning and disease monitoring. Recently, deep convolutional neural
network (DCNNs) has obtained tremendous success in 2D and 3D medical image
segmentation. However, 2D DCNNs cannot fully leverage the inter-slice
information, while 3D DCNNs are computationally expensive and memory intensive.
To address these issues, we first propose a novel dense-sparse training flow
from a data perspective, in which, densely adjacent slices and sparsely
adjacent slices are extracted as inputs for regularizing DCNNs, thereby
improving the model performance. Moreover, we design a 2.5D light-weight
nnU-Net from a network perspective, in which, depthwise separable convolutions
are adopted to improve the efficiency. Extensive experiments on the LiTS
dataset have demonstrated the superiority of the proposed method.","['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']"
Real-Time Multi-Modal Semantic Fusion on Unmanned Aerial Vehicles,"Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensors
have tremendous potential for fast autonomous or remote-controlled semantic
scene analysis, e.g., for disaster examination. In this work, we propose a UAV
system for real-time semantic inference and fusion of multiple sensor
modalities. Semantic segmentation of LiDAR scans and RGB images, as well as
object detection on RGB and thermal images, run online onboard the UAV computer
using lightweight CNN architectures and embedded inference accelerators. We
follow a late fusion approach where semantic information from multiple
modalities augments 3D point clouds and image segmentation masks while also
generating an allocentric semantic map. Our system provides augmented semantic
images and point clouds with $\approx\,$9$\,$Hz. We evaluate the integrated
system in real-world experiments in an urban environment.","['cs.CV', 'cs.RO']"
Hierarchical Random Walker Segmentation for Large Volumetric Biomedical Images,"The random walker method for image segmentation is a popular tool for
semi-automatic image segmentation, especially in the biomedical field. However,
its linear asymptotic run time and memory requirements make application to 3D
datasets of increasing sizes impractical. We propose a hierarchical framework
that, to the best of our knowledge, is the first attempt to overcome these
restrictions for the random walker algorithm and achieves sublinear run time
and constant memory complexity. The goal of this framework is -- rather than
improving the segmentation quality compared to the baseline method -- to make
interactive segmentation on out-of-core datasets possible. The method is
evaluated quantitavely on synthetic data and the CT-ORG dataset where the
expected improvements in algorithm run time while maintaining high segmentation
quality are confirmed. The incremental (i.e., interaction update) run time is
demonstrated to be in seconds on a standard PC even for volumes of hundreds of
Gigabytes in size. In a small case study the applicability to large real world
from current biomedical research is demonstrated. An implementation of the
presented method is publicly available in version 5.2 of the widely used volume
rendering and processing software Voreen (https://www.uni-muenster.de/Voreen/).",['cs.CV']
Few-Shot Segmentation with Global and Local Contrastive Learning,"In this work, we address the challenging task of few-shot segmentation.
Previous few-shot segmentation methods mainly employ the information of support
images as guidance for query image segmentation. Although some works propose to
build cross-reference between support and query images, their extraction of
query information still depends on the support images. We here propose to
extract the information from the query itself independently to benefit the
few-shot segmentation task. To this end, we first propose a prior extractor to
learn the query information from the unlabeled images with our proposed
global-local contrastive learning. Then, we extract a set of predetermined
priors via this prior extractor. With the obtained priors, we generate the
prior region maps for query images, which locate the objects, as guidance to
perform cross interaction with support features. In such a way, the extraction
of query information is detached from the support branch, overcoming the
limitation by support, and could obtain more informative query clues to achieve
better interaction. Without bells and whistles, the proposed approach achieves
new state-of-the-art performance for the few-shot segmentation task on
PASCAL-5$^{i}$ and COCO datasets.",['cs.CV']
AASeg: Attention Aware Network for Real Time Semantic Segmentation,"In this paper, we present a new network named Attention Aware Network (AASeg)
for real time semantic image segmentation. Our network incorporates spatial and
channel information using Spatial Attention (SA) and Channel Attention (CA)
modules respectively. It also uses dense local multi-scale context information
using Multi Scale Context (MSC) module. The feature maps are concatenated
individually to produce the final segmentation map. We demonstrate the
effectiveness of our method using a comprehensive analysis, quantitative
experimental results and ablation study using Cityscapes, ADE20K and Camvid
datasets. Our network performs better than most previous architectures with a
74.4\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.","['cs.CV', 'cs.LG', 'eess.IV']"
Medical image segmentation with imperfect 3D bounding boxes,"The development of high quality medical image segmentation algorithms depends
on the availability of large datasets with pixel-level labels. The challenges
of collecting such datasets, especially in case of 3D volumes, motivate to
develop approaches that can learn from other types of labels that are cheap to
obtain, e.g. bounding boxes. We focus on 3D medical images with their
corresponding 3D bounding boxes which are considered as series of per-slice
non-tight 2D bounding boxes. While current weakly-supervised approaches that
use 2D bounding boxes as weak labels can be applied to medical image
segmentation, we show that their success is limited in cases when the
assumption about the tightness of the bounding boxes breaks. We propose a new
bounding box correction framework which is trained on a small set of
pixel-level annotations to improve the tightness of a larger set of non-tight
bounding box annotations. The effectiveness of our solution is demonstrated by
evaluating a known weakly-supervised segmentation approach with and without the
proposed bounding box correction algorithm. When the tightness is improved by
our solution, the results of the weakly-supervised segmentation become much
closer to those of the fully-supervised one.",['cs.CV']
Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation,"Contrastive Learning (CL) is a recent representation learning approach, which
encourages inter-class separability and intra-class compactness in learned
image representations. Since medical images often contain multiple semantic
classes in an image, using CL to learn representations of local features (as
opposed to global) is important. In this work, we present a novel
semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization (CR) scheme, which
works in synergy with CL. It addresses the problem of confirmation bias, and
encourages better clustering in the feature space. We evaluate our method on
four public medical segmentation datasets and a novel histopathology dataset
that we introduce. Our method obtains consistent improvements over
state-of-the-art semi-supervised segmentation approaches for all datasets.",['cs.CV']
Source-Free Domain Adaptation for Image Segmentation,"Domain adaptation (DA) has drawn high interest for its capacity to adapt a
model trained on labeled source data to perform well on unlabeled or weakly
labeled target data from a different domain. Most common DA techniques require
concurrent access to the input images of both the source and target domains.
However, in practice, privacy concerns often impede the availability of source
images in the adaptation phase. This is a very frequent DA scenario in medical
imaging, where, for instance, the source and target images could come from
different clinical sites. We introduce a source-free domain adaptation for
image segmentation. Our formulation is based on minimizing a label-free entropy
loss defined over target-domain data, which we further guide with a
domain-invariant prior on the segmentation regions. Many priors can be derived
from anatomical information. Here, a class ratio prior is estimated from
anatomical knowledge and integrated in the form of a Kullback Leibler (KL)
divergence in our overall loss function. Furthermore, we motivate our overall
loss with an interesting link to maximizing the mutual information between the
target images and their label predictions. We show the effectiveness of our
prior aware entropy minimization in a variety of domain-adaptation scenarios,
with different modalities and applications, including spine, prostate, and
cardiac segmentation. Our method yields comparable results to several state of
the art adaptation techniques, despite having access to much less information,
as the source images are entirely absent in our adaptation phase. Our
straightforward adaptation strategy uses only one network, contrary to popular
adversarial techniques, which are not applicable to a source-free DA setting.
Our framework can be readily used in a breadth of segmentation problems, and
our code is publicly available: https://github.com/mathilde-b/SFDA",['cs.CV']
Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images,"Semantic segmentation of medical images is an essential first step in
computer-aided diagnosis systems for many applications. However, given many
disparate imaging modalities and inherent variations in the patient data, it is
difficult to consistently achieve high accuracy using modern deep neural
networks (DNNs). This has led researchers to propose interactive image
segmentation techniques where a medical expert can interactively correct the
output of a DNN to the desired accuracy. However, these techniques often need
separate training data with the associated human interactions, and do not
generalize to various diseases, and types of medical images. In this paper, we
suggest a novel conditional inference technique for DNNs which takes the
intervention by a medical expert as test time constraints and performs
inference conditioned upon these constraints. Our technique is generic can be
used for medical images from any modality. Unlike other methods, our approach
can correct multiple structures simultaneously and add structures missed at
initial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and
12.4 times in user annotation time than full human annotation for the nucleus,
multiple cells, liver and tumor, organ, and brain segmentation respectively. We
report a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to other
interactive segmentation techniques. Our method can be useful to clinicians for
diagnosis and post-surgical follow-up with minimal intervention from the
medical expert. The source-code and the detailed results are available here
[1].","['cs.CV', '49-06 (Primary), 49-11(Secondary)', 'I.4.6; I.5.1']"
Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction,"Recent advances in brain clearing and imaging have made it possible to image
entire mammalian brains at sub-micron resolution. These images offer the
potential to assemble brain-wide atlases of projection neuron morphology, but
manual neuron reconstruction remains a bottleneck. In this paper we present a
probabilistic method which combines a hidden Markov state process that encodes
neuron geometric properties with a random field appearance model of the
flourescence process. Our method utilizes dynamic programming to efficiently
compute the global maximizers of what we call the ""most probable"" neuron path.
We applied our algorithm to the output of image segmentation models where false
negatives severed neuronal processes, and showed that it can follow axons in
the presence of noise or nearby neurons. Our method has the potential to be
integrated into a semi or fully automated reconstruction pipeline.
Additionally, it creates a framework for conditioning the probability to fixed
start and endpoints through which users can intervene with hard constraints to,
for example, rule out certain reconstructions, or assign axons to particular
cell bodies.",['cs.CV']
Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical Image Segmentation with Normalizing Flows,"Quantifying uncertainty in medical image segmentation applications is
essential, as it is often connected to vital decision-making. Compelling
attempts have been made in quantifying the uncertainty in image segmentation
architectures, e.g. to learn a density segmentation model conditioned on the
input image. Typical work in this field restricts these learnt densities to be
strictly Gaussian. In this paper, we propose to use a more flexible approach by
introducing Normalizing Flows (NFs), which enables the learnt densities to be
more complex and facilitate more accurate modeling for uncertainty. We prove
this hypothesis by adopting the Probabilistic U-Net and augmenting the
posterior density with an NF, allowing it to be more expressive. Our
qualitative as well as quantitative (GED and IoU) evaluations on the
multi-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation
datasets, respectively, show a clear improvement. This is mostly apparent in
the quantification of aleatoric uncertainty and the increased predictive
performance of up to 14 percent. This result strongly indicates that a more
flexible density model should be seriously considered in architectures that
attempt to capture segmentation ambiguity through density modeling. The benefit
of this improved modeling will increase human confidence in annotation and
segmentation, and enable eager adoption of the technology in practice.","['cs.CV', 'cs.LG']"
Shape Modeling with Spline Partitions,"Shape modelling (with methods that output shapes) is a new and important task
in Bayesian nonparametrics and bioinformatics. In this work, we focus on
Bayesian nonparametric methods for capturing shapes by partitioning a space
using curves. In related work, the classical Mondrian process is used to
partition spaces recursively with axis-aligned cuts, and is widely applied in
multi-dimensional and relational data. The Mondrian process outputs
hyper-rectangles. Recently, the random tessellation process was introduced as a
generalization of the Mondrian process, partitioning a domain with non-axis
aligned cuts in an arbitrary dimensional space, and outputting polytopes.
Motivated by these processes, in this work, we propose a novel parallelized
Bayesian nonparametric approach to partition a domain with curves, enabling
complex data-shapes to be acquired. We apply our method to HIV-1-infected human
macrophage image dataset, and also simulated datasets sets to illustrate our
approach. We compare to support vector machines, random forests and
state-of-the-art computer vision methods such as simple linear iterative
clustering super pixel image segmentation. We develop an R package that is
available at
\url{https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions}.","['stat.ML', 'cs.LG']"
"Distribution-Free, Risk-Controlling Prediction Sets","While improving prediction accuracy has been the focus of machine learning in
recent years, this alone does not suffice for reliable decision-making.
Deploying learning systems in consequential settings also requires calibrating
and communicating the uncertainty of predictions. To convey instance-wise
uncertainty for prediction tasks, we show how to generate set-valued
predictions from a black-box predictor that control the expected loss on future
test points at a user-specified level. Our approach provides explicit
finite-sample guarantees for any dataset by using a holdout set to calibrate
the size of the prediction sets. This framework enables simple,
distribution-free, rigorous error control for many tasks, and we demonstrate it
in five large-scale machine learning problems: (1) classification problems
where some mistakes are more costly than others; (2) multi-label
classification, where each observation has multiple associated labels; (3)
classification problems where the labels have a hierarchical structure; (4)
image segmentation, where we wish to predict a set of pixels containing an
object of interest; and (5) protein structure prediction. Lastly, we discuss
extensions to uncertainty quantification for ranking, metric learning and
distributionally robust learning.","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ME', 'stat.ML']"
Recurrent Mask Refinement for Few-Shot Medical Image Segmentation,"Although having achieved great success in medical image segmentation, deep
convolutional neural networks usually require a large dataset with manual
annotations for training and are difficult to generalize to unseen classes.
Few-shot learning has the potential to address these challenges by learning new
classes from only a few labeled examples. In this work, we propose a new
framework for few-shot medical image segmentation based on prototypical
networks. Our innovation lies in the design of two key modules: 1) a context
relation encoder (CRE) that uses correlation to capture local relation features
between foreground and background regions; and 2) a recurrent mask refinement
module that repeatedly uses the CRE and a prototypical network to recapture the
change of context relationship and refine the segmentation mask iteratively.
Experiments on two abdomen CT datasets and an abdomen MRI dataset show the
proposed method obtains substantial improvement over the state-of-the-art
methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.
Code is publicly available.",['cs.CV']
Transductive image segmentation: Self-training and effect of uncertainty estimation,"Semi-supervised learning (SSL) uses unlabeled data during training to learn
better models. Previous studies on SSL for medical image segmentation focused
mostly on improving model generalization to unseen data. In some applications,
however, our primary interest is not generalization but to obtain optimal
predictions on a specific unlabeled database that is fully available during
model development. Examples include population studies for extracting imaging
phenotypes. This work investigates an often overlooked aspect of SSL,
transduction. It focuses on the quality of predictions made on the unlabeled
data of interest when they are included for optimization during training,
rather than improving generalization. We focus on the self-training framework
and explore its potential for transduction. We analyze it through the lens of
Information Gain and reveal that learning benefits from the use of calibrated
or under-confident models. Our extensive experiments on a large MRI database
for multi-class segmentation of traumatic brain lesions shows promising results
when comparing transductive with inductive predictions. We believe this study
will inspire further research on transductive learning, a well-suited paradigm
for medical image analysis.",['cs.CV']
Visual Boundary Knowledge Translation for Foreground Segmentation,"When confronted with objects of unknown types in an image, humans can
effortlessly and precisely tell their visual boundaries. This recognition
mechanism and underlying generalization capability seem to contrast to
state-of-the-art image segmentation networks that rely on large-scale
category-aware annotated training samples. In this paper, we make an attempt
towards building models that explicitly account for visual boundary knowledge,
in hope to reduce the training effort on segmenting unseen categories.
Specifically, we investigate a new task termed as Boundary Knowledge
Translation (BKT). Given a set of fully labeled categories, BKT aims to
translate the visual boundary knowledge learned from the labeled categories, to
a set of novel categories, each of which is provided only a few labeled
samples. To this end, we propose a Translation Segmentation Network
(Trans-Net), which comprises a segmentation network and two boundary
discriminators. The segmentation network, combined with a boundary-aware
self-supervised mechanism, is devised to conduct foreground segmentation, while
the two discriminators work together in an adversarial manner to ensure an
accurate segmentation of the novel categories under light supervision.
Exhaustive experiments demonstrate that, with only tens of labeled samples as
guidance, Trans-Net achieves close results on par with fully supervised
methods.",['cs.CV']
OPFython: A Python-Inspired Optimum-Path Forest Classifier,"Machine learning techniques have been paramount throughout the last years,
being applied in a wide range of tasks, such as classification, object
recognition, person identification, and image segmentation. Nevertheless,
conventional classification algorithms, e.g., Logistic Regression, Decision
Trees, and Bayesian classifiers, might lack complexity and diversity, not
suitable when dealing with real-world data. A recent graph-inspired classifier,
known as the Optimum-Path Forest, has proven to be a state-of-the-art
technique, comparable to Support Vector Machines and even surpassing it in some
tasks. This paper proposes a Python-based Optimum-Path Forest framework,
denoted as OPFython, where all of its functions and classes are based upon the
original C language implementation. Additionally, as OPFython is a Python-based
library, it provides a more friendly environment and a faster prototyping
workspace than the C language.","['cs.LG', 'cs.CV', 'stat.ML', '68T01', 'I.2.0; I.5.0']"
"Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation","Deep learning has not been routinely employed for semantic segmentation of
seabed environment for synthetic aperture sonar (SAS) imagery due to the
implicit need of abundant training data such methods necessitate. Abundant
training data, specifically pixel-level labels for all images, is usually not
available for SAS imagery due to the complex logistics (e.g., diver survey,
chase boat, precision position information) needed for obtaining accurate
ground-truth. Many hand-crafted feature based algorithms have been proposed to
segment SAS in an unsupervised fashion. However, there is still room for
improvement as the feature extraction step of these methods is fixed. In this
work, we present a new iterative unsupervised algorithm for learning deep
features for SAS image segmentation. Our proposed algorithm alternates between
clustering superpixels and updating the parameters of a convolutional neural
network (CNN) so that the feature extraction for image segmentation can be
optimized. We demonstrate the efficacy of our method on a realistic benchmark
dataset. Our results show that the performance of our proposed method is
considerably better than current state-of-the-art methods in SAS image
segmentation.",['cs.CV']
Open-World Entity Segmentation,"We introduce a new image segmentation task, termed Entity Segmentation (ES)
with the aim to segment all visual entities in an image without considering
semantic category labels. It has many practical applications in image
manipulation/editing where the segmentation mask quality is typically crucial
but category labels are less important. In this setting, all
semantically-meaningful segments are equally treated as categoryless entities
and there is no thing-stuff distinction. Based on our unified entity
representation, we propose a center-based entity segmentation framework with
two novel modules to improve mask quality. Experimentally, both our new task
and framework demonstrate superior advantages as against existing work. In
particular, ES enables the following: (1) merging multiple datasets to form a
large training set without the need to resolve label conflicts; (2) any model
trained on one dataset can generalize exceptionally well to other datasets with
unseen domains. Our code is made publicly available at
https://github.com/dvlab-research/Entity.","['cs.CV', 'cs.LG']"
Mapping Vulnerable Populations with AI,"Humanitarian actions require accurate information to efficiently delegate
support operations. Such information can be maps of building footprints,
building functions, and population densities. While the access to this
information is comparably easy in industrialized countries thanks to reliable
census data and national geo-data infrastructures, this is not the case for
developing countries, where that data is often incomplete or outdated. Building
maps derived from remote sensing images may partially remedy this challenge in
such countries, but are not always accurate due to different landscape
configurations and lack of validation data. Even when they exist, building
footprint layers usually do not reveal more fine-grained building properties,
such as the number of stories or the building's function (e.g., office,
residential, school, etc.). In this project we aim to automate building
footprint and function mapping using heterogeneous data sources. In a first
step, we intend to delineate buildings from satellite data, using deep learning
models for semantic image segmentation. Building functions shall be retrieved
by parsing social media data like for instance tweets, as well as ground-based
imagery, to automatically identify different buildings functions and retrieve
further information such as the number of building stories. Building maps
augmented with those additional attributes make it possible to derive more
accurate population density maps, needed to support the targeted provision of
humanitarian aid.","['cs.CV', 'eess.IV']"
"Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition","The latest advances in computer-assisted precision medicine are making it
feasible to move from population-wide models that are useful to discover
aggregate patterns that hold for group-based analysis to patient-specific
models that can drive patient-specific decisions with regard to treatment
choices, and predictions of outcomes of treatment. Body Composition is
recognized as an important driver and risk factor for a wide variety of
diseases, as well as a predictor of individual patient-specific clinical
outcomes to treatment choices or surgical interventions. 3D CT images are
routinely acquired in the oncological worklows and deliver accurate rendering
of internal anatomy and therefore can be used opportunistically to assess the
amount of skeletal muscle and adipose tissue compartments. Powerful tools of
artificial intelligence such as deep learning are making it feasible now to
segment the entire 3D image and generate accurate measurements of all internal
anatomy. These will enable the overcoming of the severe bottleneck that existed
previously, namely, the need for manual segmentation, which was prohibitive to
scale to the hundreds of 2D axial slices that made up a 3D volumetric image.
Automated tools such as presented here will now enable harvesting whole-body
measurements from 3D CT or MRI images, leading to a new era of discovery of the
drivers of various diseases based on individual tissue, organ volume, shape,
and functional status. These measurements were hitherto unavailable thereby
limiting the field to a very small and limited subset. These discoveries and
the potential to perform individual image segmentation with high speed and
accuracy are likely to lead to the incorporation of these 3D measures into
individual specific treatment planning models related to nutrition, aging,
chemotoxicity, surgery and survival after the onset of a major disease such as
cancer.","['cs.CV', 'q-bio.TO']"
Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP,"We introduce a method that allows to automatically segment images into
semantically meaningful regions without human supervision. Derived regions are
consistent across different images and coincide with human-defined semantic
classes on some datasets. In cases where semantic regions might be hard for
human to define and consistently label, our method is still able to find
meaningful and consistent semantic classes. In our work, we use pretrained
StyleGAN2~\cite{karras2020analyzing} generative model: clustering in the
feature space of the generative model allows to discover semantic classes. Once
classes are discovered, a synthetic dataset with generated images and
corresponding segmentation masks can be created. After that a segmentation
model is trained on the synthetic dataset and is able to generalize to real
images. Additionally, by using CLIP~\cite{radford2021learning} we are able to
use prompts defined in a natural language to discover some desired semantic
classes. We test our method on publicly available datasets and show
state-of-the-art results.",['cs.CV']
Crosslink-Net: Double-branch Encoder Segmentation Network via Fusing Vertical and Horizontal Convolutions,"Accurate image segmentation plays a crucial role in medical image analysis,
yet it faces great challenges of various shapes, diverse sizes, and blurry
boundaries. To address these difficulties, square kernel-based encoder-decoder
architecture has been proposed and widely used, but its performance remains
still unsatisfactory. To further cope with these challenges, we present a novel
double-branch encoder architecture. Our architecture is inspired by two
observations: 1) Since the discrimination of features learned via square
convolutional kernels needs to be further improved, we propose to utilize
non-square vertical and horizontal convolutional kernels in the double-branch
encoder, so features learned by the two branches can be expected to complement
each other. 2) Considering that spatial attention can help models to better
focus on the target region in a large-sized image, we develop an attention loss
to further emphasize the segmentation on small-sized targets. Together, the
above two schemes give rise to a novel double-branch encoder segmentation
framework for medical image segmentation, namely Crosslink-Net. The experiments
validate the effectiveness of our model on four datasets. The code is released
at https://github.com/Qianyu1226/Crosslink-Net.","['cs.CV', 'cs.AI', '68T07', 'I.4.6']"
Reservoir Computing Approach for Gray Images Segmentation,"The paper proposes a novel approach for gray scale images segmentation. It is
based on multiple features extraction from single feature per image pixel,
namely its intensity value, using Echo state network. The newly extracted
features -- reservoir equilibrium states -- reveal hidden image characteristics
that improve its segmentation via a clustering algorithm. Moreover, it was
demonstrated that the intrinsic plasticity tuning of reservoir fits its
equilibrium states to the original image intensity distribution thus allowing
for its better segmentation. The proposed approach is tested on the benchmark
image Lena.","['cs.CV', 'cs.LG', 'eess.IV']"
Superpixel-guided Iterative Learning from Noisy Labels for Medical Image Segmentation,"Learning segmentation from noisy labels is an important task for medical
image analysis due to the difficulty in acquiring highquality annotations. Most
existing methods neglect the pixel correlation and structural prior in
segmentation, often producing noisy predictions around object boundaries. To
address this, we adopt a superpixel representation and develop a robust
iterative learning strategy that combines noise-aware training of segmentation
network and noisy label refinement, both guided by the superpixels. This design
enables us to exploit the structural constraints in segmentation labels and
effectively mitigate the impact of label noise in learning. Experiments on two
benchmarks show that our method outperforms recent state-of-the-art approaches,
and achieves superior robustness in a wide range of label noises. Code is
available at https://github.com/gaozhitong/SP_guided_Noisy_Label_Seg.",['cs.CV']
Weighted Intersection over Union (wIoU): A New Evaluation Metric for Image Segmentation,"In this paper, we propose a novel evaluation metric for performance
evaluation of semantic segmentation. In recent years, many studies have tried
to train pixel-level classifiers on large-scale image datasets to perform
accurate semantic segmentation. The goal of semantic segmentation is to assign
a class label of each pixel in the scene. It has various potential applications
in computer vision fields e.g., object detection, classification, scene
understanding and Etc. To validate the proposed wIoU evaluation metric, we
tested state-of-the art methods on public benchmark datasets (e.g., KITTI)
based on the proposed wIoU metric and compared with other conventional
evaluation metrics.",['cs.CV']
Vessel-CAPTCHA: an efficient learning framework for vessel annotation and segmentation,"Deep learning techniques for 3D brain vessel image segmentation have not been
as successful as in the segmentation of other organs and tissues. This can be
explained by two factors. First, deep learning techniques tend to show poor
performances at the segmentation of relatively small objects compared to the
size of the full image. Second, due to the complexity of vascular trees and the
small size of vessels, it is challenging to obtain the amount of annotated
training data typically needed by deep learning methods. To address these
problems, we propose a novel annotation-efficient deep learning vessel
segmentation framework. The framework avoids pixel-wise annotations, only
requiring weak patch-level labels to discriminate between vessel and non-vessel
2D patches in the training set, in a setup similar to the CAPTCHAs used to
differentiate humans from bots in web applications. The user-provided weak
annotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labels
for vessels and background in each patch, which are used to train a
segmentation network, and 2) to train a classifier network. The classifier
network allows to generate additional weak patch labels, further reducing the
annotation burden, and it acts as a noise filter for poor quality images. We
use this framework for the segmentation of the cerebrovascular tree in
Time-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The
results show that the framework achieves state-of-the-art accuracy, while
reducing the annotation time by ~77% w.r.t. learning-based segmentation methods
using pixel-wise labels for training.",['cs.CV']
LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation,"Medical image segmentation plays an essential role in developing
computer-assisted diagnosis and therapy systems, yet still faces many
challenges. In the past few years, the popular encoder-decoder architectures
based on CNNs (e.g., U-Net) have been successfully applied in the task of
medical image segmentation. However, due to the locality of convolution
operations, they demonstrate limitations in learning global context and
long-range spatial relations. Recently, several researchers try to introduce
transformers to both the encoder and decoder components with promising results,
but the efficiency requires further improvement due to the high computational
complexity of transformers. In this paper, we propose LeViT-UNet, which
integrates a LeViT Transformer module into the U-Net architecture, for fast and
accurate medical image segmentation. Specifically, we use LeViT as the encoder
of the LeViT-UNet, which better trades off the accuracy and efficiency of the
Transformer block. Moreover, multi-scale feature maps from transformer blocks
and convolutional blocks of LeViT are passed into the decoder via
skip-connection, which can effectively reuse the spatial information of the
feature maps. Our experiments indicate that the proposed LeViT-UNet achieves
better performance comparing to various competing methods on several
challenging medical image segmentation benchmarks including Synapse and ACDC.
Code and models will be publicly available at
https://github.com/apple1986/LeViT_UNet.",['cs.CV']
Double Similarity Distillation for Semantic Image Segmentation,"The balance between high accuracy and high speed has always been a
challenging task in semantic image segmentation. Compact segmentation networks
are more widely used in the case of limited resources, while their performances
are constrained. In this paper, motivated by the residual learning and global
aggregation, we propose a simple yet general and effective knowledge
distillation framework called double similarity distillation (DSD) to improve
the classification accuracy of all existing compact networks by capturing the
similarity knowledge in pixel and category dimensions, respectively.
Specifically, we propose a pixel-wise similarity distillation (PSD) module that
utilizes residual attention maps to capture more detailed spatial dependencies
across multiple layers. Compared with exiting methods, the PSD module greatly
reduces the amount of calculation and is easy to expand. Furthermore,
considering the differences in characteristics between semantic segmentation
task and other computer vision tasks, we propose a category-wise similarity
distillation (CSD) module, which can help the compact segmentation network
strengthen the global category correlation by constructing the correlation
matrix. Combining these two modules, DSD framework has no extra parameters and
only a minimal increase in FLOPs. Extensive experiments on four challenging
datasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that
DSD outperforms current state-of-the-art methods, proving its effectiveness and
generality. The code and models will be publicly available.",['cs.CV']
A Weighted Difference of Anisotropic and Isotropic Total Variation for Relaxed Mumford-Shah Color and Multiphase Image Segmentation,"In a class of piecewise-constant image segmentation models, we propose to
incorporate a weighted difference of anisotropic and isotropic total variation
(AITV) to regularize the partition boundaries in an image. In particular, we
replace the total variation regularization in the Chan-Vese segmentation model
and a fuzzy region competition model by the proposed AITV. To deal with the
nonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), in
which the subproblems can be minimized by the primal-dual hybrid gradient
method with linesearch. The convergence of the DCA scheme is analyzed. In
addition, a generalization to color image segmentation is discussed. In the
numerical experiments, we compare the proposed models with the classic convex
approaches and the two-stage segmentation methods (smoothing and then
thresholding) on various images, showing that our models are effective in image
segmentation and robust with respect to impulsive noises.",['cs.CV']
What Image Features Boost Housing Market Predictions?,"The attractiveness of a property is one of the most interesting, yet
challenging, categories to model. Image characteristics are used to describe
certain attributes, and to examine the influence of visual factors on the price
or timeframe of the listing. In this paper, we propose a set of techniques for
the extraction of visual features for efficient numerical inclusion in
modern-day predictive algorithms. We discuss techniques such as Shannon's
entropy, calculating the center of gravity, employing image segmentation, and
using Convolutional Neural Networks. After comparing these techniques as
applied to a set of property-related images (indoor, outdoor, and satellite),
we conclude the following: (i) the entropy is the most efficient single-digit
visual measure for housing price prediction; (ii) image segmentation is the
most important visual feature for the prediction of housing lifespan; and (iii)
deep image features can be used to quantify interior characteristics and
contribute to captivation modeling. The set of 40 image features selected here
carries a significant amount of predictive power and outperforms some of the
strongest metadata predictors. Without any need to replace a human expert in a
real-estate appraisal process, we conclude that the techniques presented in
this paper can efficiently describe visible characteristics, thus introducing
perceived attractiveness as a quantitative measure into the predictive modeling
of housing.","['cs.CV', 'cs.LG']"
Learning from Partially Overlapping Labels: Image Segmentation under Annotation Shift,"Scarcity of high quality annotated images remains a limiting factor for
training accurate image segmentation models. While more and more annotated
datasets become publicly available, the number of samples in each individual
database is often small. Combining different databases to create larger amounts
of training data is appealing yet challenging due to the heterogeneity as a
result of differences in data acquisition and annotation processes, often
yielding incompatible or even conflicting information. In this paper, we
investigate and propose several strategies for learning from partially
overlapping labels in the context of abdominal organ segmentation. We find that
combining a semi-supervised approach with an adaptive cross entropy loss can
successfully exploit heterogeneously annotated data and substantially improve
segmentation accuracy compared to baseline and alternative approaches.",['cs.CV']
TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation,"In recent years, computer-aided diagnosis has become an increasingly popular
topic. Methods based on convolutional neural networks have achieved good
performance in medical image segmentation and classification. Due to the
limitations of the convolution operation, the long-term spatial features are
often not accurately obtained. Hence, we propose a TransClaw U-Net network
structure, which combines the convolution operation with the transformer
operation in the encoding part. The convolution part is applied for extracting
the shallow spatial features to facilitate the recovery of the image resolution
after upsampling. The transformer part is used to encode the patches, and the
self-attention mechanism is used to obtain global information between
sequences. The decoding part retains the bottom upsampling structure for better
detail segmentation performance. The experimental results on Synapse
Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net
is better than other network structures. The ablation experiments also prove
the generalization performance of TransClaw U-Net.","['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']"
Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images,"Convolutional neural networks (CNNs) have led to significant improvements in
tasks involving semantic segmentation of images. CNNs are vulnerable in the
area of biomedical image segmentation because of distributional gap between two
source and target domains with different data modalities which leads to domain
shift. Domain shift makes data annotations in new modalities necessary because
models must be retrained from scratch. Unsupervised domain adaptation (UDA) is
proposed to adapt a model to new modalities using solely unlabeled target
domain data. Common UDA algorithms require access to data points in the source
domain which may not be feasible in medical imaging due to privacy concerns. In
this work, we develop an algorithm for UDA in a privacy-constrained setting,
where the source domain data is inaccessible. Our idea is based on encoding the
information from the source samples into a prototypical distribution that is
used as an intermediate distribution for aligning the target domain
distribution with the source domain distribution. We demonstrate the
effectiveness of our algorithm by comparing it to state-of-the-art medical
image semantic segmentation approaches on two medical image semantic
segmentation datasets.","['cs.CV', 'cs.CR', 'cs.LG', 'eess.IV']"
A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation,"The segmentation of medical images is a fundamental step in automated
clinical decision support systems. Existing medical image segmentation methods
based on supervised deep learning, however, remain problematic because of their
reliance on large amounts of labelled training data. Although medical imaging
data repositories continue to expand, there has not been a commensurate
increase in the amount of annotated data. Hence, we propose a new spatial
guided self-supervised clustering network (SGSCN) for medical image
segmentation, where we introduce multiple loss functions designed to aid in
grouping image pixels that are spatially connected and have similar feature
representations. It iteratively learns feature representations and clustering
assignment of each pixel in an end-to-end fashion from a single image. We also
propose a context-based consistency loss that better delineates the shape and
boundaries of image regions. It enforces all the pixels belonging to a cluster
to be spatially close to the cluster centre. We evaluated our method on 2
public medical image datasets and compared it to existing conventional and
self-supervised clustering methods. Experimental results show that our method
was most accurate for medical image segmentation.",['cs.CV']
Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation,"Domain Adaptation (DA) methods are widely used in medical image segmentation
tasks to tackle the problem of differently distributed train (source) and test
(target) data. We consider the supervised DA task with a limited number of
annotated samples from the target domain. It corresponds to one of the most
relevant clinical setups: building a sufficiently accurate model on the minimum
possible amount of annotated data. Existing methods mostly fine-tune specific
layers of the pretrained Convolutional Neural Network (CNN). However, there is
no consensus on which layers are better to fine-tune, e.g. the first layers for
images with low-level domain shift or the deeper layers for images with
high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture
that automatically chooses the layers which should be optimally fine-tuned.
More specifically, on the target domain, our method additionally learns the
policy that indicates whether a specific layer should be fine-tuned or reused
from the pretrained network. We show that our method performs at the same level
as the best of the nonflexible fine-tuning methods even under the extreme
scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a
layer-wise visualization of the domain shift impact on the network, which could
be further used to develop robust domain generalization methods. In order to
extensively evaluate SpotTUnet performance, we use a publicly available dataset
of brain MR images (CC359), characterized by explicit domain shift. We release
a reproducible experimental pipeline.","['cs.CV', 'cs.LG', 'eess.IV']"
Hierarchical Self-Supervised Learning for Medical Image Segmentation Based on Multi-Domain Data Aggregation,"A large labeled dataset is a key to the success of supervised deep learning,
but for medical image segmentation, it is highly challenging to obtain
sufficient annotated images for model training. In many scenarios, unannotated
images are abundant and easy to acquire. Self-supervised learning (SSL) has
shown great potentials in exploiting raw data information and representation
learning. In this paper, we propose Hierarchical Self-Supervised Learning
(HSSL), a new self-supervised framework that boosts medical image segmentation
by making good use of unannotated data. Unlike the current literature on
task-specific self-supervised pretraining followed by supervised fine-tuning,
we utilize SSL to learn task-agnostic knowledge from heterogeneous data for
various medical image segmentation tasks. Specifically, we first aggregate a
dataset from several medical challenges, then pre-train the network in a
self-supervised manner, and finally fine-tune on labeled data. We develop a new
loss function by combining contrastive loss and classification loss and
pretrain an encoder-decoder architecture for segmentation tasks. Our extensive
experiments show that multi-domain joint pre-training benefits downstream
segmentation tasks and outperforms single-domain pre-training significantly.
Compared to learning from scratch, our new method yields better performance on
various tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated
data). With limited amounts of training data, our method can substantially
bridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of
annotated data).",['cs.CV']
TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation,"Medical image segmentation - the prerequisite of numerous clinical needs -
has been significantly prospered by recent advances in convolutional neural
networks (CNNs). However, it exhibits general limitations on modeling explicit
long-range relation, and existing cures, resorting to building deep encoders
along with aggressive downsampling operations, leads to redundant deepened
networks and loss of localized details. Hence, the segmentation task awaits a
better solution to improve the efficiency of modeling global contexts while
maintaining a strong grasp of low-level details. In this paper, we propose a
novel parallel-in-branch architecture, TransFuse, to address this challenge.
TransFuse combines Transformers and CNNs in a parallel style, where both global
dependency and low-level spatial details can be efficiently captured in a much
shallower manner. Besides, a novel fusion technique - BiFusion module is
created to efficiently fuse the multi-level features from both branches.
Extensive experiments demonstrate that TransFuse achieves the newest
state-of-the-art results on both 2D and 3D medical image sets including polyp,
skin lesion, hip, and prostate segmentation, with significant parameter
decrease and inference speed improvement.","['cs.CV', 'cs.AI']"
Towards Robust General Medical Image Segmentation,"The reliability of Deep Learning systems depends on their accuracy but also
on their robustness against adversarial perturbations to the input data.
Several attacks and defenses have been proposed to improve the performance of
Deep Neural Networks under the presence of adversarial noise in the natural
image domain. However, robustness in computer-aided diagnosis for volumetric
data has only been explored for specific tasks and with limited attacks. We
propose a new framework to assess the robustness of general medical image
segmentation systems. Our contributions are two-fold: (i) we propose a new
benchmark to evaluate robustness in the context of the Medical Segmentation
Decathlon (MSD) by extending the recent AutoAttack natural image classification
framework to the domain of volumetric data segmentation, and (ii) we present a
novel lattice architecture for RObust Generic medical image segmentation (ROG).
Our results show that ROG is capable of generalizing across different tasks of
the MSD and largely surpasses the state-of-the-art under sophisticated
adversarial attacks.",['cs.CV']
Medical Matting: A New Perspective on Medical Segmentation with Uncertainty,"In medical image segmentation, it is difficult to mark ambiguous areas
accurately with binary masks, especially when dealing with small lesions.
Therefore, it is a challenge for radiologists to reach a consensus by using
binary masks under the condition of multiple annotations. However, these areas
may contain anatomical structures that are conducive to diagnosis. Uncertainty
is introduced to study these situations. Nevertheless, the uncertainty is
usually measured by the variances between predictions in a multiple trial way.
It is not intuitive, and there is no exact correspondence in the image.
Inspired by image matting, we introduce matting as a soft segmentation method
and a new perspective to deal with and represent uncertain regions into medical
scenes, namely medical matting. More specifically, because there is no
available medical matting dataset, we first labeled two medical datasets with
alpha matte. Secondly, the matting method applied to the natural image is not
suitable for the medical scene, so we propose a new architecture to generate
binary masks and alpha matte in a row. Thirdly, the uncertainty map is
introduced to highlight the ambiguous regions from the binary results and
improve the matting performance. Evaluated on these datasets, the proposed
model outperformed state-of-the-art matting algorithms by a large margin, and
alpha matte is proved to be a more efficient labeling form than a binary mask.",['cs.CV']
Flexibly Regularized Mixture Models and Application to Image Segmentation,"Probabilistic finite mixture models are widely used for unsupervised
clustering. These models can often be improved by adapting them to the topology
of the data. For instance, in order to classify spatially adjacent data points
similarly, it is common to introduce a Laplacian constraint on the posterior
probability that each data point belongs to a class. Alternatively, the mixing
probabilities can be treated as free parameters, while assuming Gauss-Markov or
more complex priors to regularize those mixing probabilities. However, these
approaches are constrained by the shape of the prior and often lead to
complicated or intractable inference. Here, we propose a new parametrization of
the Dirichlet distribution to flexibly regularize the mixing probabilities of
over-parametrized mixture distributions. Using the Expectation-Maximization
algorithm, we show that our approach allows us to define any linear update rule
for the mixing probabilities, including spatial smoothing regularization as a
special case. We then show that this flexible design can be extended to share
class information between multiple mixture models. We apply our algorithm to
artificial and natural image segmentation tasks, and we provide quantitative
and qualitative comparison of the performance of Gaussian and Student-t
mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to
propagate class information across the layers of deep convolutional neural
networks in a probabilistically optimal way, suggesting a new interpretation
for feedback signals in biological visual systems. Our flexible approach can be
easily generalized to adapt probabilistic mixture models to arbitrary data
topologies.","['cs.CV', 'cs.LG', 'q-bio.NC']"
Hierarchical Semantic Segmentation using Psychometric Learning,"Assigning meaning to parts of image data is the goal of semantic image
segmentation. Machine learning methods, specifically supervised learning is
commonly used in a variety of tasks formulated as semantic segmentation. One of
the major challenges in the supervised learning approaches is expressing and
collecting the rich knowledge that experts have with respect to the meaning
present in the image data. Towards this, typically a fixed set of labels is
specified and experts are tasked with annotating the pixels, patches or
segments in the images with the given labels. In general, however, the set of
classes does not fully capture the rich semantic information present in the
images. For example, in medical imaging such as histology images, the different
parts of cells could be grouped and sub-grouped based on the expertise of the
pathologist.
  To achieve such a precise semantic representation of the concepts in the
image, we need access to the full depth of knowledge of the annotator. In this
work, we develop a novel approach to collect segmentation annotations from
experts based on psychometric testing. Our method consists of the psychometric
testing procedure, active query selection, query enhancement, and a deep metric
learning model to achieve a patch-level image embedding that allows for
semantic segmentation of images. We show the merits of our method with
evaluation on the synthetically generated image, aerial image and histology
image.","['cs.CV', 'cs.AI']"
Semi-supervised Left Atrium Segmentation with Mutual Consistency Training,"Semi-supervised learning has attracted great attention in the field of
machine learning, especially for medical image segmentation tasks, since it
alleviates the heavy burden of collecting abundant densely annotated data for
training. However, most of existing methods underestimate the importance of
challenging regions (e.g. small branches or blurred edges) during training. We
believe that these unlabeled regions may contain more crucial information to
minimize the uncertainty prediction for the model and should be emphasized in
the training process. Therefore, in this paper, we propose a novel Mutual
Consistency Network (MC-Net) for semi-supervised left atrium segmentation from
3D MR images. Particularly, our MC-Net consists of one encoder and two slightly
different decoders, and the prediction discrepancies of two decoders are
transformed as an unsupervised loss by our designed cycled pseudo label scheme
to encourage mutual consistency. Such mutual consistency encourages the two
decoders to have consistent and low-entropy predictions and enables the model
to gradually capture generalized features from these unlabeled challenging
regions. We evaluate our MC-Net on the public Left Atrium (LA) database and it
obtains impressive performance gains by exploiting the unlabeled data
effectively. Our MC-Net outperforms six recent semi-supervised methods for left
atrium segmentation, and sets the new state-of-the-art performance on the LA
database.",['cs.CV']
Medical Transformer: Gated Axial-Attention for Medical Image Segmentation,"Over the past decade, Deep Convolutional Neural Networks have been widely
adopted for medical image segmentation and shown to achieve adequate
performance. However, due to the inherent inductive biases present in the
convolutional architectures, they lack understanding of long-range dependencies
in the image. Recently proposed Transformer-based architectures that leverage
self-attention mechanism encode long-range dependencies and learn
representations that are highly expressive. This motivates us to explore
Transformer-based solutions and study the feasibility of using
Transformer-based network architectures for medical image segmentation tasks.
Majority of existing Transformer-based network architectures proposed for
vision applications require large-scale datasets to train properly. However,
compared to the datasets for vision applications, for medical imaging the
number of data samples is relatively low, making it difficult to efficiently
train transformers for medical applications. To this end, we propose a Gated
Axial-Attention model which extends the existing architectures by introducing
an additional control mechanism in the self-attention module. Furthermore, to
train the model effectively on medical images, we propose a Local-Global
training strategy (LoGo) which further improves the performance. Specifically,
we operate on the whole image and patches to learn global and local features,
respectively. The proposed Medical Transformer (MedT) is evaluated on three
different medical image segmentation datasets and it is shown that it achieves
better performance than the convolutional and other related transformer-based
architectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer",['cs.CV']
Label noise in segmentation networks : mitigation must deal with bias,"Imperfect labels limit the quality of predictions learned by deep neural
networks. This is particularly relevant in medical image segmentation, where
reference annotations are difficult to collect and vary significantly even
across expert annotators. Prior work on mitigating label noise focused on
simple models of mostly uniform noise. In this work, we explore biased and
unbiased errors artificially introduced to brain tumour annotations on MRI
data. We found that supervised and semi-supervised segmentation methods are
robust or fairly robust to unbiased errors but sensitive to biased errors. It
is therefore important to identify the sorts of errors expected in medical
image labels and especially mitigate the biased errors.","['cs.CV', 'cs.LG']"
Quality-Aware Memory Network for Interactive Volumetric Image Segmentation,"Despite recent progress of automatic medical image segmentation techniques,
fully automatic results usually fail to meet the clinical use and typically
require further refinement. In this work, we propose a quality-aware memory
network for interactive segmentation of 3D medical images. Provided by user
guidance on an arbitrary slice, an interaction network is firstly employed to
obtain an initial 2D segmentation. The quality-aware memory network
subsequently propagates the initial segmentation estimation bidirectionally
over the entire volume. Subsequent refinement based on additional user guidance
on other slices can be incorporated in the same manner. To further facilitate
interactive segmentation, a quality assessment module is introduced to suggest
the next slice to segment based on the current segmentation quality of each
slice. The proposed network has two appealing characteristics: 1) The
memory-augmented network offers the ability to quickly encode past segmentation
information, which will be retrieved for the segmentation of other slices; 2)
The quality assessment module enables the model to directly estimate the
qualities of segmentation predictions, which allows an active learning paradigm
where users preferentially label the lowest-quality slice for multi-round
refinement. The proposed network leads to a robust interactive segmentation
engine, which can generalize well to various types of user annotations (e.g.,
scribbles, boxes). Experimental results on various medical datasets demonstrate
the superiority of our approach in comparison with existing techniques.",['cs.CV']
A Deep Learning Object Detection Method for an Efficient Clusters Initialization,"Clustering is an unsupervised machine learning method grouping data samples
into clusters of similar objects. In practice, clustering has been used in
numerous applications such as banking customers profiling, document retrieval,
image segmentation, and e-commerce recommendation engines. However, the
existing clustering techniques present significant limitations, from which is
the dependability of their stability on the initialization parameters (e.g.
number of clusters, centroids). Different solutions were presented in the
literature to overcome this limitation (i.e. internal and external validation
metrics). However, these solutions require high computational complexity and
memory consumption, especially when dealing with big data. In this paper, we
apply the recent object detection Deep Learning (DL) model, named YOLO-v5, to
detect the initial clustering parameters such as the number of clusters with
their sizes and centroids. Mainly, the proposed solution consists of adding a
DL-based initialization phase making the clustering algorithms free of
initialization. Two model solutions are provided in this work, one for isolated
clusters and the other one for overlapping clusters. The features of the
incoming dataset determine which model to use. Moreover, The results show that
the proposed solution can provide near-optimal clusters initialization
parameters with low computational and resources overhead compared to existing
solutions.",['cs.CV']
CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search,"A strong visual object tracker nowadays relies on its well-crafted modules,
which typically consist of manually-designed network architectures to deliver
high-quality tracking results. Not surprisingly, the manual design process
becomes a particularly challenging barrier, as it demands sufficient prior
experience, enormous effort, intuition and perhaps some good luck. Meanwhile,
neural architecture search has gaining grounds in practical applications such
as image segmentation, as a promising method in tackling the issue of automated
search of feasible network structures. In this work, we propose a novel
cell-level differentiable architecture search mechanism to automate the network
design of the tracking module, aiming to adapt backbone features to the
objective of a tracking network during offline training. The proposed approach
is simple, efficient, and with no need to stack a series of modules to
construct a network. Our approach is easy to be incorporated into existing
trackers, which is empirically validated using different differentiable
architecture search-based methods and tracking objectives. Extensive
experimental evaluations demonstrate the superior performance of our approach
over five commonly-used benchmarks. Meanwhile, our automated searching process
takes 41 (18) hours for the second (first) order DARTS method on the
TrackingNet dataset.","['cs.CV', 'cs.AI', 'cs.LG']"
Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation,"Deep learning-based segmentation methods are vulnerable to unforeseen data
distribution shifts during deployment, e.g. change of image appearances or
contrasts caused by different scanners, unexpected imaging artifacts etc. In
this paper, we present a cooperative framework for training image segmentation
models and a latent space augmentation method for generating hard examples.
Both contributions improve model generalization and robustness with limited
data. The cooperative training framework consists of a fast-thinking network
(FTN) and a slow-thinking network (STN). The FTN learns decoupled image
features and shape features for image reconstruction and segmentation tasks.
The STN learns shape priors for segmentation correction and refinement. The two
networks are trained in a cooperative manner. The latent space augmentation
generates challenging examples for training by masking the decoupled latent
space in both channel-wise and spatial-wise manners. We performed extensive
experiments on public cardiac imaging datasets. Using only 10 subjects from a
single site for training, we demonstrated improved cross-site segmentation
performance and increased robustness against various unforeseen imaging
artifacts compared to strong baseline methods. Particularly, cooperative
training with latent space data augmentation yields 15% improvement in terms of
average Dice score when compared to a standard training method.","['cs.CV', 'cs.AI', 'cs.LG', 'q-bio.QM']"
UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation,"Transformer architecture has emerged to be successful in a number of natural
language processing tasks. However, its applications to medical vision remain
largely unexplored. In this study, we present UTNet, a simple yet powerful
hybrid Transformer architecture that integrates self-attention into a
convolutional neural network for enhancing medical image segmentation. UTNet
applies self-attention modules in both encoder and decoder for capturing
long-range dependency at different scales with minimal overhead. To this end,
we propose an efficient self-attention mechanism along with relative position
encoding that reduces the complexity of self-attention operation significantly
from $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also
proposed to recover fine-grained details from the skipped connections in the
encoder. Our approach addresses the dilemma that Transformer requires huge
amounts of data to learn vision inductive bias. Our hybrid layer design allows
the initialization of Transformer into convolutional networks without a need of
pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac
magnetic resonance imaging cohort. UTNet demonstrates superior segmentation
performance and robustness against the state-of-the-art approaches, holding the
promise to generalize well on other medical image segmentations.",['cs.CV']
Unsupervised Image Segmentation by Mutual Information Maximization and Adversarial Regularization,"Semantic segmentation is one of the basic, yet essential scene understanding
tasks for an autonomous agent. The recent developments in supervised machine
learning and neural networks have enjoyed great success in enhancing the
performance of the state-of-the-art techniques for this task. However, their
superior performance is highly reliant on the availability of a large-scale
annotated dataset. In this paper, we propose a novel fully unsupervised
semantic segmentation method, the so-called Information Maximization and
Adversarial Regularization Segmentation (InMARS). Inspired by human perception
which parses a scene into perceptual groups, rather than analyzing each pixel
individually, our proposed approach first partitions an input image into
meaningful regions (also known as superpixels). Next, it utilizes
Mutual-Information-Maximization followed by an adversarial training strategy to
cluster these regions into semantically meaningful classes. To customize an
adversarial training scheme for the problem, we incorporate adversarial pixel
noise along with spatial perturbations to impose photometrical and geometrical
invariance on the deep neural network. Our experiments demonstrate that our
method achieves the state-of-the-art performance on two commonly used
unsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam.",['cs.CV']
Inter Extreme Points Geodesics for Weakly Supervised Segmentation,"We introduce $\textit{InExtremIS}$, a weakly supervised 3D approach to train
a deep image segmentation network using particularly weak train-time
annotations: only 6 extreme clicks at the boundary of the objects of interest.
Our fully-automatic method is trained end-to-end and does not require any
test-time annotations. From the extreme points, 3D bounding boxes are extracted
around objects of interest. Then, deep geodesics connecting extreme points are
generated to increase the amount of ""annotated"" voxels within the bounding
boxes. Finally, a weakly supervised regularised loss derived from a Conditional
Random Field formulation is used to encourage prediction consistency over
homogeneous regions. Extensive experiments are performed on a large open
dataset for Vestibular Schwannoma segmentation. $\textit{InExtremIS}$ obtained
competitive performance, approaching full supervision and outperforming
significantly other weakly supervised techniques based on bounding boxes.
Moreover, given a fixed annotation time budget, $\textit{InExtremIS}$
outperforms full supervision. Our code and data are available online.",['cs.CV']
Segmenting two-dimensional structures with strided tensor networks,"Tensor networks provide an efficient approximation of operations involving
high dimensional tensors and have been extensively used in modelling quantum
many-body systems. More recently, supervised learning has been attempted with
tensor networks, primarily focused on tasks such as image classification. In
this work, we propose a novel formulation of tensor networks for supervised
image segmentation which allows them to operate on high resolution medical
images. We use the matrix product state (MPS) tensor network on non-overlapping
patches of a given input image to predict the segmentation mask by learning a
pixel-wise linear classification rule in a high dimensional space. The proposed
model is end-to-end trainable using backpropagation. It is implemented as a
Strided Tensor Network to reduce the parameter complexity. The performance of
the proposed method is evaluated on two public medical imaging datasets and
compared to relevant baselines. The evaluation shows that the strided tensor
network yields competitive performance compared to CNN-based models while using
fewer resources. Additionally, based on the experiments we discuss the
feasibility of using fully linear models for segmentation tasks.",['cs.CV']
Segmentation with Multiple Acceptable Annotations: A Case Study of Myocardial Segmentation in Contrast Echocardiography,"Most existing deep learning-based frameworks for image segmentation assume
that a unique ground truth is known and can be used for performance evaluation.
This is true for many applications, but not all. Myocardial segmentation of
Myocardial Contrast Echocardiography (MCE), a critical task in automatic
myocardial perfusion analysis, is an example. Due to the low resolution and
serious artifacts in MCE data, annotations from different cardiologists can
vary significantly, and it is hard to tell which one is the best. In this case,
how can we find a good way to evaluate segmentation performance and how do we
train the neural network? In this paper, we address the first problem by
proposing a new extended Dice to effectively evaluate the segmentation
performance when multiple accepted ground truth is available. Then based on our
proposed metric, we solve the second problem by further incorporating the new
metric into a loss function that enables neural networks to flexibly learn
general features of myocardium. Experiment results on our clinical MCE data set
demonstrate that the neural network trained with the proposed loss function
outperforms those existing ones that try to obtain a unique ground truth from
multiple annotations, both quantitatively and qualitatively. Finally, our
grading study shows that using extended Dice as an evaluation metric can better
identify segmentation results that need manual correction compared with using
Dice.",['cs.CV']
Information-Theoretic Segmentation by Inpainting Error Maximization,"We study image segmentation from an information-theoretic perspective,
proposing a novel adversarial method that performs unsupervised segmentation by
partitioning images into maximally independent sets. More specifically, we
group image pixels into foreground and background, with the goal of minimizing
predictability of one set from the other. An easily computed loss drives a
greedy search process to maximize inpainting error over these partitions. Our
method does not involve training deep networks, is computationally cheap,
class-agnostic, and even applicable in isolation to a single unlabeled image.
Experiments demonstrate that it achieves a new state-of-the-art in unsupervised
segmentation quality, while being substantially faster and more general than
competing approaches.",['cs.CV']
Are conditional GANs explicitly conditional?,"This paper proposes two important contributions for conditional Generative
Adversarial Networks (cGANs) to improve the wide variety of applications that
exploit this architecture. The first main contribution is an analysis of cGANs
to show that they are not explicitly conditional. In particular, it will be
shown that the discriminator and subsequently the cGAN does not automatically
learn the conditionality between inputs. The second contribution is a new
method, called acontrario, that explicitly models conditionality for both parts
of the adversarial architecture via a novel acontrario loss that involves
training the discriminator to learn unconditional (adverse) examples. This
leads to a novel type of data augmentation approach for GANs (acontrario
learning) which allows to restrict the search space of the generator to
conditional outputs using adverse examples. Extensive experimentation is
carried out to evaluate the conditionality of the discriminator by proposing a
probability distribution analysis. Comparisons with the cGAN architecture for
different applications show significant improvements in performance on well
known datasets including, semantic image synthesis, image segmentation and
monocular depth prediction using different metrics including Fr\'echet
Inception Distance(FID), mean Intersection over Union (mIoU), Root Mean Square
Error log (RMSE log) and Number of statistically-Different Bins (NDB)","['cs.CV', 'cs.AI']"
K-Net: Towards Unified Image Segmentation,"Semantic, instance, and panoptic segmentations have been addressed using
different and specialized frameworks despite their underlying connections. This
paper presents a unified, simple, and effective framework for these essentially
similar tasks. The framework, named K-Net, segments both instances and semantic
categories consistently by a group of learnable kernels, where each kernel is
responsible for generating a mask for either a potential instance or a stuff
class. To remedy the difficulties of distinguishing various instances, we
propose a kernel update strategy that enables each kernel dynamic and
conditional on its meaningful group in the input image. K-Net can be trained in
an end-to-end manner with bipartite matching, and its training and inference
are naturally NMS-free and box-free. Without bells and whistles, K-Net
surpasses all previous state-of-the-art single-model results of panoptic
segmentation on MS COCO and semantic segmentation on ADE20K with 52.1% PQ and
54.3% mIoU, respectively. Its instance segmentation performance is also on par
with Cascade Mask R-CNNon MS COCO with 60%-90% faster inference speeds. Code
and models will be released at https://github.com/open-mmlab/mmdetection.","['cs.CV', 'cs.AI']"
Poisoning the Search Space in Neural Architecture Search,"Deep learning has proven to be a highly effective problem-solving tool for
object detection and image segmentation across various domains such as
healthcare and autonomous driving. At the heart of this performance lies neural
architecture design which relies heavily on domain knowledge and prior
experience on the researchers' behalf. More recently, this process of finding
the most optimal architectures, given an initial search space of possible
operations, was automated by Neural Architecture Search (NAS). In this paper,
we evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)
against data agnostic poisoning attacks on the original search space with
carefully designed ineffective operations. By evaluating algorithm performance
on the CIFAR-10 dataset, we empirically demonstrate how our novel search space
poisoning (SSP) approach and multiple-instance poisoning attacks exploit design
flaws in the ENAS controller to result in inflated prediction error rates for
child networks. Our results provide insights into the challenges to surmount in
using NAS for more adversarially robust architecture search.","['cs.LG', 'cs.CR', 'cs.NE', 'stat.ML']"
Multi-Compound Transformer for Accurate Biomedical Image Segmentation,"The recent vision transformer(i.e.for image classification) learns non-local
attentive interaction of different patch tokens. However, prior arts miss
learning the cross-scale dependencies of different pixels, the semantic
correspondence of different labels, and the consistency of the feature
representations and semantic embeddings, which are critical for biomedical
segmentation. In this paper, we tackle the above issues by proposing a unified
transformer network, termed Multi-Compound Transformer (MCTrans), which
incorporates rich feature learning and semantic structure mining into a unified
framework. Specifically, MCTrans embeds the multi-scale convolutional features
as a sequence of tokens and performs intra- and inter-scale self-attention,
rather than single-scale attention in previous works. In addition, a learnable
proxy embedding is also introduced to model semantic relationship and feature
enhancement by using self-attention and cross-attention, respectively. MCTrans
can be easily plugged into a UNet-like network and attains a significant
improvement over the state-of-the-art methods in biomedical image segmentation
in six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,
3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,
Kavirs, ISIC2018 dataset, respectively. Code is available at
https://github.com/JiYuanFeng/MCTrans.",['cs.CV']
TransBTS: Multimodal Brain Tumor Segmentation Using Transformer,"Transformer, which can benefit from global (long-range) information modeling
using self-attention mechanisms, has been successful in natural language
processing and 2D image classification recently. However, both local and global
features are crucial for dense prediction tasks, especially for 3D medical
image segmentation. In this paper, we for the first time exploit Transformer in
3D CNN for MRI Brain Tumor Segmentation and propose a novel network named
TransBTS based on the encoder-decoder structure. To capture the local 3D
context information, the encoder first utilizes 3D CNN to extract the
volumetric spatial feature maps. Meanwhile, the feature maps are reformed
elaborately for tokens that are fed into Transformer for global feature
modeling. The decoder leverages the features embedded by Transformer and
performs progressive upsampling to predict the detailed segmentation map.
Extensive experimental results on both BraTS 2019 and 2020 datasets show that
TransBTS achieves comparable or higher results than previous state-of-the-art
3D methods for brain tumor segmentation on 3D MRI scans. The source code is
available at https://github.com/Wenxuan-1119/TransBTS","['cs.CV', 'cs.AI']"
ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation,"Fully convolutional U-shaped neural networks have largely been the dominant
approach for pixel-wise image segmentation. In this work, we tackle two defects
that hinder their deployment in real-world applications: 1) Predictions lack
uncertainty quantification that may be crucial to many decision-making systems;
2) Large memory storage and computational consumption demanding extensive
hardware resources. To address these issues and improve their practicality we
demonstrate a few-parameter compact Bayesian convolutional architecture, that
achieves a marginal improvement in accuracy in comparison to related work using
significantly fewer parameters and compute operations. The architecture
combines parameter-efficient operations such as separable convolutions,
bilinear interpolation, multi-scale feature propagation and Bayesian inference
for per-pixel uncertainty quantification through Monte Carlo Dropout. The best
performing configurations required fewer than 2.5 million parameters on diverse
challenging datasets with few observations.","['cs.CV', 'cs.AI']"
Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds to Panoramic Color Images,"In this work, we present a simple yet effective framework to address the
domain translation problem between different sensor modalities with unique data
formats. By relying only on the semantics of the scene, our modular generative
framework can, for the first time, synthesize a panoramic color image from a
given full 3D LiDAR point cloud. The framework starts with semantic
segmentation of the point cloud, which is initially projected onto a spherical
surface. The same semantic segmentation is applied to the corresponding camera
image. Next, our new conditional generative model adversarially learns to
translate the predicted LiDAR segment maps to the camera image counterparts.
Finally, generated image segments are processed to render the panoramic scene
images. We provide a thorough quantitative evaluation on the SemanticKitti
dataset and show that our proposed framework outperforms other strong baseline
models.
  Our source code is available at
https://github.com/halmstad-University/TITAN-NET",['cs.CV']
Attention Toward Neighbors: A Context Aware Framework for High Resolution Image Segmentation,"High-resolution image segmentation remains challenging and error-prone due to
the enormous size of intermediate feature maps. Conventional methods avoid this
problem by using patch based approaches where each patch is segmented
independently. However, independent patch segmentation induces errors,
particularly at the patch boundary due to the lack of contextual information in
very high-resolution images where the patch size is much smaller compared to
the full image. To overcome these limitations, in this paper, we propose a
novel framework to segment a particular patch by incorporating contextual
information from its neighboring patches. This allows the segmentation network
to see the target patch with a wider field of view without the need of larger
feature maps. Comparative analysis from a number of experiments shows that our
proposed framework is able to segment high resolution images with significantly
improved mean Intersection over Union and overall accuracy.",['cs.CV']
Boosting Semi-supervised Image Segmentation with Global and Local Mutual Information Regularization,"The scarcity of labeled data often impedes the application of deep learning
to the segmentation of medical images. Semi-supervised learning seeks to
overcome this limitation by exploiting unlabeled examples in the learning
process. In this paper, we present a novel semi-supervised segmentation method
that leverages mutual information (MI) on categorical distributions to achieve
both global representation invariance and local smoothness. In this method, we
maximize the MI for intermediate feature embeddings that are taken from both
the encoder and decoder of a segmentation network. We first propose a global MI
loss constraining the encoder to learn an image representation that is
invariant to geometric transformations. Instead of resorting to
computationally-expensive techniques for estimating the MI on continuous
feature embeddings, we use projection heads to map them to a discrete cluster
assignment where MI can be computed efficiently. Our method also includes a
local MI loss to promote spatial consistency in the feature maps of the decoder
and provide a smoother segmentation. Since mutual information does not require
a strict ordering of clusters in two different assignments, we incorporate a
final consistency regularization loss on the output which helps align the
cluster labels throughout the network. We evaluate the method on four
challenging publicly-available datasets for medical image segmentation.
Experimental results show our method to outperform recently-proposed approaches
for semi-supervised segmentation and provide an accuracy near to full
supervision while training with very few annotated images.",['cs.CV']
Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation,"Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to an unlabeled and unseen target domain, which is
usually trained on data from both domains. Access to the source domain data at
the adaptation stage, however, is often limited, due to data storage or privacy
issues. To alleviate this, in this work, we target source free UDA for
segmentation, and propose to adapt an ``off-the-shelf"" segmentation model
pre-trained in the source domain to the target domain, with an adaptive
batch-wise normalization statistics adaptation framework. Specifically, the
domain-specific low-order batch statistics, i.e., mean and variance, are
gradually adapted with an exponential momentum decay scheme, while the
consistency of domain shareable high-order batch statistics, i.e., scaling and
shifting parameters, is explicitly enforced by our optimization objective. The
transferability of each channel is adaptively measured first from which to
balance the contribution of each channel. Moreover, the proposed source free
UDA framework is orthogonal to unsupervised learning methods, e.g.,
self-entropy minimization, which can thus be simply added on top of our
framework. Extensive experiments on the BraTS 2018 database show that our
source free UDA framework outperformed existing source-relaxed UDA methods for
the cross-subtype UDA segmentation task and yielded comparable results for the
cross-modality UDA segmentation task, compared with a supervised UDA methods
with the source data.","['cs.CV', 'cs.AI', 'cs.LG']"
Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images,"Semantic segmentation from very fine resolution (VFR) urban scene images
plays a significant role in several application scenarios including autonomous
driving, land cover classification, and urban planning, etc. However, the
tremendous details contained in the VFR image severely limit the potential of
the existing deep learning approaches. More seriously, the considerable
variations in scale and appearance of objects further deteriorate the
representational capacity of those se-mantic segmentation methods, leading to
the confusion of adjacent objects. Addressing such is-sues represents a
promising research field in the remote sensing community, which paves the way
for scene-level landscape pattern analysis and decision making. In this
manuscript, we pro-pose a bilateral awareness network (BANet) which contains a
dependency path and a texture path to fully capture the long-range
relationships and fine-grained details in VFR images. Specif-ically, the
dependency path is conducted based on the ResT, a novel Transformer backbone
with memory-efficient multi-head self-attention, while the texture path is
built on the stacked convo-lution operation. Besides, using the linear
attention mechanism, a feature aggregation module (FAM) is designed to
effectively fuse the dependency features and texture features. Extensive
experiments conducted on the three large-scale urban scene image segmentation
datasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAVid
dataset, demonstrate the effective-ness of our BANet. Specifically, a 64.6%
mIoU is achieved on the UAVid dataset.",['cs.CV']
Automatic Head Overcoat Thickness Measure with NASNet-Large-Decoder Net,"Transmission electron microscopy (TEM) is one of the primary tools to show
microstructural characterization of materials as well as film thickness.
However, manual determination of film thickness from TEM images is
time-consuming as well as subjective, especially when the films in question are
very thin and the need for measurement precision is very high. Such is the case
for head overcoat (HOC) thickness measurements in the magnetic hard disk drive
industry. It is therefore necessary to develop software to automatically
measure HOC thickness. In this paper, for the first time, we propose a HOC
layer segmentation method using NASNet-Large as an encoder and then followed by
a decoder architecture, which is one of the most commonly used architectures in
deep learning for image segmentation. To further improve segmentation results,
we are the first to propose a post-processing layer to remove irrelevant
portions in the segmentation result. To measure the thickness of the segmented
HOC layer, we propose a regressive convolutional neural network (RCNN) model as
well as orthogonal thickness calculation methods. Experimental results
demonstrate a higher dice score for our model which has lower mean squared
error and outperforms current state-of-the-art manual measurement.",['cs.CV']
Distilling effective supervision for robust medical image segmentation with noisy labels,"Despite the success of deep learning methods in medical image segmentation
tasks, the human-level performance relies on massive training data with
high-quality annotations, which are expensive and time-consuming to collect.
The fact is that there exist low-quality annotations with label noise, which
leads to suboptimal performance of learned models. Two prominent directions for
segmentation learning with noisy labels include pixel-wise noise robust
training and image-level noise robust training. In this work, we propose a
novel framework to address segmenting with noisy labels by distilling effective
supervision information from both pixel and image levels. In particular, we
explicitly estimate the uncertainty of every pixel as pixel-wise noise
estimation, and propose pixel-wise robust learning by using both the original
labels and pseudo labels. Furthermore, we present an image-level robust
learning method to accommodate more information as the complements to
pixel-level learning. We conduct extensive experiments on both simulated and
real-world noisy datasets. The results demonstrate the advantageous performance
of our method compared to state-of-the-art baselines for medical image
segmentation with noisy labels.",['cs.CV']
Improving Multi-Modal Learning with Uni-Modal Teachers,"Learning multi-modal representations is an essential step towards real-world
robotic applications, and various multi-modal fusion models have been developed
for this purpose. However, we observe that existing models, whose objectives
are mostly based on joint training, often suffer from learning inferior
representations of each modality. We name this problem Modality Failure, and
hypothesize that the imbalance of modalities and the implicit bias of common
objectives in fusion method prevent encoders of each modality from sufficient
feature learning. To this end, we propose a new multi-modal learning method,
Uni-Modal Teacher, which combines the fusion objective and uni-modal
distillation to tackle the modality failure problem. We show that our method
not only drastically improves the representation of each modality, but also
improves the overall multi-modal task performance. Our method can be
effectively generalized to most multi-modal fusion approaches. We achieve more
than 3% improvement on the VGGSound audio-visual classification task, as well
as improving performance on the NYU depth V2 RGB-D image segmentation task.",['cs.LG']
Large-scale image segmentation based on distributed clustering algorithms,"Many approaches to 3D image segmentation are based on hierarchical clustering
of supervoxels into image regions. Here we describe a distributed algorithm
capable of handling a tremendous number of supervoxels. The algorithm works
recursively, the regions are divided into chunks that are processed
independently in parallel by multiple workers. At each round of the recursive
procedure, the chunk size in all dimensions are doubled until a single chunk
encompasses the entire image. The final result is provably independent of the
chunking scheme, and the same as if the entire image were processed without
division into chunks. This is nontrivial because a pair of adjacent regions is
scored by some statistical property (e.g. mean or median) of the affinities at
the interface, and the interface may extend over arbitrarily many chunks. The
trick is to delay merge decisions for regions that touch chunk boundaries, and
only complete them in a later round after the regions are fully contained
within a chunk. We demonstrate the algorithm by clustering an affinity graph
with over 1.5 trillion edges between 135 billion supervoxels derived from a 3D
electron microscopic brain image.",['cs.CV']
"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers","Vision Transformers (ViT) have been shown to attain highly competitive
performance for a wide range of vision applications, such as image
classification, object detection and semantic image segmentation. In comparison
to convolutional neural networks, the Vision Transformer's weaker inductive
bias is generally found to cause an increased reliance on model regularization
or data augmentation (``AugReg'' for short) when training on smaller training
datasets. We conduct a systematic empirical study in order to better understand
the interplay between the amount of training data, AugReg, model size and
compute budget. As one result of this study we find that the combination of
increased compute and AugReg can yield models with the same performance as
models trained on an order of magnitude more training data: we train ViT models
of various sizes on the public ImageNet-21k dataset which either match or
outperform their counterparts trained on the larger, but not publicly available
JFT-300M dataset.","['cs.CV', 'cs.AI', 'cs.LG']"
Positional Contrastive Learning for Volumetric Medical Image Segmentation,"The success of deep learning heavily depends on the availability of large
labeled training sets. However, it is hard to get large labeled datasets in
medical image domain because of the strict privacy concern and costly labeling
efforts. Contrastive learning, an unsupervised learning technique, has been
proved powerful in learning image-level representations from unlabeled data.
The learned encoder can then be transferred or fine-tuned to improve the
performance of downstream tasks with limited labels. A critical step in
contrastive learning is the generation of contrastive data pairs, which is
relatively simple for natural image classification but quite challenging for
medical image segmentation due to the existence of the same tissue or organ
across the dataset. As a result, when applied to medical image segmentation,
most state-of-the-art contrastive learning frameworks inevitably introduce a
lot of false-negative pairs and result in degraded segmentation quality. To
address this issue, we propose a novel positional contrastive learning (PCL)
framework to generate contrastive data pairs by leveraging the position
information in volumetric medical images. Experimental results on CT and MRI
datasets demonstrate that the proposed PCL method can substantially improve the
segmentation performance compared to existing methods in both semi-supervised
setting and transfer learning setting.",['cs.CV']
Guided interactive image segmentation using machine learning and color based data set clustering,"We present a novel approach that combines machine learning based interactive
image segmentation using supervoxels with a clustering method for the automated
identification of similarly colored images in large data sets which enables a
guided reuse of classifiers. Our approach solves the problem of significant
color variability prevalent and often unavoidable in biological and medical
images which typically leads to deteriorated segmentation and quantification
accuracy thereby greatly reducing the necessary training effort. This increase
in efficiency facilitates the quantification of much larger numbers of images
thereby enabling interactive image analysis for recent new technological
advances in high-throughput imaging. The presented methods are applicable for
almost any image type and represent a useful tool for image analysis tasks in
general.","['cs.CV', 'eess.IV']"
Knowledge distillation from multi-modal to mono-modal segmentation networks,"The joint use of multiple imaging modalities for medical image segmentation
has been widely studied in recent years. The fusion of information from
different modalities has demonstrated to improve the segmentation accuracy,
with respect to mono-modal segmentations, in several applications. However,
acquiring multiple modalities is usually not possible in a clinical setting due
to a limited number of physicians and scanners, and to limit costs and scan
time. Most of the time, only one modality is acquired. In this paper, we
propose KD-Net, a framework to transfer knowledge from a trained multi-modal
network (teacher) to a mono-modal one (student). The proposed method is an
adaptation of the generalized distillation framework where the student network
is trained on a subset (1 modality) of the teacher's inputs (n modalities). We
illustrate the effectiveness of the proposed framework in brain tumor
segmentation with the BraTS 2018 dataset. Using different architectures, we
show that the student network effectively learns from the teacher and always
outperforms the baseline mono-modal network in terms of segmentation accuracy.","['cs.CV', 'cs.AI', 'stat.ML']"
Trilateral Attention Network for Real-time Medical Image Segmentation,"Accurate segmentation of medical images into anatomically meaningful regions
is critical for the extraction of quantitative indices or biomarkers. The
common pipeline for segmentation comprises regions of interest detection stage
and segmentation stage, which are independent of each other and typically
performed using separate deep learning networks. The performance of the
segmentation stage highly relies on the extracted set of spatial features and
the receptive fields. In this work, we propose an end-to-end network, called
Trilateral Attention Network (TaNet), for real-time detection and segmentation
in medical images. TaNet has a module for region localization, and three
segmentation pathways: 1) handcrafted pathway with hand-designed convolutional
kernels, 2) detail pathway with regular convolutional kernels, and 3) a global
pathway to enlarge the receptive field. The first two pathways encode rich
handcrafted and low-level features extracted by hand-designed and regular
kernels while the global pathway encodes high-level context information. By
jointly training the network for localization and segmentation using different
sets of features, TaNet achieved superior performance, in terms of accuracy and
speed, when evaluated on an echocardiography dataset for cardiac segmentation.
The code and models will be made publicly available in TaNet Github page.",['cs.CV']
CMF: Cascaded Multi-model Fusion for Referring Image Segmentation,"In this work, we address the task of referring image segmentation (RIS),
which aims at predicting a segmentation mask for the object described by a
natural language expression. Most existing methods focus on establishing
unidirectional or directional relationships between visual and linguistic
features to associate two modalities together, while the multi-scale context is
ignored or insufficiently modeled. Multi-scale context is crucial to localize
and segment those objects that have large scale variations during the
multi-modal fusion process. To solve this problem, we propose a simple yet
effective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple
atrous convolutional layers in parallel and further introduces a cascaded
branch to fuse visual and linguistic features. The cascaded branch can
progressively integrate multi-scale contextual information and facilitate the
alignment of two modalities during the multi-modal fusion process. Experimental
results on four benchmark datasets demonstrate that our method outperforms most
state-of-the-art methods. Code is available at
https://github.com/jianhua2022/CMF-Refseg.",['cs.CV']
Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation,"This paper addresses the domain shift problem for segmentation. As a
solution, we propose OLVA, a novel and lightweight unsupervised domain
adaptation method based on a Variational Auto-Encoder (VAE) and Optimal
Transport (OT) theory. Thanks to the VAE, our model learns a shared
cross-domain latent space that follows a normal distribution, which reduces the
domain shift. To guarantee valid segmentations, our shared latent space is
designed to model the shape rather than the intensity variations. We further
rely on an OT loss to match and align the remaining discrepancy between the two
domains in the latent space. We demonstrate OLVA's effectiveness for the
segmentation of multiple cardiac structures on the public Multi-Modality Whole
Heart Segmentation (MM-WHS) dataset, where the source domain consists of
annotated 3D MR images and the unlabelled target domain of 3D CTs. Our results
show remarkable improvements with an additional margin of 12.5\% dice score
over concurrent generative training approaches.","['cs.LG', 'cs.AI', 'cs.CV']"
A Hybrid mmWave and Camera System for Long-Range Depth Imaging,"mmWave radars offer excellent depth resolution owing to their high bandwidth
at mmWave radio frequencies. Yet, they suffer intrinsically from poor angular
resolution, that is an order-of-magnitude worse than camera systems, and are
therefore not a capable 3-D imaging solution in isolation. We propose
Metamoran, a system that combines the complimentary strengths of radar and
camera systems to obtain depth images at high azimuthal resolutions at
distances of several tens of meters with high accuracy, all from a single fixed
vantage point. Metamoran enables rich long-range depth imaging outdoors with
applications to roadside safety infrastructure, surveillance and wide-area
mapping. Our key insight is to use the high azimuth resolution from cameras
using computer vision techniques, including image segmentation and monocular
depth estimation, to obtain object shapes and use these as priors for our novel
specular beamforming algorithm. We also design this algorithm to work in
cluttered environments with weak reflections and in partially occluded
scenarios. We perform a detailed evaluation of Metamoran's depth imaging and
sensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation
shows that Metamoran estimates the depth of an object up to 60~m away with a
median error of 28~cm, an improvement of 13$\times$ compared to a naive
radar+camera baseline and 23$\times$ compared to monocular depth estimation.","['cs.CV', 'cs.NI', 'cs.RO', 'eess.SP']"
Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery,"This paper reports a CPU-level real-time stereo matching method for surgical
images (10 Hz on 640 * 480 image with a single core of i5-9400). The proposed
method is built on the fast ''dense inverse searching'' algorithm, which
estimates the disparity of the stereo images. The overlapping image patches
(arbitrary squared image segment) from the images at different scales are
aligned based on the photometric consistency presumption. We propose a Bayesian
framework to evaluate the probability of the optimized patch disparity at
different scales. Moreover, we introduce a spatial Gaussian mixed probability
distribution to address the pixel-wise probability within the patch. In-vivo
and synthetic experiments show that our method can handle ambiguities resulted
from the textureless surfaces and the photometric inconsistency caused by the
Lambertian reflectance. Our Bayesian method correctly balances the probability
of the patch for stereo images at different scales. Experiments indicate that
the estimated depth has higher accuracy and fewer outliers than the baseline
methods in the surgical scenario.",['cs.CV']
Adversarial Segmentation Loss for Sketch Colorization,"We introduce a new method for generating color images from sketches or edge
maps. Current methods either require some form of additional user-guidance or
are limited to the ""paired"" translation approach. We argue that segmentation
information could provide valuable guidance for sketch colorization. To this
end, we propose to leverage semantic image segmentation, as provided by a
general purpose panoptic segmentation network, to create an additional
adversarial loss function. Our loss function can be integrated to any baseline
GAN model. Our method is not limited to datasets that contain segmentation
labels, and it can be trained for ""unpaired"" translation tasks. We show the
effectiveness of our method on four different datasets spanning scene level
indoor, outdoor, and children book illustration images using qualitative,
quantitative and user study analysis. Our model improves its baseline up to 35
points on the FID metric. Our code and pretrained models can be found at
https://github.com/giddyyupp/AdvSegLoss.",['cs.CV']
DS-TransUNet:Dual Swin Transformer U-Net for Medical Image Segmentation,"Automatic medical image segmentation has made great progress benefit from the
development of deep learning. However, most existing methods are based on
convolutional neural networks (CNNs), which fail to build long-range
dependencies and global context connections due to the limitation of receptive
field in convolution operation. Inspired by the success of Transformer in
modeling the long-range contextual information, some researchers have expended
considerable efforts in designing the robust variants of Transformer-based
U-Net. Moreover, the patch division used in vision transformers usually ignores
the pixel-level intrinsic structural features inside each patch. To alleviate
these problems, we propose a novel deep medical image segmentation framework
called Dual Swin Transformer U-Net (DS-TransUNet), which might be the first
attempt to concurrently incorporate the advantages of hierarchical Swin
Transformer into both encoder and decoder of the standard U-shaped architecture
to enhance the semantic segmentation quality of varying medical images. Unlike
many prior Transformer-based solutions, the proposed DS-TransUNet first adopts
dual-scale encoder subnetworks based on Swin Transformer to extract the coarse
and fine-grained feature representations of different semantic scales. As the
core component for our DS-TransUNet, a well-designed Transformer Interactive
Fusion (TIF) module is proposed to effectively establish global dependencies
between features of different scales through the self-attention mechanism.
Furthermore, we also introduce the Swin Transformer block into decoder to
further explore the long-range contextual information during the up-sampling
process. Extensive experiments across four typical tasks for medical image
segmentation demonstrate the effectiveness of DS-TransUNet, and show that our
approach significantly outperforms the state-of-the-art methods.",['cs.CV']
Self-Ensembling Contrastive Learning for Semi-Supervised Medical Image Segmentation,"Deep learning has demonstrated significant improvements in medical image
segmentation using a sufficiently large amount of training data with manual
labels. Acquiring well-representative labels requires expert knowledge and
exhaustive labors. In this paper, we aim to boost the performance of
semi-supervised learning for medical image segmentation with limited labels
using a self-ensembling contrastive learning technique. To this end, we propose
to train an encoder-decoder network at image-level with small amounts of
labeled images, and more importantly, we learn latent representations directly
at feature-level by imposing contrastive loss on unlabeled images. This method
strengthens intra-class compactness and inter-class separability, so as to get
a better pixel classifier. Moreover, we devise a student encoder for online
learning and an exponential moving average version of it, called teacher
encoder, to improve the performance iteratively in a self-ensembling manner. To
construct contrastive samples with unlabeled images, two sampling strategies
that exploit structure similarity across medical images and utilize
pseudo-labels for construction, termed region-aware and anatomical-aware
contrastive sampling, are investigated. We conduct extensive experiments on an
MRI and a CT segmentation dataset and demonstrate that in a limited label
setting, the proposed method achieves state-of-the-art performance. Moreover,
the anatomical-aware strategy that prepares contrastive samples on-the-fly
using pseudo-labels realizes better contrastive regularization on feature
representations.",['cs.CV']
RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation,"The segmentation of nanoscale electron microscopy (EM) images is crucial but
challenging in connectomics. Recent advances in deep learning have demonstrated
the significant potential of automatic segmentation for tera-scale EM images.
However, none of the existing segmentation methods are error-free, and they
require proofreading, which is typically implemented as an interactive,
semi-automatic process via manual intervention. Herein, we propose a fully
automatic proofreading method based on reinforcement learning. The main idea is
to model the human decision process in proofreading using a reinforcement agent
to achieve fully automatic proofreading. We systematically design the proposed
system by combining multiple reinforcement learning agents in a hierarchical
manner, where each agent focuses only on a specific task while preserving
dependency between agents. Furthermore, we also demonstrate that the episodic
task setting of reinforcement learning can efficiently manage a combination of
merge and split errors concurrently presented in the input. We demonstrate the
efficacy of the proposed system by comparing it with state-of-the-art
proofreading methods using various testing examples.",['cs.CV']
Left Ventricle Contouring in Cardiac Images Based on Deep Reinforcement Learning,"Medical image segmentation is one of the important tasks of computer-aided
diagnosis in medical image analysis. Since most medical images have the
characteristics of blurred boundaries and uneven intensity distribution,
through existing segmentation methods, the discontinuity within the target area
and the discontinuity of the target boundary are likely to lead to rough or
even erroneous boundary delineation. In this paper, we propose a new iterative
refined interactive segmentation method for medical images based on agent
reinforcement learning, which focuses on the problem of target segmentation
boundaries. We model the dynamic process of drawing the target contour in a
certain order as a Markov Decision Process (MDP) based on a deep reinforcement
learning method. In the dynamic process of continuous interaction between the
agent and the image, the agent tracks the boundary point by point in order
within a limited length range until the contour of the target is completely
drawn. In this process, the agent can quickly improve the segmentation
performance by exploring an interactive policy in the image. The method we
proposed is simple and effective. At the same time, we evaluate our method on
the cardiac MRI scan data set. Experimental results show that our method has a
better segmentation effect on the left ventricle in a small number of medical
image data sets, especially in terms of segmentation boundaries, this method is
better than existing methods. Based on our proposed method, the dynamic
generation process of the predicted contour trajectory of the left ventricle
will be displayed online at https://github.com/H1997ym/LV-contour-trajectory.","['cs.CV', 'cs.AI']"
GSVNet: Guided Spatially-Varying Convolution for Fast Semantic Segmentation on Video,"This paper addresses fast semantic segmentation on video.Video segmentation
often calls for real-time, or even fasterthan real-time, processing. One common
recipe for conserving computation arising from feature extraction is to
propagate features of few selected keyframes. However, recent advances in fast
image segmentation make these solutions less attractive. To leverage fast image
segmentation for furthering video segmentation, we propose a simple yet
efficient propagation framework. Specifically, we perform lightweight flow
estimation in 1/8-downscaled image space for temporal warping in segmentation
outpace space. Moreover, we introduce a guided spatially-varying convolution
for fusing segmentations derived from the previous and current frames, to
mitigate propagation error and enable lightweight feature extraction on
non-keyframes. Experimental results on Cityscapes and CamVid show that our
scheme achieves the state-of-the-art accuracy-throughput trade-off on video
segmentation.","['cs.CV', 'cs.LG']"
Weakly Supervised Volumetric Image Segmentation with Deformed Templates,"There are many approaches that use weak-supervision to train networks to
segment 2D images. By contrast, existing 3D approaches rely on full-supervision
of a subset of 2D slices of the 3D image volume. In this paper, we propose an
approach that is truly weakly-supervised in the sense that we only need to
provide a sparse set of 3D point on the surface of target objects, an easy task
that can be quickly done. We use the 3D points to deform a 3D template so that
it roughly matches the target object outlines and we introduce an architecture
that exploits the supervision provided by coarse template to train a network to
find accurate boundaries.
  We evaluate the performance of our approach on Computed Tomography (CT),
Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets.
We will show that it outperforms a more traditional approach to
weak-supervision in 3D at a reduced supervision cost.",['cs.CV']
Few-shot segmentation of medical images based on meta-learning with implicit gradients,"Classical supervised methods commonly used often suffer from the requirement
of an abudant number of training samples and are unable to generalize on unseen
datasets. As a result, the broader application of any trained model is very
limited in clinical settings. However, few-shot approaches can minimize the
need for enormous reliable ground truth labels that are both labor intensive
and expensive. To this end, we propose to exploit an optimization-based
implicit model agnostic meta-learning {iMAML} algorithm in a few-shot setting
for medical image segmentation. Our approach can leverage the learned weights
from a diverse set of training samples and can be deployed on a new unseen
dataset. We show that unlike classical few-shot learning approaches, our method
has improved generalization capability. To our knowledge, this is the first
work that exploits iMAML for medical image segmentation. Our quantitative
results on publicly available skin and polyp datasets show that the proposed
method outperforms the naive supervised baseline model and two recent few-shot
segmentation approaches by large margins.",['cs.CV']
Using GANs to Augment Data for Cloud Image Segmentation Task,"While cloud/sky image segmentation has extensive real-world applications, a
large amount of labelled data is needed to train a highly accurate models to
perform the task. Scarcity of such volumes of cloud/sky images with
corresponding ground-truth binary maps makes it highly difficult to train such
complex image segmentation models. In this paper, we demonstrate the
effectiveness of using Generative Adversarial Networks (GANs) to generate data
to augment the training set in order to increase the prediction accuracy of
image segmentation model. We further present a way to estimate ground-truth
binary maps for the GAN-generated images to facilitate their effective use as
augmented images. Finally, we validate our work with different statistical
techniques.","['cs.CV', 'eess.IV']"
Semi-Supervised Domain Adaptation via Adaptive and Progressive Feature Alignment,"Contemporary domain adaptive semantic segmentation aims to address data
annotation challenges by assuming that target domains are completely
unannotated. However, annotating a few target samples is usually very
manageable and worthwhile especially if it improves the adaptation performance
substantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptive
image Segmentation network that employs a few labeled target samples as anchors
for adaptive and progressive feature alignment between labeled source samples
and unlabeled target samples. We position the few labeled target samples as
references that gauge the similarity between source and target features and
guide adaptive inter-domain alignment for learning more similar source
features. In addition, we replace the dissimilar source features by
high-confidence target features continuously during the iterative training
process, which achieves progressive intra-domain alignment between confident
and unconfident target features. Extensive experiments show the proposed SSDAS
greatly outperforms a number of baselines, i.e., UDA-based semantic
segmentation and SSDA-based image classification. In addition, SSDAS is
complementary and can be easily incorporated into UDA-based methods with
consistent improvements in domain adaptive semantic segmentation.",['cs.CV']
Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks,"In applied image segmentation tasks, the ability to provide numerous and
precise labels for training is paramount to the accuracy of the model at
inference time. However, this overhead is often neglected, and recently
proposed segmentation architectures rely heavily on the availability and
fidelity of ground truth labels to achieve state-of-the-art accuracies. Failure
to acknowledge the difficulty in creating adequate ground truths can lead to an
over-reliance on pre-trained models or a lack of adoption in real-world
applications. We introduce Points2Polygons (P2P), a model which makes use of
contextual metric learning techniques that directly addresses this problem.
Points2Polygons performs well against existing fully-supervised segmentation
baselines with limited training data, despite using lightweight segmentation
models (U-Net with a ResNet18 backbone) and having access to only weak labels
in the form of object centroids and no pre-training. We demonstrate this on
several different small but non-trivial datasets. We show that metric learning
using contextual data provides key insights for self-supervised tasks in
general, and allow segmentation models to easily generalize across
traditionally label-intensive domains in computer vision.",['cs.CV']
Attention-Guided Supervised Contrastive Learning for Semantic Segmentation,"Contrastive learning has shown superior performance in embedding global and
spatial invariant features in computer vision (e.g., image classification).
However, its overall success of embedding local and spatial variant features is
still limited, especially for semantic segmentation. In a per-pixel prediction
task, more than one label can exist in a single image for segmentation (e.g.,
an image contains both cat, dog, and grass), thereby it is difficult to define
'positive' or 'negative' pairs in a canonical contrastive learning setting. In
this paper, we propose an attention-guided supervised contrastive learning
approach to highlight a single semantic object every time as the target. With
our design, the same image can be embedded to different semantic clusters with
semantic attention (i.e., coerce semantic masks) as an additional input
channel. To achieve such attention, a novel two-stage training strategy is
presented. We evaluate the proposed method on multi-organ medical image
segmentation task, as our major task, with both in-house data and BTCV 2015
datasets. Comparing with the supervised and semi-supervised training
state-of-the-art in the backbone of ResNet-50, our proposed pipeline yields
substantial improvement of 5.53% and 6.09% in Dice score for both medical image
segmentation cohorts respectively. The performance of the proposed method on
natural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%
substantial improvement.","['cs.CV', 'cs.AI', 'cs.LG']"
IPatch: A Remote Adversarial Patch,"Applications such as autonomous vehicles and medical screening use deep
learning models to localize and identify hundreds of objects in a single frame.
In the past, it has been shown how an attacker can fool these models by placing
an adversarial patch within a scene. However, these patches must be placed in
the target location and do not explicitly alter the semantics elsewhere in the
image.
  In this paper, we introduce a new type of adversarial patch which alters a
model's perception of an image's semantics. These patches can be placed
anywhere within an image to change the classification or semantics of locations
far from the patch. We call this new class of adversarial examples `remote
adversarial patches' (RAP).
  We implement our own RAP called IPatch and perform an in-depth analysis on
image segmentation RAP attacks using five state-of-the-art architectures with
eight different encoders on the CamVid street view dataset. Moreover, we
demonstrate that the attack can be extended to object recognition models with
preliminary results on the popular YOLOv3 model. We found that the patch can
change the classification of a remote target region with a success rate of up
to 93% on average.","['cs.CV', 'cs.AI', 'cs.CR', 'cs.LG']"
nnDetection: A Self-configuring Method for Medical Object Detection,"Simultaneous localisation and categorization of objects in medical images,
also referred to as medical object detection, is of high clinical relevance
because diagnostic decisions often depend on rating of objects rather than e.g.
pixels. For this task, the cumbersome and iterative process of method
configuration constitutes a major research bottleneck. Recently, nnU-Net has
tackled this challenge for the task of image segmentation with great success.
Following nnU-Net's agenda, in this work we systematize and automate the
configuration process for medical object detection. The resulting
self-configuring method, nnDetection, adapts itself without any manual
intervention to arbitrary medical detection problems while achieving results en
par with or superior to the state-of-the-art. We demonstrate the effectiveness
of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10
further medical object detection tasks on public data sets for comprehensive
method evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection .",['cs.CV']
A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy,"Performing inference in graphs is a common task within several machine
learning problems, e.g., image segmentation, community detection, among others.
For a given undirected connected graph, we tackle the statistical problem of
exactly recovering an unknown ground-truth binary labeling of the nodes from a
single corrupted observation of each edge. Such problem can be formulated as a
quadratic combinatorial optimization problem over the boolean hypercube, where
it has been shown before that one can (with high probability and in polynomial
time) exactly recover the ground-truth labeling of graphs that have an
isoperimetric number that grows with respect to the number of nodes (e.g.,
complete graphs, regular expanders). In this work, we apply a powerful
hierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the
combinatorial problem. Motivated by empirical evidence on the improvement in
exact recoverability, we center our attention on the degree-4 SoS relaxation
and set out to understand the origin of such improvement from a graph
theoretical perspective. We show that the solution of the dual of the relaxed
problem is related to finding edge weights of the Johnson and Kneser graphs,
where the weights fulfill the SoS constraints and intuitively allow the input
graph to increase its algebraic connectivity. Finally, as byproduct of our
analysis, we derive a novel Cheeger-type lower bound for the algebraic
connectivity of graphs with signed edge weights.","['cs.LG', 'cs.AI', 'stat.ML']"
Analysis of Vision-based Abnormal Red Blood Cell Classification,"Identification of abnormalities in red blood cells (RBC) is key to diagnosing
a range of medical conditions from anaemia to liver disease. Currently this is
done manually, a time-consuming and subjective process. This paper presents an
automated process utilising the advantages of machine learning to increase
capacity and standardisation of cell abnormality detection, and its performance
is analysed. Three different machine learning technologies were used: a Support
Vector Machine (SVM), a classical machine learning technology; TabNet, a deep
learning architecture for tabular data; U-Net, a semantic segmentation network
designed for medical image segmentation. A critical issue was the highly
imbalanced nature of the dataset which impacts the efficacy of machine
learning. To address this, synthesising minority class samples in feature space
was investigated via Synthetic Minority Over-sampling Technique (SMOTE) and
cost-sensitive learning. A combination of these two methods is investigated to
improve the overall performance. These strategies were found to increase
sensitivity to minority classes. The impact of unknown cells on semantic
segmentation is demonstrated, with some evidence of the model applying learning
of labelled cells to these anonymous cells. These findings indicate both
classical models and new deep learning networks as promising methods in
automating RBC abnormality detection.",['cs.CV']
DAAIN: Detection of Anomalous and Adversarial Input using Normalizing Flows,"Despite much recent work, detecting out-of-distribution (OOD) inputs and
adversarial attacks (AA) for computer vision models remains a challenge. In
this work, we introduce a novel technique, DAAIN, to detect OOD inputs and AA
for image segmentation in a unified setting. Our approach monitors the inner
workings of a neural network and learns a density estimator of the activation
distribution. We equip the density estimator with a classification head to
discriminate between regular and anomalous inputs. To deal with the
high-dimensional activation-space of typical segmentation networks, we
subsample them to obtain a homogeneous spatial and layer-wise coverage. The
subsampling pattern is chosen once per monitored model and kept fixed for all
inputs. Since the attacker has access to neither the detection model nor the
sampling key, it becomes harder for them to attack the segmentation network, as
the attack cannot be backpropagated through the detector. We demonstrate the
effectiveness of our approach using an ESPNet trained on the Cityscapes dataset
as segmentation model, an affine Normalizing Flow as density estimator and use
blue noise to ensure homogeneous sampling. Our model can be trained on a single
GPU making it compute efficient and deployable without requiring specialized
accelerators.","['cs.CV', 'cs.CR', 'cs.LG']"
CFPNet-M: A Light-Weight Encoder-Decoder Based Network for Multimodal Biomedical Image Real-Time Segmentation,"Currently, developments of deep learning techniques are providing
instrumental to identify, classify, and quantify patterns in medical images.
Segmentation is one of the important applications in medical image analysis. In
this regard, U-Net is the predominant approach to medical image segmentation
tasks. However, we found that those U-Net based models have limitations in
several aspects, for example, millions of parameters in the U-Net consuming
considerable computation resource and memory, lack of global information, and
missing some tough objects. Therefore, we applied two modifications to improve
the U-Net model: 1) designed and added the dilated channel-wise CNN module, 2)
simplified the U shape network. Based on these two modifications, we proposed a
novel light-weight architecture -- Channel-wise Feature Pyramid Network for
Medicine (CFPNet-M). To evaluate our method, we selected five datasets with
different modalities: thermography, electron microscopy, endoscopy, dermoscopy,
and digital retinal images. And we compared its performance with several models
having different parameter scales. This paper also involves our previous
studies of DC-UNet and some commonly used light-weight neural networks. We
applied the Tanimoto similarity instead of the Jaccard index for gray-level
image measurements. By comparison, CFPNet-M achieves comparable segmentation
results on all five medical datasets with only 0.65 million parameters, which
is about 2% of U-Net, and 8.8 MB memory. Meanwhile, the inference speed can
reach 80 FPS on a single RTX 2070Ti GPU with the 256 by 192 pixels input size.","['cs.CV', 'eess.IV']"
High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection,"As the most economical and routine auxiliary examination in the diagnosis of
root canal treatment, oral X-ray has been widely used by stomatologists. It is
still challenging to segment the tooth root with a blurry boundary for the
traditional image segmentation method. To this end, we propose a model for
high-resolution segmentation based on polynomial curve fitting with landmark
detection (HS-PCL). It is based on detecting multiple landmarks evenly
distributed on the edge of the tooth root to fit a smooth polynomial curve as
the segmentation of the tooth root, thereby solving the problem of fuzzy edge.
In our model, a maximum number of the shortest distances algorithm (MNSDA) is
proposed to automatically reduce the negative influence of the wrong landmarks
which are detected incorrectly and deviate from the tooth root on the fitting
result. Our numerical experiments demonstrate that the proposed approach not
only reduces Hausdorff95 (HD95) by 33.9% and Average Surface Distance (ASD) by
42.1% compared with the state-of-the-art method, but it also achieves excellent
results on the minute quantity of datasets, which greatly improves the
feasibility of automatic root canal therapy evaluation by medical image
computing.",['cs.CV']
Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data,"Estimating the amount of electricity that can be produced by rooftop
photovoltaic systems is a time-consuming process that requires on-site
measurements, a difficult task to achieve on a large scale. In this paper, we
present an approach to estimate the solar potential of rooftops based on their
location and architectural characteristics, as well as the amount of solar
radiation they receive annually. Our technique uses computer vision to achieve
semantic segmentation of roof sections and roof objects on the one hand, and a
machine learning model based on structured building features to predict roof
pitch on the other hand. We then compute the azimuth and maximum number of
solar panels that can be installed on a rooftop with geometric approaches.
Finally, we compute precise shading masks and combine them with solar
irradiation data that enables us to estimate the yearly solar potential of a
rooftop.","['cs.CV', 'cs.LG']"
Learning Fuzzy Clustering for SPECT/CT Segmentation via Convolutional Neural Networks,"Quantitative bone single-photon emission computed tomography (QBSPECT) has
the potential to provide a better quantitative assessment of bone metastasis
than planar bone scintigraphy due to its ability to better quantify activity in
overlapping structures. An important element of assessing response of bone
metastasis is accurate image segmentation. However, limited by the properties
of QBSPECT images, the segmentation of anatomical regions-of-interests (ROIs)
still relies heavily on the manual delineation by experts. This work proposes a
fast and robust automated segmentation method for partitioning a QBSPECT image
into lesion, bone, and background. We present a new unsupervised segmentation
loss function and its semi- and supervised variants for training a
convolutional neural network (ConvNet). The loss functions were developed based
on the objective function of the classical Fuzzy C-means (FCM) algorithm. We
conducted a comprehensive study to compare our proposed methods with ConvNets
trained using supervised loss functions and conventional clustering methods.
The Dice similarity coefficient (DSC) and several other metrics were used as
figures of merit as applied to the task of delineating lesion and bone in both
simulated and clinical SPECT/CT images. We experimentally demonstrated that the
proposed methods yielded good segmentation results on a clinical dataset even
though the training was done using realistic simulated images. A ConvNet-based
image segmentation method that uses novel loss functions was developed and
evaluated. The method can operate in unsupervised, semi-supervised, or
fully-supervised modes depending on the availability of annotated training
data. The results demonstrated that the proposed method provides fast and
robust lesion and bone segmentation for QBSPECT/CT. The method can potentially
be applied to other medical image segmentation applications.",['cs.CV']
Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions,"Generative flows and diffusion models have been predominantly trained on
ordinal data, for example natural images. This paper introduces two extensions
of flows and diffusion for categorical data such as language or image
segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined
by a composition of a continuous distribution (such as a normalizing flow), and
an argmax function. To optimize this model, we learn a probabilistic inverse
for the argmax that lifts the categorical data to a continuous space.
Multinomial Diffusion gradually adds categorical noise in a diffusion process,
for which the generative denoising process is learned. We demonstrate that our
method outperforms existing dequantization approaches on text modelling and
modelling on image segmentation maps in log-likelihood.","['stat.ML', 'cs.CL', 'cs.LG']"
Learning Uncertainty For Safety-Oriented Semantic Segmentation In Autonomous Driving,"In this paper, we show how uncertainty estimation can be leveraged to enable
safety critical image segmentation in autonomous driving, by triggering a
fallback behavior if a target accuracy cannot be guaranteed. We introduce a new
uncertainty measure based on disagreeing predictions as measured by a
dissimilarity function. We propose to estimate this dissimilarity by training a
deep neural architecture in parallel to the task-specific network. It allows
this observer to be dedicated to the uncertainty estimation, and let the
task-specific network make predictions. We propose to use self-supervision to
train the observer, which implies that our method does not require additional
training data. We show experimentally that our proposed approach is much less
computationally intensive at inference time than competing methods (e.g.
MCDropout), while delivering better results on safety-oriented evaluation
metrics on the CamVid dataset, especially in the case of glare artifacts.",['cs.CV']
BoundarySqueeze: Image Segmentation as Boundary Squeezing,"We propose a novel method for fine-grained high-quality image segmentation of
both objects and scenes. Inspired by dilation and erosion from morphological
image processing techniques, we treat the pixel level segmentation problems as
squeezing object boundary. From this perspective, we propose \textbf{Boundary
Squeeze} module: a novel and efficient module that squeezes the object boundary
from both inner and outer directions which leads to precise mask
representation. To generate such squeezed representation, we propose a new
bidirectionally flow-based warping process and design specific loss signals to
supervise the learning process. Boundary Squeeze Module can be easily applied
to both instance and semantic segmentation tasks as a plug-and-play module by
building on top of existing models. We show that our simple yet effective
design can lead to high qualitative results on several different datasets and
we also provide several different metrics on boundary to prove the
effectiveness over previous work. Moreover, the proposed module is
light-weighted and thus has potential for practical usage. Our method yields
large gains on COCO, Cityscapes, for both instance and semantic segmentation
and outperforms previous state-of-the-art PointRend in both accuracy and speed
under the same setting. Code and model will be available.",['cs.CV']
Gaussian Dynamic Convolution for Efficient Single-Image Segmentation,"Interactive single-image segmentation is ubiquitous in the scientific and
commercial imaging software. In this work, we focus on the single-image
segmentation problem only with some seeds such as scribbles. Inspired by the
dynamic receptive field in the human being's visual system, we propose the
Gaussian dynamic convolution (GDC) to fast and efficiently aggregate the
contextual information for neural networks. The core idea is randomly selecting
the spatial sampling area according to the Gaussian distribution offsets. Our
GDC can be easily used as a module to build lightweight or complex segmentation
networks. We adopt the proposed GDC to address the typical single-image
segmentation tasks. Furthermore, we also build a Gaussian dynamic pyramid
Pooling to show its potential and generality in common semantic segmentation.
Experiments demonstrate that the GDC outperforms other existing convolutions on
three benchmark segmentation datasets including Pascal-Context, Pascal-VOC
2012, and Cityscapes. Additional experiments are also conducted to illustrate
that the GDC can produce richer and more vivid features compared with other
convolutions. In general, our GDC is conducive to the convolutional neural
networks to form an overall impression of the image.",['cs.CV']
Full Page Handwriting Recognition via Image to Sequence Extraction,"We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']"
"Uma implementao do jogo Pedra, Papel e Tesoura utilizando Visao Computacional","This paper presents a game, controlled by computer vision, in identification
of hand gestures (hand-tracking). The proposed work is based on image
segmentation and construction of a convex hull with Jarvis Algorithm , and
determination of the pattern based on the extraction of area characteristics in
the convex hull.",['cs.CV']
Deep Active Contours Using Locally Controlled Distance Vector Flow,"Active contours Model (ACM) has been extensively used in computer vision and
image processing. In recent studies, Convolutional Neural Networks (CNNs) have
been combined with active contours replacing the user in the process of contour
evolution and image segmentation to eliminate limitations associated with ACM's
dependence on parameters of the energy functional and initialization. However,
prior works did not aim for automatic initialization which is addressed here.
In addition to manual initialization, current methods are highly sensitive to
initial location and fail to delineate borders accurately. We propose a fully
automatic image segmentation method to address problems of manual
initialization, insufficient capture range, and poor convergence to boundaries,
in addition to the problem of assignment of energy functional parameters. We
train two CNNs, which predict active contour weighting parameters and generate
a ground truth mask to extract Distance Transform (DT) and an initialization
circle. Distance transform is used to form a vector field pointing from each
pixel of the image towards the closest point on the boundary, the size of which
is equal to the Euclidean distance map. We evaluate our method on four publicly
available datasets including two building instance segmentation datasets,
Vaihingen and Bing huts, and two mammography image datasets, INBreast and
DDSM-BCRP. Our approach outperforms latest research by 0.59 ans 2.39 percent in
mean Intersection-over-Union (mIoU), 7.38 and 8.62 percent in Boundary F-score
(BoundF) for Vaihingen and Bing huts datasets, respectively. Dice similarity
coefficient for the INBreast and DDSM-BCRP datasets is 94.23% and 90.89%,
respectively indicating our method is comparable to state-of-the-art
frameworks.",['cs.CV']
SemSegLoss: A python package of loss functions for semantic segmentation,"Image Segmentation has been an active field of research as it has a wide
range of applications, ranging from automated disease detection to self-driving
cars. In recent years, various research papers proposed different loss
functions used in case of biased data, sparse segmentation, and unbalanced
dataset. In this paper, we introduce SemSegLoss, a python package consisting of
some of the well-known loss functions widely used for image segmentation. It is
developed with the intent to help researchers in the development of novel loss
functions and perform an extensive set of experiments on model architectures
for various applications. The ease-of-use and flexibility of the presented
package have allowed reducing the development time and increased evaluation
strategies of machine learning models for semantic segmentation. Furthermore,
different applications that use image segmentation can use SemSegLoss because
of the generality of its functions. This wide range of applications will lead
to the development and growth of AI across all industries.","['cs.LG', 'cs.CV', 'eess.IV']"
Finding an Unsupervised Image Segmenter in Each of Your Deep Generative Models,"Recent research has shown that numerous human-interpretable directions exist
in the latent space of GANs. In this paper, we develop an automatic procedure
for finding directions that lead to foreground-background image separation, and
we use these directions to train an image segmentation model without human
supervision. Our method is generator-agnostic, producing strong segmentation
results with a wide range of different GAN architectures. Furthermore, by
leveraging GANs pretrained on large datasets such as ImageNet, we are able to
segment images from a range of domains without further training or finetuning.
Evaluating our method on image segmentation benchmarks, we compare favorably to
prior work while using neither human supervision nor access to the training
data. Broadly, our results demonstrate that automatically extracting
foreground-background structure from pretrained deep generative models can
serve as a remarkably effective substitute for human supervision.","['cs.CV', 'cs.AI']"
Voxel-level Siamese Representation Learning for Abdominal Multi-Organ Segmentation,"Recent works in medical image segmentation have actively explored various
deep learning architectures or objective functions to encode high-level
features from volumetric data owing to limited image annotations. However, most
existing approaches tend to ignore cross-volume global context and define
context relations in the decision space. In this work, we propose a novel
voxel-level Siamese representation learning method for abdominal multi-organ
segmentation to improve representation space. The proposed method enforces
voxel-wise feature relations in the representation space for leveraging limited
datasets more comprehensively to achieve better performance. Inspired by recent
progress in contrastive learning, we suppressed voxel-wise relations from the
same class to be projected to the same point without using negative samples.
Moreover, we introduce a multi-resolution context aggregation method that
aggregates features from multiple hidden layers, which encodes both the global
and local contexts for segmentation. Our experiments on the multi-organ dataset
outperformed the existing approaches by 2% in Dice score coefficient. The
qualitative visualizations of the representation spaces demonstrate that the
improvements were gained primarily by a disentangled feature space.",['cs.CV']
CheXseg: Combining Expert Annotations with DNN-generated Saliency Maps for X-ray Segmentation,"Medical image segmentation models are typically supervised by expert
annotations at the pixel-level, which can be expensive to acquire. In this
work, we propose a method that combines the high quality of pixel-level expert
annotations with the scale of coarse DNN-generated saliency maps for training
multi-label semantic segmentation models. We demonstrate the application of our
semi-supervised method, which we call CheXseg, on multi-label chest X-ray
interpretation. We find that CheXseg improves upon the performance (mIoU) of
fully-supervised methods that use only pixel-level expert annotations by 9.7%
and weakly-supervised methods that use only DNN-generated saliency maps by
73.1%. Our best method is able to match radiologist agreement on three out of
ten pathologies and reduces the overall performance gap by 57.2% as compared to
weakly-supervised methods.","['cs.CV', 'cs.AI', 'cs.LG']"
GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving,"Scalable sensor simulation is an important yet challenging open problem for
safety-critical domains such as self-driving. Current works in image simulation
either fail to be photorealistic or do not model the 3D environment and the
dynamic objects within, losing high-level control and physical realism. In this
paper, we present GeoSim, a geometry-aware image composition process which
synthesizes novel urban driving scenarios by augmenting existing images with
dynamic objects extracted from other scenes and rendered at novel poses.
Towards this goal, we first build a diverse bank of 3D objects with both
realistic geometry and appearance from sensor data. During simulation, we
perform a novel geometry-aware simulation-by-composition procedure which 1)
proposes plausible and realistic object placements into a given scene, 2)
render novel views of dynamic objects from the asset bank, and 3) composes and
blends the rendered image segments. The resulting synthetic images are
realistic, traffic-aware, and geometrically consistent, allowing our approach
to scale to complex use cases. We demonstrate two such important applications:
long-range realistic video simulation across multiple camera sensors, and
synthetic data generation for data augmentation on downstream segmentation
tasks. Please check https://tmux.top/publication/geosim/ for high-resolution
video results.","['cs.CV', 'cs.AI', 'cs.GR', 'cs.RO']"
Cross-Modal Progressive Comprehension for Referring Segmentation,"Given a natural language expression and an image/video, the goal of referring
segmentation is to produce the pixel-level masks of the entities described by
the subject of the expression. Previous approaches tackle this problem by
implicit feature interaction and fusion between visual and linguistic
modalities in a one-stage manner. However, human tends to solve the referring
problem in a progressive manner based on informative words in the expression,
i.e., first roughly locating candidate entities and then distinguishing the
target one. In this paper, we propose a Cross-Modal Progressive Comprehension
(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I
(Image) module and a CMPC-V (Video) module to improve referring image and video
segmentation models. For image data, our CMPC-I module first employs entity and
attribute words to perceive all the related entities that might be considered
by the expression. Then, the relational words are adopted to highlight the
target entity as well as suppress other irrelevant ones by spatial graph
reasoning. For video data, our CMPC-V module further exploits action words
based on CMPC-I to highlight the correct entity matched with the action cues by
temporal graph reasoning. In addition to the CMPC, we also introduce a simple
yet effective Text-Guided Feature Exchange (TGFE) module to integrate the
reasoned multimodal features corresponding to different levels in the visual
backbone under the guidance of textual information. In this way, multi-level
features can communicate with each other and be mutually refined based on the
textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or
video version referring segmentation frameworks and our frameworks achieve new
state-of-the-art performances on four referring image segmentation benchmarks
and three referring video segmentation benchmarks respectively.","['cs.CV', 'cs.MM']"
Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation,"Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, manually annotating
medical data is often laborious, and most existing learning-based approaches
fail to accurately delineate object boundaries without effective geometric
constraints. Contrastive learning, a sub-area of self-supervised learning, has
recently been noted as a promising direction in multiple application fields. In
this work, we present a novel Contrastive Voxel-wise Representation Learning
(CVRL) method with geometric constraints to learn global-local visual
representations for volumetric medical image segmentation with limited
annotations. Our framework can effectively learn global and local features by
capturing 3D spatial context and rich anatomical information. Specifically, we
introduce a voxel-to-volume contrastive algorithm to learn global information
from 3D images, and propose to perform local voxel-to-voxel contrast to
explicitly make use of local cues in the embedding space. Moreover, we
integrate an elastic interaction-based active contour model as a geometric
regularization term to enable fast and reliable object delineations in an
end-to-end learning manner. Results on the Atrial Segmentation Challenge
dataset demonstrate superiority of our proposed scheme, especially in a setting
with a very limited number of annotated data.","['cs.CV', 'cs.LG', 'eess.IV']"
VICE: Visual Identification and Correction of Neural Circuit Errors,"A connectivity graph of neurons at the resolution of single synapses provides
scientists with a tool for understanding the nervous system in health and
disease. Recent advances in automatic image segmentation and synapse prediction
in electron microscopy (EM) datasets of the brain have made reconstructions of
neurons possible at the nanometer scale. However, automatic segmentation
sometimes struggles to segment large neurons correctly, requiring human effort
to proofread its output. General proofreading involves inspecting large volumes
to correct segmentation errors at the pixel level, a visually intensive and
time-consuming process. This paper presents the design and implementation of an
analytics framework that streamlines proofreading, focusing on
connectivity-related errors. We accomplish this with automated likely-error
detection and synapse clustering that drives the proofreading effort with
highly interactive 3D visualizations. In particular, our strategy centers on
proofreading the local circuit of a single cell to ensure a basic level of
completeness. We demonstrate our framework's utility with a user study and
report quantitative and subjective feedback from our users. Overall, users find
the framework more efficient for proofreading, understanding evolving graphs,
and sharing error correction strategies.",['cs.CV']
A Novel Falling-Ball Algorithm for Image Segmentation,"Image segmentation refers to the separation of objects from the background,
and has been one of the most challenging aspects of digital image processing.
Practically it is impossible to design a segmentation algorithm which has 100%
accuracy, and therefore numerous segmentation techniques have been proposed in
the literature, each with certain limitations. In this paper, a novel
Falling-Ball algorithm is presented, which is a region-based segmentation
algorithm, and an alternative to watershed transform (based on waterfall
model). The proposed algorithm detects the catchment basins by assuming that a
ball falling from hilly terrains will stop in a catchment basin. Once catchment
basins are identified, the association of each pixel with one of the catchment
basin is obtained using multi-criterion fuzzy logic. Edges are constructed by
dividing image into different catchment basins with the help of a membership
function. Finally closed contour algorithm is applied to find closed regions
and objects within closed regions are segmented using intensity information.
The performance of the proposed algorithm is evaluated both objectively as well
as subjectively. Simulation results show that the proposed algorithms gives
superior performance over conventional Sobel edge detection methods and the
watershed segmentation algorithm. For comparative analysis, various comparison
methods are used for demonstrating the superiority of proposed methods over
existing segmentation methods.","['cs.CV', 'cs.CY']"
A Large-Scale Benchmark for Food Image Segmentation,"Food image segmentation is a critical and indispensible task for developing
health-related applications such as estimating food calories and nutrients.
Existing food image segmentation models are underperforming due to two reasons:
(1) there is a lack of high quality food image datasets with fine-grained
ingredient labels and pixel-wise location masks -- the existing datasets either
carry coarse ingredient labels or are small in size; and (2) the complex
appearance of food makes it difficult to localize and recognize ingredients in
food images, e.g., the ingredients may overlap one another in the same image,
and the identical ingredient may appear distinctly in different food images. In
this work, we build a new food image dataset FoodSeg103 (and its extension
FoodSeg154) containing 9,490 images. We annotate these images with 154
ingredient classes and each image has an average of 6 ingredient labels and
pixel-wise masks. In addition, we propose a multi-modality pre-training
approach called ReLeM that explicitly equips a segmentation model with rich and
semantic food knowledge. In experiments, we use three popular semantic
segmentation methods (i.e., Dilated Convolution based, Feature Pyramid based,
and Vision Transformer based) as baselines, and evaluate them as well as ReLeM
on our new datasets. We believe that the FoodSeg103 (and its extension
FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to
facilitate future works on fine-grained food image understanding. We make all
these datasets and methods public at
\url{https://xiongweiwu.github.io/foodseg103.html}.","['cs.CV', 'cs.MM']"
Boundary-Aware Segmentation Network for Mobile and Web Applications,"Although deep models have greatly improved the accuracy and robustness of
image segmentation, obtaining segmentation results with highly accurate
boundaries and fine structures is still a challenging problem. In this paper,
we propose a simple yet powerful Boundary-Aware Segmentation Network (BASNet),
which comprises a predict-refine architecture and a hybrid loss, for highly
accurate image segmentation. The predict-refine architecture consists of a
densely supervised encoder-decoder network and a residual refinement module,
which are respectively used to predict and refine a segmentation probability
map. The hybrid loss is a combination of the binary cross entropy, structural
similarity and intersection-over-union losses, which guide the network to learn
three-level (ie, pixel-, patch- and map- level) hierarchy representations. We
evaluate our BASNet on two reverse tasks including salient object segmentation,
camouflaged object segmentation, showing that it achieves very competitive
performance with sharp segmentation boundaries. Importantly, BASNet runs at
over 70 fps on a single GPU which benefits many potential real applications.
Based on BASNet, we further developed two (close to) commercial applications:
AR COPY & PASTE, in which BASNet is integrated with augmented reality for
""COPYING"" and ""PASTING"" real-world objects, and OBJECT CUT, which is a
web-based tool for automatic object background removal. Both applications have
already drawn huge amount of attention and have important real-world impacts.
The code and two applications will be publicly available at:
https://github.com/NathanUA/BASNet.",['cs.CV']
Auxiliary Learning by Implicit Differentiation,"Training neural networks with auxiliary tasks is a common practice for
improving the performance on a main task of interest. Two main challenges arise
in this multi-task learning setting: (i) designing useful auxiliary tasks; and
(ii) combining auxiliary tasks into a single coherent loss. Here, we propose a
novel framework, AuxiLearn, that targets both challenges based on implicit
differentiation. First, when useful auxiliaries are known, we propose learning
a network that combines all losses into a single coherent objective function.
This network can learn non-linear interactions between tasks. Second, when no
useful auxiliary task is known, we describe how to learn a network that
generates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series
of tasks and domains, including image segmentation and learning with attributes
in the low data regime, and find that it consistently outperforms competing
methods.","['cs.CV', 'cs.LG', 'stat.ML']"
MDA-Net: Multi-Dimensional Attention-Based Neural Network for 3D Image Segmentation,"Segmenting an entire 3D image often has high computational complexity and
requires large memory consumption; by contrast, performing volumetric
segmentation in a slice-by-slice manner is efficient but does not fully
leverage the 3D data. To address this challenge, we propose a multi-dimensional
attention network (MDA-Net) to efficiently integrate slice-wise, spatial, and
channel-wise attention into a U-Net based network, which results in high
segmentation accuracy with a low computational cost. We evaluate our model on
the MICCAI iSeg and IBSR datasets, and the experimental results demonstrate
consistent improvements over existing methods.","['cs.CV', 'eess.IV']"
A Characteristic Function-based Algorithm for Geodesic Active Contours,"Active contour models have been widely used in image segmentation, and the
level set method (LSM) is the most popular approach for solving the models, via
implicitly representing the contour by a level set function. However, the LSM
suffers from high computational burden and numerical instability, requiring
additional regularization terms or re-initialization techniques. In this paper,
we use characteristic functions to implicitly represent the contours, propose a
new representation to the geodesic active contours and derive an efficient
algorithm termed as the iterative convolution-thresholding method (ICTM).
Compared to the LSM, the ICTM is simpler and much more efficient. In addition,
the ICTM enjoys most desired features of the level set-based methods. Extensive
experiments, on 2D synthetic, 2D ultrasound, 3D CT, and 3D MR images for
nodule, organ and lesion segmentation, demonstrate that the proposed method not
only obtains comparable or even better segmentation results (compared to the
LSM) but also achieves significant acceleration.","['cs.CV', 'cs.NA', 'math.NA']"
Geodesic Paths for Image Segmentation with Implicit Region-based Homogeneity Enhancement,"Minimal paths are regarded as a powerful and efficient tool for boundary
detection and image segmentation due to its global optimality and the
well-established numerical solutions such as fast marching method. In this
paper, we introduce a flexible interactive image segmentation model based on
the Eikonal partial differential equation (PDE) framework in conjunction with
region-based homogeneity enhancement. A key ingredient in the introduced model
is the construction of local geodesic metrics, which are capable of integrating
anisotropic and asymmetric edge features, implicit region-based homogeneity
features and/or curvature regularization. The incorporation of the region-based
homogeneity features into the metrics considered relies on an implicit
representation of these features, which is one of the contributions of this
work. Moreover, we also introduce a way to build simple closed contours as the
concatenation of two disjoint open curves. Experimental results prove that the
proposed model indeed outperforms state-of-the-art minimal paths-based image
segmentation approaches.","['cs.CV', 'cs.CG']"
Impact of individual rater style on deep learning uncertainty in medical imaging segmentation,"While multiple studies have explored the relation between inter-rater
variability and deep learning model uncertainty in medical segmentation tasks,
little is known about the impact of individual rater style. This study
quantifies rater style in the form of bias and consistency and explores their
impacts when used to train deep learning models. Two multi-rater public
datasets were used, consisting of brain multiple sclerosis lesion and spinal
cord grey matter segmentation. On both datasets, results show a correlation
($R^2 = 0.60$ and $0.93$) between rater bias and deep learning uncertainty. The
impact of label fusion between raters' annotations on this relationship is also
explored, and we show that multi-center consensuses are more effective than
single-center consensuses to reduce uncertainty, since rater style is mostly
center-specific.","['cs.CV', 'eess.IV']"
Bayesian Logistic Shape Model Inference: application to cochlea image segmentation,"Incorporating shape information is essential for the delineation of many
organs and anatomical structures in medical images. While previous work has
mainly focused on parametric spatial transformations applied on reference
template shapes, in this paper, we address the Bayesian inference of parametric
shape models for segmenting medical images with the objective to provide
interpretable results. The proposed framework defines a likelihood appearance
probability and a prior label probability based on a generic shape function
through a logistic function. A reference length parameter defined in the
sigmoid controls the trade-off between shape and appearance information. The
inference of shape parameters is performed within an Expectation-Maximisation
approach where a Gauss-Newton optimization stage allows to provide an
approximation of the posterior probability of shape parameters. This framework
is applied to the segmentation of cochlea structures from clinical CT images
constrained by a 10 parameter shape model. It is evaluated on three different
datasets, one of which includes more than 200 patient images. The results show
performances comparable to supervised methods and better than previously
proposed unsupervised ones. It also enables an analysis of parameter
distributions and the quantification of segmentation uncertainty including the
effect of the shape model.","['cs.CV', 'cs.AI']"
Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation,"Recently, referring image segmentation has aroused widespread interest.
Previous methods perform the multi-modal fusion between language and vision at
the decoding side of the network. And, linguistic feature interacts with visual
feature of each scale separately, which ignores the continuous guidance of
language to multi-scale visual features. In this work, we propose an encoder
fusion network (EFN), which transforms the visual encoder into a multi-modal
feature learning network, and uses language to refine the multi-modal features
progressively. Moreover, a co-attention mechanism is embedded in the EFN to
realize the parallel update of multi-modal features, which can promote the
consistent of the cross-modal information representation in the semantic space.
Finally, we propose a boundary enhancement module (BEM) to make the network pay
more attention to the fine structure. The experiment results on four benchmark
datasets demonstrate that the proposed approach achieves the state-of-the-art
performance under different evaluation metrics without any post-processing.",['cs.CV']
Combining Supervised and Un-supervised Learning for Automatic Citrus Segmentation,"Citrus segmentation is a key step of automatic citrus picking. While most
current image segmentation approaches achieve good segmentation results by
pixel-wise segmentation, these supervised learning-based methods require a
large amount of annotated data, and do not consider the continuous temporal
changes of citrus position in real-world applications. In this paper, we first
train a simple CNN with a small number of labelled citrus images in a
supervised manner, which can roughly predict the citrus location from each
frame. Then, we extend a state-of-the-art unsupervised learning approach to
pre-learn the citrus's potential movements between frames from unlabelled
citrus's videos. To take advantages of both networks, we employ the multimodal
transformer to combine supervised learned static information and unsupervised
learned movement information. The experimental results show that combing both
network allows the prediction accuracy reached at 88.3$\%$ IOU and 93.6$\%$
precision, outperforming the original supervised baseline 1.2$\%$ and 2.4$\%$.
Compared with most of the existing citrus segmentation methods, our method uses
a small amount of supervised data and a large number of unsupervised data,
while learning the pixel level location information and the temporal
information of citrus changes to enhance the segmentation effect.",['cs.CV']
Computer vision for liquid samples in hospitals and medical labs using hierarchical image segmentation and relations prediction,"This work explores the use of computer vision for image segmentation and
classification of medical fluid samples in transparent containers (for example,
tubes, syringes, infusion bags). Handling fluids such as infusion fluids,
blood, and urine samples is a significant part of the work carried out in
medical labs and hospitals. The ability to accurately identify and segment the
liquids and the vessels that contain them from images can help in automating
such processes. Modern computer vision typically involves training deep neural
nets on large datasets of annotated images. This work presents a new dataset
containing 1,300 annotated images of medical samples involving vessels
containing liquids and solid material. The images are annotated with the type
of liquid (e.g., blood, urine), the phase of the material (e.g., liquid, solid,
foam, suspension), the type of vessel (e.g., syringe, tube, cup, infusion
bottle/bag), and the properties of the vessel (transparent, opaque). In
addition, vessel parts such as corks, labels, spikes, and valves are annotated.
Relations and hierarchies between vessels and materials are also annotated,
such as which vessel contains which material or which vessels are linked or
contain each other. Three neural networks are trained on the dataset: One
network learns to detect vessels, a second net detects the materials and parts
inside each vessel, and a third net identifies relationships and connectivity
between vessels.",['cs.CV']
A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation,"The Voronoi diagram-based dual-front active contour models are known as a
powerful and efficient way for addressing the image segmentation and domain
partitioning problems. In the basic formulation of the dual-front models, the
evolving contours can be considered as the interfaces of adjacent Voronoi
regions. Among these dual-front models, a crucial ingredient is regarded as the
geodesic metrics by which the geodesic distances and the corresponding Voronoi
diagram can be estimated. In this paper, we introduce a type of asymmetric
quadratic metrics dual-front model. The metrics considered are built by the
integration of the image features and a vector field derived from the evolving
contours. The use of the asymmetry enhancement can reduce the risk of contour
shortcut or leakage problems especially when the initial contours are far away
from the target boundaries or the images have complicated intensity
distributions. Moreover, the proposed dual-front model can be applied for image
segmentation in conjunction with various region-based homogeneity terms. The
numerical experiments on both synthetic and real images show that the proposed
dual-front model indeed achieves encouraging results.","['cs.CV', 'cs.CG']"
Beyond pixel-wise supervision for segmentation: A few global shape descriptors might be surprisingly good!,"Standard losses for training deep segmentation networks could be seen as
individual classifications of pixels, instead of supervising the global shape
of the predicted segmentations. While effective, they require exact knowledge
of the label of each pixel in an image.
  This study investigates how effective global geometric shape descriptors
could be, when used on their own as segmentation losses for training deep
networks. Not only interesting theoretically, there exist deeper motivations to
posing segmentation problems as a reconstruction of shape descriptors:
Annotations to obtain approximations of low-order shape moments could be much
less cumbersome than their full-mask counterparts, and anatomical priors could
be readily encoded into invariant shape descriptions, which might alleviate the
annotation burden. Also, and most importantly, we hypothesize that, given a
task, certain shape descriptions might be invariant across image acquisition
protocols/modalities and subject populations, which might open interesting
research avenues for generalization in medical image segmentation.
  We introduce and formulate a few shape descriptors in the context of deep
segmentation, and evaluate their potential as standalone losses on two
different challenging tasks. Inspired by recent works in constrained
optimization for deep networks, we propose a way to use those descriptors to
supervise segmentation, without any pixel-level label. Very surprisingly, as
little as 4 descriptors values per class can approach the performance of a
segmentation mask with 65k individual discrete labels. We also found that shape
descriptors can be a valid way to encode anatomical priors about the task,
enabling to leverage expert knowledge without additional annotations. Our
implementation is publicly available and can be easily extended to other tasks
and descriptors: https://github.com/hkervadec/shape_descriptors",['cs.CV']
CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for Embedded Vision Systems and Applications,"The field of view (FOV) of convolutional neural networks is highly related to
the accuracy of inference. Dilated convolutions are known as an effective
solution to the problems which require large FOVs. However, for general-purpose
hardware or dedicated hardware, it usually takes extra time to handle dilated
convolutions compared with standard convolutions. In this paper, we propose a
network module, Cascaded and Separable Structure of Dilated (CASSOD)
Convolution, and a special hardware system to handle the CASSOD networks
efficiently. A CASSOD-Net includes multiple cascaded $2 \times 2$ dilated
filters, which can be used to replace the traditional $3 \times 3$ dilated
filters without decreasing the accuracy of inference. Two example applications,
face detection and image segmentation, are tested with dilated convolutions and
the proposed CASSOD modules. The new network for face detection achieves higher
accuracy than the previous work with only 47% of filter weights in the dilated
convolution layers of the context module. Moreover, the proposed hardware
system can accelerate the computations of dilated convolutions, and it is 2.78
times faster than traditional hardware systems when the filter size is $3
\times 3$.","['cs.CV', 'cs.AR']"
Distributional Gaussian Process Layers for Outlier Detection in Image Segmentation,"We propose a parameter efficient Bayesian layer for hierarchical
convolutional Gaussian Processes that incorporates Gaussian Processes operating
in Wasserstein-2 space to reliably propagate uncertainty. This directly
replaces convolving Gaussian Processes with a distance-preserving affine
operator on distributions. Our experiments on brain tissue-segmentation show
that the resulting architecture approaches the performance of well-established
deterministic segmentation algorithms (U-Net), which has never been achieved
with previous hierarchical Gaussian Processes. Moreover, by applying the same
segmentation model to out-of-distribution data (i.e., images with pathology
such as brain tumors), we show that our uncertainty estimates result in
out-of-distribution detection that outperforms the capabilities of previous
Bayesian networks and reconstruction-based approaches that learn normative
distributions.","['stat.ML', 'cs.LG']"
Two stages for visual object tracking,"Siamese-based trackers have achived promising performance on visual object
tracking tasks. Most existing Siamese-based trackers contain two separate
branches for tracking, including classification branch and bounding box
regression branch. In addition, image segmentation provides an alternative way
to obetain the more accurate target region. In this paper, we propose a novel
tracker with two-stages: detection and segmentation. The detection stage is
capable of locating the target by Siamese networks. Then more accurate tracking
results are obtained by segmentation module given the coarse state estimation
in the first stage. We conduct experiments on four benchmarks. Our approach
achieves state-of-the-art results, with the EAO of 52.6$\%$ on VOT2016,
51.3$\%$ on VOT2018, and 39.0$\%$ on VOT2019 datasets, respectively.","['cs.CV', 'cs.AI']"
Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation,"Pixel-wise segmentation is one of the most data and annotation hungry tasks
in our field. Providing representative and accurate annotations is often
mission-critical especially for challenging medical applications. In this
paper, we propose a semi-weakly supervised segmentation algorithm to overcome
this barrier. Our approach is based on a new formulation of deep supervision
and student-teacher model and allows for easy integration of different
supervision signals. In contrast to previous work, we show that care has to be
taken how deep supervision is integrated in lower layers and we present
multi-label deep supervision as the most important secret ingredient for
success. With our novel training regime for segmentation that flexibly makes
use of images that are either fully labeled, marked with bounding boxes, just
global labels, or not at all, we are able to cut the requirement for expensive
labels by 94.22% - narrowing the gap to the best fully supervised baseline to
only 5% mean IoU. Our approach is validated by extensive experiments on retinal
fluid segmentation and we provide an in-depth analysis of the anticipated
effect each annotation type can have in boosting segmentation performance.","['cs.CV', 'I.4.6; I.2.6; I.5.4; I.5.1']"
Rethinking BiSeNet For Real-time Semantic Segmentation,"BiSeNet has been proved to be a popular two-stream network for real-time
segmentation. However, its principle of adding an extra path to encode spatial
information is time-consuming, and the backbones borrowed from pretrained
tasks, e.g., image classification, may be inefficient for image segmentation
due to the deficiency of task-specific design. To handle these problems, we
propose a novel and efficient structure named Short-Term Dense Concatenate
network (STDC network) by removing structure redundancy. Specifically, we
gradually reduce the dimension of feature maps and use the aggregation of them
for image representation, which forms the basic module of STDC network. In the
decoder, we propose a Detail Aggregation module by integrating the learning of
spatial information into low-level layers in single-stream manner. Finally, the
low-level features and deep features are fused to predict the final
segmentation results. Extensive experiments on Cityscapes and CamVid dataset
demonstrate the effectiveness of our method by achieving promising trade-off
between segmentation accuracy and inference speed. On Cityscapes, we achieve
71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti,
which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0
FPS while inferring on higher resolution images.",['cs.CV']
Benefits of Linear Conditioning with Metadata for Image Segmentation,"Medical images are often accompanied by metadata describing the image
(vendor, acquisition parameters) and the patient (disease type or severity,
demographics, genomics). This metadata is usually disregarded by image
segmentation methods. In this work, we adapt a linear conditioning method
called FiLM (Feature-wise Linear Modulation) for image segmentation tasks. This
FiLM adaptation enables integrating metadata into segmentation models for
better performance. We observed an average Dice score increase of 5.1% on
spinal cord tumor segmentation when incorporating the tumor type with FiLM. The
metadata modulates the segmentation process through low-cost affine
transformations applied on feature maps which can be included in any neural
network's architecture. Additionally, we assess the relevance of segmentation
FiLM layers for tackling common challenges in medical imaging: multi-class
training with missing segmentations, model adaptation to multiple tasks, and
training with a limited or unbalanced number of annotated data. Our results
demonstrated the following benefits of FiLM for segmentation: FiLMed U-Net was
robust to missing labels and reached higher Dice scores with few labels (up to
16.7%) compared to single-task U-Net. The code is open-source and available at
www.ivadomed.org.","['cs.CV', 'eess.IV']"
MIDeepSeg: Minimally Interactive Segmentation of Unseen Objects from Medical Images Using Deep Learning,"Segmentation of organs or lesions from medical images plays an essential role
in many clinical applications such as diagnosis and treatment planning. Though
Convolutional Neural Networks (CNN) have achieved the state-of-the-art
performance for automatic segmentation, they are often limited by the lack of
clinically acceptable accuracy and robustness in complex cases. Therefore,
interactive segmentation is a practical alternative to these methods. However,
traditional interactive segmentation methods require a large amount of user
interactions, and recently proposed CNN-based interactive segmentation methods
are limited by poor performance on previously unseen objects. To solve these
problems, we propose a novel deep learning-based interactive segmentation
method that not only has high efficiency due to only requiring clicks as user
inputs but also generalizes well to a range of previously unseen objects.
Specifically, we first encode user-provided interior margin points via our
proposed exponentialized geodesic distance that enables a CNN to achieve a good
initial segmentation result of both previously seen and unseen objects, then we
use a novel information fusion method that combines the initial segmentation
with only few additional user clicks to efficiently obtain a refined
segmentation. We validated our proposed framework through extensive experiments
on 2D and 3D medical image segmentation tasks with a wide range of previous
unseen objects that were not present in the training set. Experimental results
showed that our proposed framework 1) achieves accurate results with fewer user
interactions and less time compared with state-of-the-art interactive
frameworks and 2) generalizes well to previously unseen objects.",['cs.CV']
Quantization of Deep Neural Networks for Accurate EdgeComputing,"Deep neural networks (DNNs) have demonstrated their great potential in recent
years, exceeding the per-formance of human experts in a wide range of
applications. Due to their large sizes, however, compressiontechniques such as
weight quantization and pruning are usually applied before they can be
accommodated onthe edge. It is generally believed that quantization leads to
performance degradation, and plenty of existingworks have explored quantization
strategies aiming at minimum accuracy loss. In this paper, we argue
thatquantization, which essentially imposes regularization on weight
representations, can sometimes help toimprove accuracy. We conduct
comprehensive experiments on three widely used applications: fully con-nected
network (FCN) for biomedical image segmentation, convolutional neural network
(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)
for automatic speech recognition, and experi-mental results show that
quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three
applicationsrespectively with 3.5x-6.4x memory reduction.",['cs.CV']
GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement,"Advances in object-centric generative models (OCGMs) have culminated in the
development of a broad range of methods for unsupervised object segmentation
and interpretable object-centric scene generation. These methods, however, are
limited to simulated and real-world datasets with limited visual complexity.
Moreover, object representations are often inferred using RNNs which do not
scale well to large images or iterative refinement which avoids imposing an
unnatural ordering on objects in an image but requires the a priori
initialisation of a fixed number of object representations. In contrast to
established paradigms, this work proposes an embedding-based approach in which
embeddings of pixels are clustered in a differentiable fashion using a
stochastic, non-parametric stick-breaking process. Similar to iterative
refinement, this clustering procedure also leads to randomly ordered object
representations, but without the need of initialising a fixed number of
clusters a priori. This is used to develop a new model, GENESIS-V2, which can
infer a variable number of object representations without using RNNs or
iterative refinement. We show that GENESIS-V2 outperforms previous methods for
unsupervised image segmentation and object-centric scene generation on
established synthetic datasets as well as more complex real-world datasets.","['cs.CV', 'cs.LG', 'stat.ML']"
ObjectAug: Object-level Data Augmentation for Semantic Image Segmentation,"Semantic image segmentation aims to obtain object labels with precise
boundaries, which usually suffers from overfitting. Recently, various data
augmentation strategies like regional dropout and mix strategies have been
proposed to address the problem. These strategies have proved to be effective
for guiding the model to attend on less discriminative parts. However, current
strategies operate at the image level, and objects and the background are
coupled. Thus, the boundaries are not well augmented due to the fixed semantic
scenario. In this paper, we propose ObjectAug to perform object-level
augmentation for semantic image segmentation. ObjectAug first decouples the
image into individual objects and the background using the semantic labels.
Next, each object is augmented individually with commonly used augmentation
methods (e.g., scaling, shifting, and rotation). Then, the black area brought
by object augmentation is further restored using image inpainting. Finally, the
augmented objects and background are assembled as an augmented image. In this
way, the boundaries can be fully explored in the various semantic scenarios. In
addition, ObjectAug can support category-aware augmentation that gives various
possibilities to objects in each category, and can be easily combined with
existing image-level augmentation methods to further boost performance.
Comprehensive experiments are conducted on both natural image and medical image
datasets. Experiment results demonstrate that our ObjectAug can evidently
improve segmentation performance.",['cs.CV']
Flow-based Video Segmentation for Human Head and Shoulders,"Video segmentation for the human head and shoulders is essential in creating
elegant media for videoconferencing and virtual reality applications. The main
challenge is to process high-quality background subtraction in a real-time
manner and address the segmentation issues under motion blurs, e.g., shaking
the head or waving hands during conference video. To overcome the motion blur
problem in video segmentation, we propose a novel flow-based encoder-decoder
network (FUNet) that combines both traditional Horn-Schunck optical-flow
estimation technique and convolutional neural networks to perform robust
real-time video segmentation. We also introduce a video and image segmentation
dataset: ConferenceVideoSegmentationDataset. Code and pre-trained models are
available on our GitHub repository:
\url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.",['cs.CV']
DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort,"We introduce DatasetGAN: an automatic procedure to generate massive datasets
of high-quality semantically segmented images requiring minimal human effort.
Current deep networks are extremely data-hungry, benefiting from training on
large-scale datasets, which are time consuming to annotate. Our method relies
on the power of recent GANs to generate realistic images. We show how the GAN
latent code can be decoded to produce a semantic segmentation of the image.
Training the decoder only needs a few labeled examples to generalize to the
rest of the latent space, resulting in an infinite annotated dataset generator!
These generated datasets can then be used for training any computer vision
architecture just as real datasets are. As only a few images need to be
manually segmented, it becomes possible to annotate images in extreme detail
and generate datasets with rich object and part segmentations. To showcase the
power of our approach, we generated datasets for 7 image segmentation tasks
which include pixel-level labels for 34 human face parts, and 32 car parts. Our
approach outperforms all semi-supervised baselines significantly and is on par
with fully supervised methods, which in some cases require as much as 100x more
annotated data as our method.",['cs.CV']
Interpretability-Driven Sample Selection Using Self Supervised Learning For Disease Classification And Segmentation,"In supervised learning for medical image analysis, sample selection
methodologies are fundamental to attain optimum system performance promptly and
with minimal expert interactions (e.g. label querying in an active learning
setup). In this paper we propose a novel sample selection methodology based on
deep features leveraging information contained in interpretability saliency
maps. In the absence of ground truth labels for informative samples, we use a
novel self supervised learning based approach for training a classifier that
learns to identify the most informative sample in a given batch of images. We
demonstrate the benefits of the proposed approach, termed
Interpretability-Driven Sample Selection (IDEAL), in an active learning setup
aimed at lung disease classification and histopathology image segmentation. We
analyze three different approaches to determine sample informativeness from
interpretability saliency maps: (i) an observational model stemming from
findings on previous uncertainty-based sample selection approaches, (ii) a
radiomics-based model, and (iii) a novel data-driven self-supervised approach.
We compare IDEAL to other baselines using the publicly available NIH chest
X-ray dataset for lung disease classification, and a public histopathology
segmentation dataset (GLaS), demonstrating the potential of using
interpretability information for sample selection in active learning systems.
Results show our proposed self supervised approach outperforms other approaches
in selecting informative samples leading to state of the art performance with
fewer samples.",['cs.CV']
Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization,"Training deep networks with limited labeled data while achieving a strong
generalization ability is key in the quest to reduce human annotation efforts.
This is the goal of semi-supervised learning, which exploits more widely
available unlabeled data to complement small labeled data sets. In this paper,
we propose a novel framework for discriminative pixel-level tasks using a
generative model of both images and labels. Concretely, we learn a generative
adversarial network that captures the joint image-label distribution and is
trained efficiently using a large set of unlabeled images supplemented with
only few labeled ones. We build our architecture on top of StyleGAN2, augmented
with a label synthesis branch. Image labeling at test time is achieved by first
embedding the target image into the joint latent space via an encoder network
and test-time optimization, and then generating the label from the inferred
embedding. We evaluate our approach in two important domains: medical image
segmentation and part-based face segmentation. We demonstrate strong in-domain
performance compared to several baselines, and are the first to showcase
extreme out-of-domain generalization, such as transferring from CT to MRI in
medical imaging, and photographs of real faces to paintings, sculptures, and
even cartoons and animal faces. Project Page:
\url{https://nv-tlabs.github.io/semanticGAN/}","['cs.CV', 'cs.AI', 'cs.LG']"
Spatially Varying Label Smoothing: Capturing Uncertainty from Expert Annotations,"The task of image segmentation is inherently noisy due to ambiguities
regarding the exact location of boundaries between anatomical structures. We
argue that this information can be extracted from the expert annotations at no
extra cost, and when integrated into state-of-the-art neural networks, it can
lead to improved calibration between soft probabilistic predictions and the
underlying uncertainty. We built upon label smoothing (LS) where a network is
trained on 'blurred' versions of the ground truth labels which has been shown
to be effective for calibrating output predictions. However, LS is not taking
the local structure into account and results in overly smoothed predictions
with low confidence even for non-ambiguous regions. Here, we propose Spatially
Varying Label Smoothing (SVLS), a soft labeling technique that captures the
structural uncertainty in semantic segmentation. SVLS also naturally lends
itself to incorporate inter-rater uncertainty when multiple labelmaps are
available. The proposed approach is extensively validated on four clinical
segmentation tasks with different imaging modalities, number of classes and
single and multi-rater expert annotations. The results demonstrate that SVLS,
despite its simplicity, obtains superior boundary prediction with improved
uncertainty and model calibration.",['cs.CV']
An MRF-UNet Product of Experts for Image Segmentation,"While convolutional neural networks (CNNs) trained by back-propagation have
seen unprecedented success at semantic segmentation tasks, they are known to
struggle on out-of-distribution data. Markov random fields (MRFs) on the other
hand, encode simpler distributions over labels that, although less flexible
than UNets, are less prone to over-fitting. In this paper, we propose to fuse
both strategies by computing the product of distributions of a UNet and an MRF.
As this product is intractable, we solve for an approximate distribution using
an iterative mean-field approach. The resulting MRF-UNet is trained jointly by
back-propagation. Compared to other works using conditional random fields
(CRFs), the MRF has no dependency on the imaging data, which should allow for
less over-fitting. We show on 3D neuroimaging data that this novel network
improves generalisation to out-of-distribution samples. Furthermore, it allows
the overall number of parameters to be reduced while preserving high accuracy.
These results suggest that a classic MRF smoothness prior can allow for less
over-fitting when principally integrated into a CNN model. Our implementation
is available at https://github.com/balbasty/nitorch.","['cs.CV', 'eess.IV']"
Constrained domain adaptation for Image segmentation,"We propose to adapt segmentation networks with a constrained formulation,
which embeds domain-invariant prior knowledge about the segmentation regions.
Such knowledge may take the form of simple anatomical information, e.g.,
structure size or shape, estimated from source samples or known a priori. Our
method imposes domain-invariant inequality constraints on the network outputs
of unlabeled target samples. It implicitly matches prediction statistics
between target and source domains with permitted uncertainty of prior
knowledge. We address our constrained problem with a differentiable penalty,
fully suited for standard stochastic gradient descent approaches, removing the
need for computationally expensive Lagrangian optimization with dual
projections. Unlike current two-step adversarial training, our formulation is
based on a single loss in a single network, which simplifies adaptation by
avoiding extra adversarial steps, while improving convergence and quality of
training.
  The comparison of our approach with state-of-the-art adversarial methods
reveals substantially better performance on the challenging task of adapting
spine segmentation across different MRI modalities. Our results also show a
robustness to imprecision of size priors, approaching the accuracy of a fully
supervised model trained directly in a target domain.Our method can be readily
used for various constraints and segmentation problems.",['cs.CV']
Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation,"Colorectal cancer (CRC) is the first cause of death in many countries. CRC
originates from a small clump of cells on the lining of the colon called
polyps, which over time might grow and become malignant. Early detection and
removal of polyps are therefore necessary for the prevention of colon cancer.
In this paper, we introduce an ensemble of medical polyp segmentation
algorithms. Based on an observation that different segmentation algorithms will
perform well on different subsets of examples because of the nature and size of
training sets they have been exposed to and because of method-intrinsic
factors, we propose to measure the confidence in the prediction of each
algorithm and then use an associate threshold to determine whether the
confidence is acceptable or not. An algorithm is selected for the ensemble if
the confidence is below its associate threshold. The optimal threshold for each
segmentation algorithm is found by using Comprehensive Learning Particle Swarm
Optimization (CLPSO), a swarm intelligence algorithm. The Dice coefficient, a
popular performance metric for image segmentation, is used as the fitness
criteria. Experimental results on two polyp segmentation datasets MICCAI2015
and Kvasir-SEG confirm that our ensemble achieves better results compared to
some well-known segmentation algorithms.",['cs.CV']
Two layer Ensemble of Deep Learning Models for Medical Image Segmentation,"In recent years, deep learning has rapidly become a method of choice for the
segmentation of medical images. Deep Neural Network (DNN) architectures such as
UNet have achieved state-of-the-art results on many medical datasets. To
further improve the performance in the segmentation task, we develop an
ensemble system which combines various deep learning architectures. We propose
a two-layer ensemble of deep learning models for the segmentation of medical
images. The prediction for each training image pixel made by each model in the
first layer is used as the augmented data of the training image for the second
layer of the ensemble. The prediction of the second layer is then combined by
using a weights-based scheme in which each model contributes differently to the
combined result. The weights are found by solving linear regression problems.
Experiments conducted on two popular medical datasets namely CAMUS and
Kvasir-SEG show that the proposed method achieves better results concerning two
performance metrics (Dice Coefficient and Hausdorff distance) compared to some
well-known benchmark algorithms.",['cs.CV']
Deep ensembles based on Stochastic Activation Selection for Polyp Segmentation,"Semantic segmentation has a wide array of applications ranging from
medical-image analysis, scene understanding, autonomous driving and robotic
navigation. This work deals with medical image segmentation and in particular
with accurate polyp detection and segmentation during colonoscopy examinations.
Several convolutional neural network architectures have been proposed to
effectively deal with this task and with the problem of segmenting objects at
different scale input. The basic architecture in image segmentation consists of
an encoder and a decoder: the first uses convolutional filters to extract
features from the image, the second is responsible for generating the final
output. In this work, we compare some variant of the DeepLab architecture
obtained by varying the decoder backbone. We compare several decoder
architectures, including ResNet, Xception, EfficentNet, MobileNet and we
perturb their layers by substituting ReLU activation layers with other
functions. The resulting methods are used to create deep ensembles which are
shown to be very effective. Our experimental evaluations show that our best
ensemble produces good segmentation results by achieving high evaluation scores
with a dice coefficient of 0.884, and a mean Intersection over Union (mIoU) of
0.818 for the Kvasir-SEG dataset. To improve reproducibility and research
efficiency the MATLAB source code used for this research is available at
GitHub: https://github.com/LorisNanni.","['cs.CV', 'cs.AI']"
Source-Relaxed Domain Adaptation for Image Segmentation,"Domain adaptation (DA) has drawn high interests for its capacity to adapt a
model trained on labeled source data to perform well on unlabeled or weakly
labeled target data from a different domain. Most common DA techniques require
the concurrent access to the input images of both the source and target
domains. However, in practice, it is common that the source images are not
available in the adaptation phase. This is a very frequent DA scenario in
medical imaging, for instance, when the source and target images come from
different clinical sites. We propose a novel formulation for adapting
segmentation networks, which relaxes such a constraint. Our formulation is
based on minimizing a label-free entropy loss defined over target-domain data,
which we further guide with a domain invariant prior on the segmentation
regions. Many priors can be used, derived from anatomical information. Here, a
class-ratio prior is learned via an auxiliary network and integrated in the
form of a Kullback-Leibler (KL) divergence in our overall loss function. We
show the effectiveness of our prior-aware entropy minimization in adapting
spine segmentation across different MRI modalities. Our method yields
comparable results to several state-of-the-art adaptation techniques, even
though is has access to less information, the source images being absent in the
adaptation phase. Our straight-forward adaptation strategy only uses one
network, contrary to popular adversarial techniques, which cannot perform
without the presence of the source images. Our framework can be readily used
with various priors and segmentation problems.",['cs.CV']
Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax,"Computer vision has shown promising results in medical image processing.
Pneumothorax is a deadly condition and if not diagnosed and treated at time
then it causes death. It can be diagnosed with chest X-ray images. We need an
expert and experienced radiologist to predict whether a person is suffering
from pneumothorax or not by looking at the chest X-ray images. Everyone does
not have access to such a facility. Moreover, in some cases, we need quick
diagnoses. So we propose an image segmentation model to predict and give the
output a mask that will assist the doctor in taking this crucial decision. Deep
Learning has proved their worth in many areas and outperformed man
state-of-the-art models. We want to use the power of these deep learning model
to solve this problem. We have used U-net [13] architecture with ResNet [17] as
a backbone and achieved promising results. U-net [13] performs very well in
medical image processing and semantic segmentation. Our problem falls in the
semantic segmentation category.",['cs.CV']
Deep Tiered Image Segmentation For Detecting Internal Ice Layers in Radar Imagery,"Understanding the structure of Earth's polar ice sheets is important for
modeling how global warming will impact polar ice and, in turn, the Earth's
climate. Ground-penetrating radar is able to collect observations of the
internal structure of snow and ice, but the process of manually labeling these
observations is slow and laborious. Recent work has developed automatic
techniques for finding the boundaries between the ice and the bedrock, but
finding internal layers - the subtle boundaries that indicate where one year's
ice accumulation ended and the next began - is much more challenging because
the number of layers varies and the boundaries often merge and split. In this
paper, we propose a novel deep neural network for solving a general class of
tiered segmentation problems. We then apply it to detecting internal layers in
polar ice, evaluating on a large-scale dataset of polar ice radar data with
human-labeled annotations as ground truth.",['cs.CV']
Hierarchical Image Peeling: A Flexible Scale-space Filtering Framework,"The importance of hierarchical image organization has been witnessed by a
wide spectrum of applications in computer vision and graphics. Different from
image segmentation with the spatial whole-part consideration, this work designs
a modern framework for disassembling an image into a family of derived signals
from a scale-space perspective. Specifically, we first offer a formal
definition of image disassembly. Then, by concerning desired properties, such
as peeling hierarchy and structure preservation, we convert the original
complex problem into a series of two-component separation sub-problems,
significantly reducing the complexity. The proposed framework is flexible to
both supervised and unsupervised settings. A compact recurrent network, namely
hierarchical image peeling net, is customized to efficiently and effectively
fulfill the task, which is about 3.5Mb in size, and can handle 1080p images in
more than 60 fps per recurrence on a GTX 2080Ti GPU, making it attractive for
practical use. Both theoretical findings and experimental results are provided
to demonstrate the efficacy of the proposed framework, reveal its superiority
over other state-of-the-art alternatives, and show its potential to various
applicable scenarios. Our code is available at
\url{https://github.com/ForawardStar/HIPe}.",['cs.CV']
Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation,"We present ObSuRF, a method which turns a single image of a scene into a 3D
model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF
corresponding to a different object. A single forward pass of an encoder
network outputs a set of latent vectors describing the objects in the scene.
These vectors are used independently to condition a NeRF decoder, defining the
geometry and appearance of each object. We make learning more computationally
efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs
without explicit ray marching. After confirming that the model performs equal
or better than state of the art on three 2D image segmentation benchmarks, we
apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a
novel dataset in which scenes are populated by ShapeNet models. We find that
after training ObSuRF on RGB-D views of training scenes, it is capable of not
only recovering the 3D geometry of a scene depicted in a single input image,
but also to segment it into objects, despite receiving no supervision in that
regard.","['cs.CV', 'cs.LG', 'stat.ML']"
Analysis of Information Flow Through U-Nets,"Deep Neural Networks (DNNs) have become ubiquitous in medical image
processing and analysis. Among them, U-Nets are very popular in various image
segmentation tasks. Yet, little is known about how information flows through
these networks and whether they are indeed properly designed for the tasks they
are being proposed for. In this paper, we employ information-theoretic tools in
order to gain insight into information flow through U-Nets. In particular, we
show how mutual information between input/output and an intermediate layer can
be a useful tool to understand information flow through various portions of a
U-Net, assess its architectural efficiency, and even propose more efficient
designs.","['cs.LG', 'cs.CV', 'eess.IV']"
MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking,"MeanShift is a popular mode-seeking clustering algorithm used in a wide range
of applications in machine learning. However, it is known to be prohibitively
slow, with quadratic runtime per iteration. We propose MeanShift++, an
extremely fast mode-seeking algorithm based on MeanShift that uses a grid-based
approach to speed up the mean shift step, replacing the computationally
expensive neighbors search with a density-weighted mean of adjacent grid cells.
In addition, we show that this grid-based technique for density estimation
comes with theoretical guarantees. The runtime is linear in the number of
points and exponential in dimension, which makes MeanShift++ ideal on
low-dimensional applications such as image segmentation and object tracking. We
provide extensive experimental analysis showing that MeanShift++ can be more
than 10,000x faster than MeanShift with competitive clustering results on
benchmark datasets and nearly identical image segmentations as MeanShift.
Finally, we show promising results for object tracking.","['cs.CV', 'cs.LG']"
FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation,"With the increase in available large clinical and experimental datasets,
there has been substantial amount of work being done on addressing the
challenges in the area of biomedical image analysis. Image segmentation, which
is crucial for any quantitative analysis, has especially attracted attention.
Recent hardware advancement has led to the success of deep learning approaches.
However, although deep learning models are being trained on large datasets,
existing methods do not use the information from different learning epochs
effectively. In this work, we leverage the information of each training epoch
to prune the prediction maps of the subsequent epochs. We propose a novel
architecture called feedback attention network (FANet) that unifies the
previous epoch mask with the feature map of the current training epoch. The
previous epoch mask is then used to provide a hard attention to the learnt
feature maps at different convolutional layers. The network also allows to
rectify the predictions in an iterative fashion during the test time. We show
that our proposed feedback attention model provides a substantial improvement
on most segmentation metrics tested on seven publicly available biomedical
imaging datasets demonstrating the effectiveness of the proposed FANet.","['cs.CV', 'eess.IV']"
Polygonal Building Segmentation by Frame Field Learning,"While state of the art image segmentation models typically output
segmentations in raster format, applications in geographic information systems
often require vector polygons. To help bridge the gap between deep network
output and the format used in downstream tasks, we add a frame field output to
a deep segmentation model for extracting buildings from remote sensing images.
We train a deep neural network that aligns a predicted frame field to ground
truth contours. This additional objective improves segmentation quality by
leveraging multi-task learning and provides structural information that later
facilitates polygonization; we also introduce a polygonization algorithm that
utilizes the frame field along with the raster segmentation. Our code is
available at https://github.com/Lydorn/Polygonization-by-Frame-Field-Learning.","['cs.CV', 'cs.LG', 'eess.IV']"
Topology-Preserving 3D Image Segmentation Based On Hyperelastic Regularization,"Image segmentation is to extract meaningful objects from a given image. For
degraded images due to occlusions, obscurities or noises, the accuracy of the
segmentation result can be severely affected. To alleviate this problem, prior
information about the target object is usually introduced. In [10], a
topology-preserving registration-based segmentation model was proposed, which
is restricted to segment 2D images only. In this paper, we propose a novel 3D
topology-preserving registration-based segmentation model with the hyperelastic
regularization, which can handle both 2D and 3D images. The existence of the
solution of the proposed model is established. We also propose a converging
iterative scheme to solve the proposed model. Numerical experiments have been
carried out on the synthetic and real images, which demonstrate the
effectiveness of our proposed model.",['cs.CV']
Task-agnostic Out-of-Distribution Detection Using Kernel Density Estimation,"In the recent years, researchers proposed a number of successful methods to
perform out-of-distribution (OOD) detection in deep neural networks (DNNs). So
far the scope of the highly accurate methods has been limited to image level
classification tasks. However, attempts for generally applicable methods beyond
classification did not attain similar performance. In this paper, we address
this limitation by proposing a simple yet effective task-agnostic OOD detection
method. We estimate the probability density functions (pdfs) of intermediate
features of a pre-trained DNN by performing kernel density estimation (KDE) on
the training dataset. As direct application of KDE to feature maps is hindered
by their high dimensionality, we use a set of lower-dimensional marginalized
KDE models instead of a single high-dimensional one. At test time, we evaluate
the pdfs on a test sample and produce a confidence score that indicates the
sample is OOD. The use of KDE eliminates the need for making simplifying
assumptions about the underlying feature pdfs and makes the proposed method
task-agnostic. We perform extensive experiments on classification tasks using
benchmark datasets for OOD detection. Additionally, we perform experiments on
medical image segmentation tasks using brain MRI datasets. The results
demonstrate that the proposed method consistently achieves high OOD detection
performance in both classification and segmentation tasks and improves
state-of-the-art in almost all cases. Code is available at
\url{https://github.com/eerdil/task_agnostic_ood}","['cs.CV', 'cs.LG']"
Boundary IoU: Improving Object-Centric Image Segmentation Evaluation,"We present Boundary IoU (Intersection-over-Union), a new segmentation
evaluation measure focused on boundary quality. We perform an extensive
analysis across different error types and object sizes and show that Boundary
IoU is significantly more sensitive than the standard Mask IoU measure to
boundary errors for large objects and does not over-penalize errors on smaller
objects. The new quality measure displays several desirable characteristics
like symmetry w.r.t. prediction/ground truth pairs and balanced responsiveness
across scales, which makes it more suitable for segmentation evaluation than
other boundary-focused measures like Trimap IoU and F-measure. Based on
Boundary IoU, we update the standard evaluation protocols for instance and
panoptic segmentation tasks by proposing the Boundary AP (Average Precision)
and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show
that the new evaluation metrics track boundary quality improvements that are
generally overlooked by current Mask IoU-based evaluation metrics. We hope that
the adoption of the new boundary-sensitive evaluation metrics will lead to
rapid progress in segmentation methods that improve boundary quality.",['cs.CV']
Locate then Segment: A Strong Pipeline for Referring Image Segmentation,"Referring image segmentation aims to segment the objects referred by a
natural language expression. Previous methods usually focus on designing an
implicit and recurrent feature interaction mechanism to fuse the
visual-linguistic features to directly generate the final segmentation mask
without explicitly modeling the localization information of the referent
instances. To tackle these problems, we view this task from another perspective
by decoupling it into a ""Locate-Then-Segment"" (LTS) scheme. Given a language
expression, people generally first perform attention to the corresponding
target image regions, then generate a fine segmentation mask about the object
based on its context. The LTS first extracts and fuses both visual and textual
features to get a cross-modal representation, then applies a cross-model
interaction on the visual-textual features to locate the referred object with
position prior, and finally generates the segmentation result with a
light-weight segmentation network. Our LTS is simple but surprisingly
effective. On three popular benchmark datasets, the LTS outperforms all the
previous state-of-the-art methods by a large margin (e.g., +3.2% on RefCOCO+
and +3.4% on RefCOCOg). In addition, our model is more interpretable with
explicitly locating the object, which is also proved by visualization
experiments. We believe this framework is promising to serve as a strong
baseline for referring image segmentation.",['cs.CV']
Is segmentation uncertainty useful?,"Probabilistic image segmentation encodes varying prediction confidence and
inherent ambiguity in the segmentation problem. While different probabilistic
segmentation models are designed to capture different aspects of segmentation
uncertainty and ambiguity, these modelling differences are rarely discussed in
the context of applications of uncertainty. We consider two common use cases of
segmentation uncertainty, namely assessment of segmentation quality and active
learning. We consider four established strategies for probabilistic
segmentation, discuss their modelling capabilities, and investigate their
performance in these two tasks. We find that for all models and both tasks,
returned uncertainty correlates positively with segmentation error, but does
not prove to be useful for active learning.",['cs.CV']
Self-Guided and Cross-Guided Learning for Few-Shot Segmentation,"Few-shot segmentation has been attracting a lot of attention due to its
effectiveness to segment unseen object classes with a few annotated samples.
Most existing approaches use masked Global Average Pooling (GAP) to encode an
annotated support image to a feature vector to facilitate query image
segmentation. However, this pipeline unavoidably loses some discriminative
information due to the average operation. In this paper, we propose a simple
but effective self-guided learning approach, where the lost critical
information is mined. Specifically, through making an initial prediction for
the annotated support image, the covered and uncovered foreground regions are
encoded to the primary and auxiliary support vectors using masked GAP,
respectively. By aggregating both primary and auxiliary support vectors, better
segmentation performances are obtained on query images. Enlightened by our
self-guided module for 1-shot segmentation, we propose a cross-guided module
for multiple shot segmentation, where the final mask is fused using predictions
from multiple annotated samples with high-quality support vectors contributing
more and vice versa. This module improves the final prediction in the inference
stage without re-training. Extensive experiments show that our approach
achieves new state-of-the-art performances on both PASCAL-5i and COCO-20i
datasets.",['cs.CV']
DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation,"Recently, neural architecture search (NAS) has been applied to automatically
search high-performance networks for medical image segmentation. The NAS search
space usually contains a network topology level (controlling connections among
cells with different spatial scales) and a cell level (operations within each
cell). Existing methods either require long searching time for large-scale 3D
image datasets, or are limited to pre-defined topologies (such as U-shaped or
single-path). In this work, we focus on three important aspects of NAS in 3D
medical image segmentation: flexible multi-path network topology, high search
efficiency, and budgeted GPU memory usage. A novel differentiable search
framework is proposed to support fast gradient-based search within a highly
flexible network topology search space. The discretization of the searched
optimal continuous model in differentiable scheme may produce a sub-optimal
final discrete model (discretization gap). Therefore, we propose a topology
loss to alleviate this problem. In addition, the GPU memory usage for the
searched 3D model is limited with budget constraints during search. Our
Differentiable Network Topology Search scheme (DiNTS) is evaluated on the
Medical Segmentation Decathlon (MSD) challenge, which contains ten challenging
segmentation tasks. Our method achieves the state-of-the-art performance and
the top ranking on the MSD challenge leaderboard.",['cs.CV']
Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers,"Transformers are increasingly dominating multi-modal reasoning tasks, such as
visual question answering, achieving state-of-the-art results thanks to their
ability to contextualize information using the self-attention and co-attention
mechanisms. These attention modules also play a role in other computer vision
tasks including object detection and image segmentation. Unlike Transformers
that only use self-attention, Transformers with co-attention require to
consider multiple attention maps in parallel in order to highlight the
information that is relevant to the prediction in the model's input. In this
work, we propose the first method to explain prediction by any
Transformer-based architecture, including bi-modal Transformers and
Transformers with co-attentions. We provide generic solutions and apply these
to the three most commonly used of these architectures: (i) pure
self-attention, (ii) self-attention combined with co-attention, and (iii)
encoder-decoder attention. We show that our method is superior to all existing
methods which are adapted from single modality explainability.","['cs.CV', 'cs.LG']"
DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation,"Neural Architecture Search (NAS) has shown great potentials in automatically
designing scalable network architectures for dense image predictions. However,
existing NAS algorithms usually compromise on restricted search space and
search on proxy task to meet the achievable computational demands. To allow as
wide as possible network architectures and avoid the gap between target and
proxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which
directly searches the optimal network structures for the multi-scale
representations of visual information, over a large-scale target dataset.
Specifically, by connecting cells with each other using learnable weights, we
introduce a densely connected search space to cover an abundance of mainstream
network designs. Moreover, by combining both path-level and channel-level
sampling strategies, we design a fusion module to reduce the memory consumption
of ample search space. We demonstrate that the architecture obtained from our
DCNAS algorithm achieves state-of-the-art performances on public semantic image
segmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC
2012. We also retain leading performances when evaluating the architecture on
the more challenging ADE20K and Pascal Context dataset.","['cs.CV', 'cs.LG', 'eess.IV']"
Instance segmentation with the number of clusters incorporated in embedding learning,"Semantic and instance segmentation algorithms are two general yet distinct
image segmentation solutions powered by Convolution Neural Network. While
semantic segmentation benefits extensively from the end-to-end training
strategy, instance segmentation is frequently framed as a multi-stage task,
supported by learning-based discrimination and post-process clustering.
Independent optimizations on substages instigate the accumulation of
segmentation errors. In this work, we propose to embed prior clustering
information into an embedding learning framework FCRNet, stimulating the
one-stage instance segmentation. FCRNet relieves the complexity of post process
by incorporating the number of clustering groups into the embedding space. The
superior performance of FCRNet is verified and compared with other methods on
the nucleus dataset BBBC006.",['cs.CV']
Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates,"Large, fine-grained image segmentation datasets, annotated at pixel-level,
are difficult to obtain, particularly in medical imaging, where annotations
also require expert knowledge. Weakly-supervised learning can train models by
relying on weaker forms of annotation, such as scribbles. Here, we learn to
segment using scribble annotations in an adversarial game. With unpaired
segmentation masks, we train a multi-scale GAN to generate realistic
segmentation masks at multiple resolutions, while we use scribbles to learn
their correct position in the image. Central to the model's success is a novel
attention gating mechanism, which we condition with adversarial signals to act
as a shape prior, resulting in better object localization at multiple scales.
Subject to adversarial conditioning, the segmentor learns attention maps that
are semantic, suppress the noisy activations outside the objects, and reduce
the vanishing gradient problem in the deeper layers of the segmentor. We
evaluated our model on several medical (ACDC, LVSC, CHAOS) and non-medical
(PPSS) datasets, and we report performance levels matching those achieved by
models trained with fully annotated segmentation masks. We also demonstrate
extensions in a variety of settings: semi-supervised learning; combining
multiple scribble sources (a crowdsourcing scenario) and multi-task learning
(combining scribble and mask supervision). We release expert-made scribble
annotations for the ACDC dataset, and the code used for the experiments, at
https://vios-s.github.io/multiscale-adversarial-attention-gates",['cs.CV']
"Learning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond","Conventional deformable registration methods aim at solving an optimization
model carefully designed on image pairs and their computational costs are
exceptionally high. In contrast, recent deep learning based approaches can
provide fast deformation estimation. These heuristic network architectures are
fully data-driven and thus lack explicit geometric constraints, e.g.,
topology-preserving, which are indispensable to generate plausible
deformations. We design a new deep learning based framework to optimize a
diffeomorphic model via multi-scale propagation in order to integrate
advantages and avoid limitations of these two categories of approaches.
Specifically, we introduce a generic optimization model to formulate
diffeomorphic registration and develop a series of learnable architectures to
obtain propagative updating in the coarse-to-fine feature space. Moreover, we
propose a novel bilevel self-tuned training strategy, allowing efficient search
of task-specific hyper-parameters. This training strategy increases the
flexibility to various types of data while reduces computational and human
burdens. We conduct two groups of image registration experiments on 3D volume
datasets including image-to-atlas registration on brain MRI data and
image-to-image registration on liver CT data. Extensive results demonstrate the
state-of-the-art performance of the proposed method with diffeomorphic
guarantee and extreme efficiency. We also apply our framework to challenging
multi-modal image registration, and investigate how our registration to support
the down-streaming tasks for medical image analysis including multi-modal
fusion and image segmentation.",['cs.CV']
Test-Time Training for Deformable Multi-Scale Image Registration,"Registration is a fundamental task in medical robotics and is often a crucial
step for many downstream tasks such as motion analysis, intra-operative
tracking and image segmentation. Popular registration methods such as ANTs and
NiftyReg optimize objective functions for each pair of images from scratch,
which are time-consuming for 3D and sequential images with complex
deformations. Recently, deep learning-based registration approaches such as
VoxelMorph have been emerging and achieve competitive performance. In this
work, we construct a test-time training for deep deformable image registration
to improve the generalization ability of conventional learning-based
registration model. We design multi-scale deep networks to consecutively model
the residual deformations, which is effective for high variational
deformations. Extensive experiments validate the effectiveness of multi-scale
deep registration with test-time training based on Dice coefficient for image
segmentation and mean square error (MSE), normalized local cross-correlation
(NLCC) for tissue dense tracking tasks. Two videos are in
https://www.youtube.com/watch?v=NvLrCaqCiAE and
https://www.youtube.com/watch?v=pEA6ZmtTNuQ","['cs.CV', 'cs.LG', 'cs.NE', 'cs.RO', 'eess.IV']"
Learning Versatile Neural Architectures by Propagating Network Codes,"This work explores how to design a single neural network that is capable of
adapting to multiple heterogeneous tasks of computer vision, such as image
segmentation, 3D detection, and video recognition. This goal is challenging
because network architecture designs in different tasks are inconsistent. We
solve this challenge by proposing Network Coding Propagation (NCP), a novel
""neural predictor"", which is able to predict an architecture's performance in
multiple datasets and tasks. Unlike prior arts of neural architecture search
(NAS) that typically focus on a single task, NCP has several unique benefits.
(1) NCP can be trained on different NAS benchmarks, such as NAS-Bench-201 and
NAS-Bench-MR, which contains a novel network space designed by us for jointly
searching an architecture among multiple tasks, including ImageNet, Cityscapes,
KITTI, and HMDB51. (2) NCP learns from network codes but not original data,
enabling it to update the architecture efficiently across datasets. (3)
Extensive experiments evaluate NCP on object classification, detection,
segmentation, and video recognition. For example, with 17\% fewer FLOPs, a
single architecture returned by NCP achieves 86\% and 77.16\% on
ImageNet-50-1000 and Cityscapes respectively, outperforming its counterparts.
More interestingly, NCP enables a single architecture applicable to both image
segmentation and video recognition, which achieves competitive performance on
both HMDB51 and ADE20K compared to the singular counterparts. Code is available
at https://github.com/dingmyu/NCP}{https://github.com/dingmyu/NCP.",['cs.CV']
Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images,"Fluorescence microscopy images play the critical role of capturing spatial or
spatiotemporal information of biomedical processes in life sciences. Their
simple structures and semantics provide unique advantages in elucidating
learning behavior of deep neural networks (DNNs). It is generally assumed that
accurate image annotation is required to train DNNs for accurate image
segmentation. In this study, however, we find that DNNs trained by label images
in which nearly half (49%) of the binary pixel labels are randomly flipped
provide largely the same segmentation performance. This suggests that DNNs
learn high-level structures rather than pixel-level labels per se to segment
fluorescence microscopy images. We refer to these structures as
meta-structures. In support of the existence of the meta-structures, when DNNs
are trained by a series of label images with progressively less meta-structure
information, we find progressive degradation in their segmentation performance.
Motivated by the learning behavior of DNNs trained by random labels and the
characteristics of meta-structures, we propose an unsupervised segmentation
model. Experiments show that it achieves remarkably competitive performance in
comparison to supervised segmentation models.",['cs.CV']
Improving Image co-segmentation via Deep Metric Learning,"Deep Metric Learning (DML) is helpful in computer vision tasks. In this
paper, we firstly introduce DML into image co-segmentation. We propose a novel
Triplet loss for Image Segmentation, called IS-Triplet loss for short, and
combine it with traditional image segmentation loss. Different from the general
DML task which learns the metric between pictures, we treat each pixel as a
sample, and use their embedded features in high-dimensional space to form
triples, then we tend to force the distance between pixels of different
categories greater than of the same category by optimizing IS-Triplet loss so
that the pixels from different categories are easier to be distinguished in the
high-dimensional feature space. We further present an efficient triple sampling
strategy to make a feasible computation of IS-Triplet loss. Finally, the
IS-Triplet loss is combined with 3 traditional image segmentation losses to
perform image segmentation. We apply the proposed approach to image
co-segmentation and test it on the SBCoseg dataset and the Internet dataset.
The experimental result shows that our approach can effectively improve the
discrimination of pixels' categories in high-dimensional space and thus help
traditional loss achieve better performance of image segmentation with fewer
training epochs.","['cs.CV', 'cs.AI']"
Interior Object Detection and Color Harmonization,"Confused about renovating your space? Choosing the perfect color for your
walls is always a challenging task. One does rounds of color consultation and
several patch tests. This paper proposes an AI tool to pitch paint based on
attributes of your room and other furniture, and visualize it on your walls. It
makes the color selection process easy. It takes in images of a room, detects
furniture objects using YOLO object detection. Once these objects have been
detected, the tool picks out color of the object. Later this object specific
information gets appended to the room attributes (room_type, room_size,
preferred_tone, etc) and a deep neural net is trained to make predictions for
color/texture/wallpaper for the walls. Finally, these predictions are
visualized on the walls from the images provided. The idea is to take the
knowledge of a color consultant and pitch colors that suit the walls and
provide a good contrast with the furniture and harmonize with different colors
in the room. Transfer learning for YOLO object detection from the COCO dataset
was used as a starting point and the weights were later fine-tuned by training
on additional images. The model was trained on 1000 records listing the room
and furniture attributes, to predict colors. Given the room image, this method
finds the best color scheme for the walls. These predictions are then
visualized on the walls in the image using image segmentation. The results are
visually appealing and automatically enhance the color look-and-feel.","['cs.CV', 'cs.AI']"
A Location-Sensitive Local Prototype Network for Few-Shot Medical Image Segmentation,"Despite the tremendous success of deep neural networks in medical image
segmentation, they typically require a large amount of costly, expert-level
annotated data. Few-shot segmentation approaches address this issue by learning
to transfer knowledge from limited quantities of labeled examples.
Incorporating appropriate prior knowledge is critical in designing
high-performance few-shot segmentation algorithms. Since strong spatial priors
exist in many medical imaging modalities, we propose a prototype-based method
-- namely, the location-sensitive local prototype network -- that leverages
spatial priors to perform few-shot medical image segmentation. Our approach
divides the difficult problem of segmenting the entire image with global
prototypes into easily solvable subproblems of local region segmentation with
local prototypes. For organ segmentation experiments on the VISCERAL CT image
dataset, our method outperforms the state-of-the-art approaches by 10% in the
mean Dice coefficient. Extensive ablation studies demonstrate the substantial
benefits of incorporating spatial information and confirm the effectiveness of
our approach.",['cs.CV']
Topology-Aware Segmentation Using Discrete Morse Theory,"In the segmentation of fine-scale structures from natural and biomedical
images, per-pixel accuracy is not the only metric of concern. Topological
correctness, such as vessel connectivity and membrane closure, is crucial for
downstream analysis tasks. In this paper, we propose a new approach to train
deep image segmentation networks for better topological accuracy. In
particular, leveraging the power of discrete Morse theory (DMT), we identify
global structures, including 1D skeletons and 2D patches, which are important
for topological accuracy. Trained with a novel loss based on these global
structures, the network performance is significantly improved especially near
topologically challenging locations (such as weak spots of connections and
membranes). On diverse datasets, our method achieves superior performance on
both the DICE score and topological metrics.","['cs.CV', 'cs.CG']"
Contrastive Registration for Unsupervised Medical Image Segmentation,"Medical image segmentation is a relevant task as it serves as the first step
for several diagnosis processes, thus it is indispensable in clinical usage.
Whilst major success has been reported using supervised techniques, they assume
a large and well-representative labelled set. This is a strong assumption in
the medical domain where annotations are expensive, time-consuming, and
inherent to human bias. To address this problem, unsupervised techniques have
been proposed in the literature yet it is still an open problem due to the
difficulty of learning any transformation pattern. In this work, we present a
novel optimisation model framed into a new CNN-based contrastive registration
architecture for unsupervised medical image segmentation. The core of our
approach is to exploit image-level registration and feature-level from a
contrastive learning mechanism, to perform registration-based segmentation.
Firstly, we propose an architecture to capture the image-to-image
transformation pattern via registration for unsupervised medical image
segmentation. Secondly, we embed a contrastive learning mechanism into the
registration architecture to enhance the discriminating capacity of the network
in the feature-level. We show that our proposed technique mitigates the major
drawbacks of existing unsupervised techniques. We demonstrate, through
numerical and visual experiments, that our technique substantially outperforms
the current state-of-the-art unsupervised segmentation methods on two major
medical image datasets.",['cs.CV']
Semi-Supervised Learning for Eye Image Segmentation,"Recent advances in appearance-based models have shown improved eye tracking
performance in difficult scenarios like occlusion due to eyelashes, eyelids or
camera placement, and environmental reflections on the cornea and glasses. The
key reason for the improvement is the accurate and robust identification of eye
parts (pupil, iris, and sclera regions). The improved accuracy often comes at
the cost of labeling an enormous dataset, which is complex and time-consuming.
This work presents two semi-supervised learning frameworks to identify
eye-parts by taking advantage of unlabeled images where labeled datasets are
scarce. With these frameworks, leveraging the domain-specific augmentation and
novel spatially varying transformations for image segmentation, we show
improved performance on various test cases. For instance, for a model trained
on just 48 labeled images, these frameworks achieved an improvement of 0.38%
and 0.65% in segmentation performance over the baseline model, which is trained
only with the labeled dataset.",['cs.CV']
Margin Preserving Self-paced Contrastive Learning Towards Domain Adaptation for Medical Image Segmentation,"To bridge the gap between the source and target domains in unsupervised
domain adaptation (UDA), the most common strategy puts focus on matching the
marginal distributions in the feature space through adversarial learning.
However, such category-agnostic global alignment lacks of exploiting the
class-level joint distributions, causing the aligned distribution less
discriminative. To address this issue, we propose in this paper a novel margin
preserving self-paced contrastive Learning (MPSCL) model for cross-modal
medical image segmentation. Unlike the conventional construction of contrastive
pairs in contrastive learning, the domain-adaptive category prototypes are
utilized to constitute the positive and negative sample pairs. With the
guidance of progressively refined semantic prototypes, a novel margin
preserving contrastive loss is proposed to boost the discriminability of
embedded representation space. To enhance the supervision for contrastive
learning, more informative pseudo-labels are generated in target domain in a
self-paced way, thus benefiting the category-aware distribution alignment for
UDA. Furthermore, the domain-invariant representations are learned through
joint contrastive learning between the two domains. Extensive experiments on
cross-modal cardiac segmentation tasks demonstrate that MPSCL significantly
improves semantic segmentation performance, and outperforms a wide variety of
state-of-the-art methods by a large margin.",['cs.CV']
Adapt Everywhere: Unsupervised Adaptation of Point-Clouds and Entropy Minimisation for Multi-modal Cardiac Image Segmentation,"Deep learning models are sensitive to domain shift phenomena. A model trained
on images from one domain cannot generalise well when tested on images from a
different domain, despite capturing similar anatomical structures. It is mainly
because the data distribution between the two domains is different. Moreover,
creating annotation for every new modality is a tedious and time-consuming
task, which also suffers from high inter- and intra- observer variability.
Unsupervised domain adaptation (UDA) methods intend to reduce the gap between
source and target domains by leveraging source domain labelled data to generate
labels for the target domain. However, current state-of-the-art (SOTA) UDA
methods demonstrate degraded performance when there is insufficient data in
source and target domains. In this paper, we present a novel UDA method for
multi-modal cardiac image segmentation. The proposed method is based on
adversarial learning and adapts network features between source and target
domain in different spaces. The paper introduces an end-to-end framework that
integrates: a) entropy minimisation, b) output feature space alignment and c) a
novel point-cloud shape adaptation based on the latent features learned by the
segmentation model. We validated our method on two cardiac datasets by adapting
from the annotated source domain, bSSFP-MRI (balanced Steady-State Free
Procession-MRI), to the unannotated target domain, LGE-MRI (Late-gadolinium
enhance-MRI), for the multi-sequence dataset; and from MRI (source) to CT
(target) for the cross-modality dataset. The results highlighted that by
enforcing adversarial learning in different parts of the network, the proposed
method delivered promising performance, compared to other SOTA methods.",['cs.CV']
Image Segmentation Methods for Non-destructive testing Applications,"In this paper, we present new image segmentation methods based on hidden
Markov random fields (HMRFs) and cuckoo search (CS) variants. HMRFs model the
segmentation problem as a minimization of an energy function. CS algorithm is
one of the recent powerful optimization techniques. Therefore, five variants of
the CS algorithm are used to compute a solution. Through tests, we conduct a
study to choose the CS variant with parameters that give good results
(execution time and quality of segmentation). CS variants are evaluated and
compared with non-destructive testing (NDT) images using a misclassification
error (ME) criterion.","['cs.CV', 'cs.AI']"
Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation,"We aim to estimate food portion size, a property that is strongly related to
the presence of food object in 3D space, from single monocular images under
real life setting. Specifically, we are interested in end-to-end estimation of
food portion size, which has great potential in the field of personal health
management. Unlike image segmentation or object recognition where annotation
can be obtained through large scale crowd sourcing, it is much more challenging
to collect datasets for portion size estimation since human cannot accurately
estimate the size of an object in an arbitrary 2D image without expert
knowledge. To address such challenge, we introduce a real life food image
dataset collected from a nutrition study where the groundtruth food energy
(calorie) is provided by registered dietitians, and will be made available to
the research community. We propose a deep regression process for portion size
estimation by combining features estimated from both RGB and learned energy
distribution domains. Our estimates of food energy achieved state-of-the-art
with a MAPE of 11.47%, significantly outperforms non-expert human estimates by
27.56%.",['cs.CV']
Semi-supervised Learning for Aggregated Multilayer Graphs Using Diffuse Interface Methods and Fast Matrix Vector Products,"We generalize a graph-based multiclass semi-supervised classification
technique based on diffuse interface methods to multilayer graphs. Besides the
treatment of various applications with an inherent multilayer structure, we
present a very flexible approach that interprets high-dimensional data in a
low-dimensional multilayer graph representation. Highly efficient numerical
methods involving the spectral decomposition of the corresponding differential
graph operators as well as fast matrix-vector products based on the
nonequispaced fast Fourier transform (NFFT) enable the rapid treatment of large
and high-dimensional data sets. We perform various numerical tests putting a
special focus on image segmentation. In particular, we test the performance of
our method on data sets with up to 10 million nodes per layer as well as up to
104 dimensions resulting in graphs with up to 52 layers. While all presented
numerical experiments can be run on an average laptop computer, the linear
dependence per iteration step of the runtime on the network size in all stages
of our algorithm makes it scalable to even larger and higher-dimensional
problems.","['cs.LG', 'cs.NA', 'math.NA', 'stat.ML', '68R10, 05C50, 65F15, 65T50, 68T05, 62H30']"
PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation,"Aerial Image Segmentation is a particular semantic segmentation problem and
has several challenging characteristics that general semantic segmentation does
not have. There are two critical issues: The one is an extremely
foreground-background imbalanced distribution, and the other is multiple small
objects along with the complex background. Such problems make the recent dense
affinity context modeling perform poorly even compared with baselines due to
over-introduced background context. To handle these problems, we propose a
point-wise affinity propagation module based on the Feature Pyramid Network
(FPN) framework, named PointFlow. Rather than dense affinity learning, a sparse
affinity map is generated upon selected points between the adjacent features,
which reduces the noise introduced by the background while keeping efficiency.
In particular, we design a dual point matcher to select points from the salient
area and object boundaries, respectively. Experimental results on three
different aerial segmentation datasets suggest that the proposed method is more
effective and efficient than state-of-the-art general semantic segmentation
methods. Especially, our methods achieve the best speed and accuracy trade-off
on three aerial benchmarks. Further experiments on three general semantic
segmentation datasets prove the generality of our method. Code will be provided
in (https: //github.com/lxtGH/PFSegNets).",['cs.CV']
Deep Superpixel Cut for Unsupervised Image Segmentation,"Image segmentation, one of the most critical vision tasks, has been studied
for many years. Most of the early algorithms are unsupervised methods, which
use hand-crafted features to divide the image into many regions. Recently,
owing to the great success of deep learning technology, CNNs based methods show
superior performance in image segmentation. However, these methods rely on a
large number of human annotations, which are expensive to collect. In this
paper, we propose a deep unsupervised method for image segmentation, which
contains the following two stages. First, a Superpixelwise Autoencoder
(SuperAE) is designed to learn the deep embedding and reconstruct a smoothed
image, then the smoothed image is passed to generate superpixels. Second, we
present a novel clustering algorithm called Deep Superpixel Cut (DSC), which
measures the deep similarity between superpixels and formulates image
segmentation as a soft partitioning problem. Via backpropagation, DSC
adaptively partitions the superpixels into perceptual regions. Experimental
results on the BSDS500 dataset demonstrate the effectiveness of the proposed
method.",['cs.CV']
FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space,"Federated learning allows distributed medical institutions to collaboratively
learn a shared prediction model with privacy protection. While at clinical
deployment, the models trained in federated learning can still suffer from
performance drop when applied to completely unseen hospitals outside the
federation. In this paper, we point out and solve a novel problem setting of
federated domain generalization (FedDG), which aims to learn a federated model
from multiple distributed source domains such that it can directly generalize
to unseen target domains. We present a novel approach, named as Episodic
Learning in Continuous Frequency Space (ELCFS), for this problem by enabling
each client to exploit multi-source data distributions under the challenging
constraint of data decentralization. Our approach transmits the distribution
information across clients in a privacy-protecting way through an effective
continuous frequency space interpolation mechanism. With the transferred
multi-source distributions, we further carefully design a boundary-oriented
episodic learning paradigm to expose the local learning to domain distribution
shifts and particularly meet the challenges of model generalization in medical
image segmentation scenario. The effectiveness of our method is demonstrated
with superior performance over state-of-the-arts and in-depth ablation
experiments on two medical image segmentation tasks. The code is available at
""https://github.com/liuquande/FedDG-ELCFS"".",['cs.CV']
Principal component-based image segmentation: a new approach to outline in vitro cell colonies,"The in vitro clonogenic assay is a technique to study the ability of a cell
to form a colony in a culture dish. By optical imaging, dishes with stained
colonies can be scanned and assessed digitally. Identification, segmentation
and counting of stained colonies play a vital part in high-throughput screening
and quantitative assessment of biological assays. Image processing of such
pictured/scanned assays can be affected by image/scan acquisition artifacts
like background noise and spatially varying illumination, and contaminants in
the suspension medium. Although existing approaches tackle these issues, the
segmentation quality requires further improvement, particularly on noisy and
low contrast images. In this work, we present an objective and versatile
machine learning procedure to amend these issues by characterizing, extracting
and segmenting inquired colonies using principal component analysis, k-means
clustering and a modified watershed segmentation algorithm. The intention is to
automatically identify visible colonies through spatial texture assessment and
accordingly discriminate them from background in preparation for successive
segmentation. The proposed segmentation algorithm yielded a similar quality as
manual counting by human observers. High F1 scores (>0.9) and low
root-mean-square errors (around 14%) underlined good agreement with ground
truth data. Moreover, it outperformed a recent state-of-the-art method. The
methodology will be an important tool in future cancer research applications.",['cs.CV']
Capturing Omni-Range Context for Omnidirectional Segmentation,"Convolutional Networks (ConvNets) excel at semantic segmentation and have
become a vital component for perception in autonomous driving. Enabling an
all-encompassing view of street-scenes, omnidirectional cameras present
themselves as a perfect fit in such systems. Most segmentation models for
parsing urban environments operate on common, narrow Field of View (FoV)
images. Transferring these models from the domain they were designed for to
360-degree perception, their performance drops dramatically, e.g., by an
absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of
FoV and structural distribution between the imaging domains, we introduce
Efficient Concurrent Attention Networks (ECANets), directly capturing the
inherent long-range dependencies in omnidirectional imagery. In addition to the
learned attention-based contextual priors that can stretch across 360-degree
images, we upgrade model training by leveraging multi-source and
omni-supervised learning, taking advantage of both: Densely labeled and
unlabeled data originating from multiple datasets. To foster progress in
panoramic image segmentation, we put forward and extensively evaluate models on
Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture
diverse scenes from all around the globe. Our novel model, training regimen and
multi-source prediction fusion elevate the performance (mIoU) to new
state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS
(69.0%) benchmarks.","['cs.CV', 'cs.RO', 'eess.IV']"
The Weakly-Labeled Rand Index,"Synthetic Aperture Sonar (SAS) surveys produce imagery with large regions of
transition between seabed types. Due to these regions, it is difficult to label
and segment the imagery and, furthermore, challenging to score the image
segmentations appropriately. While there are many approaches to quantify
performance in standard crisp segmentation schemes, drawing hard boundaries in
remote sensing imagery where gradients and regions of uncertainty exist is
inappropriate. These cases warrant weak labels and an associated appropriate
scoring approach. In this paper, a labeling approach and associated modified
version of the Rand index for weakly-labeled data is introduced to address
these issues. Results are evaluated with the new index and compared to
traditional segmentation evaluation methods. Experimental results on a SAS data
set containing must-link and cannot-link labels show that our Weakly-Labeled
Rand index scores segmentations appropriately in reference to qualitative
performance and is more suitable than traditional quantitative metrics for
scoring weakly-labeled data.","['cs.CV', 'cs.LG', 'eess.IV']"
Semi-supervised Medical Image Segmentation through Dual-task Consistency,"Deep learning-based semi-supervised learning (SSL) algorithms have led to
promising results in medical images segmentation and can alleviate doctors'
expensive annotations by leveraging unlabeled data. However, most of the
existing SSL algorithms in literature tend to regularize the model training by
perturbing networks and/or data. Observing that multi/dual-task learning
attends to various levels of information which have inherent prediction
perturbation, we ask the question in this work: can we explicitly build
task-level regularization rather than implicitly constructing networks- and/or
data-level perturbation-and-transformation for SSL? To answer this question, we
propose a novel dual-task-consistency semi-supervised framework for the first
time. Concretely, we use a dual-task deep network that jointly predicts a
pixel-wise segmentation map and a geometry-aware level set representation of
the target. The level set representation is converted to an approximated
segmentation map through a differentiable task transform layer. Simultaneously,
we introduce a dual-task consistency regularization between the level
set-derived segmentation maps and directly predicted segmentation maps for both
labeled and unlabeled data. Extensive experiments on two public datasets show
that our method can largely improve the performance by incorporating the
unlabeled data. Meanwhile, our framework outperforms the state-of-the-art
semi-supervised medical image segmentation methods. Code is available at:
https://github.com/Luoxd1996/DTC",['cs.CV']
Morphological Operation Residual Blocks: Enhancing 3D Morphological Feature Representation in Convolutional Neural Networks for Semantic Segmentation of Medical Images,"The shapes and morphology of the organs and tissues are important prior
knowledge in medical imaging recognition and segmentation. The morphological
operation is a well-known method for morphological feature extraction. As the
morphological operation is performed well in hand-crafted image segmentation
techniques, it is also promising to design an approach to approximate
morphological operation in the convolutional networks. However, using the
traditional convolutional neural network as a black-box is usually hard to
specify the morphological operation action. Here, we introduced a 3D
morphological operation residual block to extract morphological features in
end-to-end deep learning models for semantic segmentation. This study proposed
a novel network block architecture that embedded the morphological operation as
an infinitely strong prior in the convolutional neural network. Several 3D deep
learning models with the proposed morphological operation block were built and
compared in different medical imaging segmentation tasks. Experimental results
showed the proposed network achieved a relatively higher performance in the
segmentation tasks comparing with the conventional approach. In conclusion, the
novel network block could be easily embedded in traditional networks and
efficiently reinforce the deep learning models for medical imaging
segmentation.","['cs.CV', 'cs.LG', 'eess.IV']"
CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation,"Convolutional neural networks (CNNs) have been the de facto standard for
nowadays 3D medical image segmentation. The convolutional operations used in
these networks, however, inevitably have limitations in modeling the long-range
dependency due to their inductive bias of locality and weight sharing. Although
Transformer was born to address this issue, it suffers from extreme
computational and spatial complexities in processing high-resolution 3D feature
maps. In this paper, we propose a novel framework that efficiently bridges a
{\bf Co}nvolutional neural network and a {\bf Tr}ansformer {\bf (CoTr)} for
accurate 3D medical image segmentation. Under this framework, the CNN is
constructed to extract feature representations and an efficient deformable
Transformer (DeTrans) is built to model the long-range dependency on the
extracted feature maps. Different from the vanilla Transformer which treats all
image positions equally, our DeTrans pays attention only to a small set of key
positions by introducing the deformable self-attention mechanism. Thus, the
computational and spatial complexities of DeTrans have been greatly reduced,
making it possible to process the multi-scale and high-resolution feature maps,
which are usually of paramount importance for image segmentation. We conduct an
extensive evaluation on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV)
dataset that covers 11 major human organs. The results indicate that our CoTr
leads to a substantial performance improvement over other CNN-based,
transformer-based, and hybrid methods on the 3D multi-organ segmentation task.
Code is available at \def\UrlFont{\rm\small\ttfamily}
\url{https://github.com/YtongXie/CoTr}",['cs.CV']
K-means Segmentation Based-on Lab Color Space for Embryo Egg Detection,"The hatching process also influences the success of hatching eggs beside the
initial egg factor. So that the results have a large percentage of hatching, it
is necessary to check the development of the embryo at the beginning of the
hatching. This process aims to sort eggs that have embryos to remain hatched
until the end. Maximum checking is done the first week in the hatching period.
This study aims to detect the presence of embryos in eggs. Detection of the
existence of embryos is processed using segmentation. Egg images are segmented
using the K-means algorithm based on Lab color images. The results of the
images acquisition are converted into Lab color space images. The results of
Lab color space images are processed using K-means for each color. The K-means
process uses cluster k=3, where this cluster divided the image into three
parts, namely background, eggs, and yolk eggs. Yolk eggs are part of eggs that
have embryonic characteristics. This study applies the concept of color in the
initial segmentation and grayscale in the final stages. The results of the
initial phase show that the image segmentation results using k-means clustering
based on Lab color space provide a grouping of three parts. At the grayscale
image processing stage, the results of color image segmentation are processed
with grayscaling, image enhancement, and morphology. Thus, it seems clear that
the yolk segmented shows the presence of egg embryos. Based on this process and
results, K-means segmentation based on Lab color space can be used for the
initial stages of the embryo detection process. The evaluation uses MSE and
MSSIM, with values of 0.0486 and 0.9979; this can be used as a reference that
the results obtained can indicate the detection of embryos in egg yolk.","['cs.CV', 'eess.IV']"
Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation,"With the widespread success of deep learning in biomedical image
segmentation, domain shift becomes a critical and challenging problem, as the
gap between two domains can severely affect model performance when deployed to
unseen data with heterogeneous features. To alleviate this problem, we present
a novel unsupervised domain adaptation network, for generalizing models learned
from the labeled source domain to the unlabeled target domain for
cross-modality biomedical image segmentation. Specifically, our approach
consists of two key modules, a conditional domain discriminator~(CDD) and a
category-centric prototype aligner~(CCPA). The CDD, extended from conditional
domain adversarial networks in classifier tasks, is effective and robust in
handling complex cross-modality biomedical images. The CCPA, improved from the
graph-induced prototype alignment mechanism in cross-domain object detection,
can exploit precise instance-level features through an elaborate prototype
representation. In addition, it can address the negative effect of class
imbalance via entropy-based loss. Extensive experiments on a public benchmark
for the cardiac substructure segmentation task demonstrate that our method
significantly improves performance on the target domain.",['cs.CV']
Uncertainty guided semi-supervised segmentation of retinal layers in OCT images,"Deep convolutional neural networks have shown outstanding performance in
medical image segmentation tasks. The usual problem when training supervised
deep learning methods is the lack of labeled data which is time-consuming and
costly to obtain. In this paper, we propose a novel uncertainty-guided
semi-supervised learning based on a student-teacher approach for training the
segmentation network using limited labeled samples and a large number of
unlabeled images. First, a teacher segmentation model is trained from the
labeled samples using Bayesian deep learning. The trained model is used to
generate soft segmentation labels and uncertainty maps for the unlabeled set.
The student model is then updated using the softly segmented samples and the
corresponding pixel-wise confidence of the segmentation quality estimated from
the uncertainty of the teacher model using a newly designed loss function.
Experimental results on a retinal layer segmentation task show that the
proposed method improves the segmentation performance in comparison to the
fully supervised approach and is on par with the expert annotator. The proposed
semi-supervised segmentation framework is a key contribution and applicable for
biomedical image segmentation across various imaging modalities where access to
annotated medical images is challenging",['cs.CV']
Generative Synthetic Augmentation using Label-to-Image Translation for Nuclei Image Segmentation,"In medical image diagnosis, pathology image analysis using semantic
segmentation becomes important for efficient screening as a field of digital
pathology. The spatial augmentation is ordinary used for semantic segmentation.
Tumor images under malignant are rare and to annotate the labels of nuclei
region takes much time-consuming. We require an effective use of dataset to
maximize the segmentation accuracy. It is expected that some augmentation to
transform generalized images influence the segmentation performance. We propose
a synthetic augmentation using label-to-image translation, mapping from a
semantic label with the edge structure to a real image. Exactly this paper deal
with stain slides of nuclei in tumor. Actually, we demonstrate several
segmentation algorithms applied to the initial dataset that contains real
images and labels using synthetic augmentation in order to add their
generalized images. We computes and reports that a proposed synthetic
augmentation procedure improve their accuracy.","['cs.LG', 'eess.IV', 'stat.ML', 'I.4.6; I.2.6']"
A Novel CNN-LSTM-based Approach to Predict Urban Expansion,"Time-series remote sensing data offer a rich source of information that can
be used in a wide range of applications, from monitoring changes in land cover
to surveilling crops, coastal changes, flood risk assessment, and urban sprawl.
This paper addresses the challenge of using time-series satellite images to
predict urban expansion. Building upon previous work, we propose a novel
two-step approach based on semantic image segmentation in order to predict
urban expansion. The first step aims to extract information about urban regions
at different time scales and prepare them for use in the training step. The
second step combines Convolutional Neural Networks (CNN) with Long Short Term
Memory (LSTM) methods in order to learn temporal features and thus predict
urban expansion. In this paper, experimental results are conducted using
several multi-date satellite images representing the three largest cities in
Saudi Arabia, namely: Riyadh, Jeddah, and Dammam. We empirically evaluated our
proposed technique, and examined its results by comparing them with
state-of-the-art approaches. Following this evaluation, we determined that our
results reveal improved performance for the new-coupled CNN-LSTM approach,
particularly in terms of assessments based on Mean Square Error, Root Mean
Square Error, Peak Signal to Noise Ratio, Structural Similarity Index, and
overall classification accuracy.","['cs.CV', 'cs.LG']"
Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models,"Despite the remarkable performance of deep learning methods on various tasks,
most cutting-edge models rely heavily on large-scale annotated training
examples, which are often unavailable for clinical and health care tasks. The
labeling costs for medical images are very high, especially in medical image
segmentation, which typically requires intensive pixel/voxel-wise labeling.
Therefore, the strong capability of learning and generalizing from limited
supervision, including a limited amount of annotations, sparse annotations, and
inaccurate annotations, is crucial for the successful application of deep
learning models in medical image segmentation. However, due to its intrinsic
difficulty, segmentation with limited supervision is challenging and specific
model design and/or learning strategies are needed. In this paper, we provide a
systematic and up-to-date review of the solutions above, with summaries and
comments about the methodologies. We also highlight several problems in this
field, discussed future directions observing further investigations.","['cs.CV', 'cs.LG']"
Face Mask Extraction in Video Sequence,"Inspired by the recent development of deep network-based methods in semantic
image segmentation, we introduce an end-to-end trainable model for face mask
extraction in video sequence. Comparing to landmark-based sparse face shape
representation, our method can produce the segmentation masks of individual
facial components, which can better reflect their detailed shape variations. By
integrating Convolutional LSTM (ConvLSTM) algorithm with Fully Convolutional
Networks (FCN), our new ConvLSTM-FCN model works on a per-sequence basis and
takes advantage of the temporal correlation in video clips. In addition, we
also propose a novel loss function, called Segmentation Loss, to directly
optimise the Intersection over Union (IoU) performances. In practice, to
further increase segmentation accuracy, one primary model and two additional
models were trained to focus on the face, eyes, and mouth regions,
respectively. Our experiment shows the proposed method has achieved a 16.99%
relative improvement (from 54.50% to 63.76% mean IoU) over the baseline FCN
model on the 300 Videos in the Wild (300VW) dataset.",['cs.CV']
MixSearch: Searching for Domain Generalized Medical Image Segmentation Architectures,"Considering the scarcity of medical data, most datasets in medical image
analysis are an order of magnitude smaller than those of natural images.
However, most Network Architecture Search (NAS) approaches in medical images
focused on specific datasets and did not take into account the generalization
ability of the learned architectures on unseen datasets as well as different
domains. In this paper, we address this point by proposing to search for
generalizable U-shape architectures on a composited dataset that mixes medical
images from multiple segmentation tasks and domains creatively, which is named
MixSearch. Specifically, we propose a novel approach to mix multiple
small-scale datasets from multiple domains and segmentation tasks to produce a
large-scale dataset. Then, a novel weaved encoder-decoder structure is designed
to search for a generalized segmentation network in both cell-level and
network-level. The network produced by the proposed MixSearch framework
achieves state-of-the-art results compared with advanced encoder-decoder
networks across various datasets.","['cs.CV', 'cs.AI']"
Semantically Meaningful Class Prototype Learning for One-Shot Image Semantic Segmentation,"One-shot semantic image segmentation aims to segment the object regions for
the novel class with only one annotated image. Recent works adopt the episodic
training strategy to mimic the expected situation at testing time. However,
these existing approaches simulate the test conditions too strictly during the
training process, and thus cannot make full use of the given label information.
Besides, these approaches mainly focus on the foreground-background target
class segmentation setting. They only utilize binary mask labels for training.
In this paper, we propose to leverage the multi-class label information during
the episodic training. It will encourage the network to generate more
semantically meaningful features for each category. After integrating the
target class cues into the query features, we then propose a pyramid feature
fusion module to mine the fused features for the final classifier. Furthermore,
to take more advantage of the support image-mask pair, we propose a
self-prototype guidance branch to support image segmentation. It can constrain
the network for generating more compact features and a robust prototype for
each semantic class. For inference, we propose a fused prototype guidance
branch for the segmentation of the query image. Specifically, we leverage the
prediction of the query image to extract the pseudo-prototype and combine it
with the initial prototype. Then we utilize the fused prototype to guide the
final segmentation of the query image. Extensive experiments demonstrate the
superiority of our proposed approach.","['cs.CV', 'cs.MM']"
EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks,"Environmental Microorganism Data Set Fifth Version (EMDS-5) is a microscopic
image dataset including original Environmental Microorganism (EM) images and
two sets of Ground Truth (GT) images. The GT image sets include a single-object
GT image set and a multi-object GT image set. The EMDS-5 dataset has 21 types
of EMs, each of which contains 20 original EM images, 20 single-object GT
images and 20 multi-object GT images. EMDS-5 can realize to evaluate image
preprocessing, image segmentation, feature extraction, image classification and
image retrieval functions. In order to prove the effectiveness of EMDS-5, for
each function, we select the most representative algorithms and price
indicators for testing and evaluation. The image preprocessing functions
contain two parts: image denoising and image edge detection. Image denoising
uses nine kinds of filters to denoise 13 kinds of noises, respectively. In the
aspect of edge detection, six edge detection operators are used to detect the
edges of the images, and two evaluation indicators, peak-signal to noise ratio
and mean structural similarity, are used for evaluation. Image segmentation
includes single-object image segmentation and multi-object image segmentation.
Six methods are used for single-object image segmentation, while k-means and
U-net are used for multi-object segmentation.We extract nine features from the
images in EMDS-5 and use the Support Vector Machine classifier for testing. In
terms of image classification, we select the VGG16 feature to test different
classifiers. We test two types of retrieval approaches: texture feature
retrieval and deep learning feature retrieval. We select the last layer of
features of these two deep learning networks as feature vectors. We use mean
average precision as the evaluation index for retrieval.",['cs.CV']
Analyzing Overfitting under Class Imbalance in Neural Networks for Image Segmentation,"Class imbalance poses a challenge for developing unbiased, accurate
predictive models. In particular, in image segmentation neural networks may
overfit to the foreground samples from small structures, which are often
heavily under-represented in the training set, leading to poor generalization.
In this study, we provide new insights on the problem of overfitting under
class imbalance by inspecting the network behavior. We find empirically that
when training with limited data and strong class imbalance, at test time the
distribution of logit activations may shift across the decision boundary, while
samples of the well-represented class seem unaffected. This bias leads to a
systematic under-segmentation of small structures. This phenomenon is
consistently observed for different databases, tasks and network architectures.
To tackle this problem, we introduce new asymmetric variants of popular loss
functions and regularization techniques including a large margin loss, focal
loss, adversarial training, mixup and data augmentation, which are explicitly
designed to counter logit shift of the under-represented classes. Extensive
experiments are conducted on several challenging segmentation tasks. Our
results demonstrate that the proposed modifications to the objective function
can lead to significantly improved segmentation accuracy compared to baselines
and alternative approaches.",['cs.CV']
Using the Split Bregman Algorithm to Solve the Self-repelling Snake Model,"Preserving contour topology during image segmentation is useful in many
practical scenarios. By keeping the contours isomorphic, it is possible to
prevent over-segmentation and under-segmentation, as well as to adhere to given
topologies. The Self-repelling Snake model (SR) is a variational model that
preserves contour topology by combining a non-local repulsion term with the
geodesic active contour model (GAC). The SR is traditionally solved using the
additive operator splitting (AOS) scheme. In our paper, we propose an
alternative solution to the SR using the Split Bregman method. Our algorithm
breaks the problem down into simpler sub-problems to use lower-order evolution
equations and a simple projection scheme rather than re-initialization. The
sub-problems can be solved via fast Fourier transform (FFT) or an approximate
soft thresholding formula which maintains stability, shortening the convergence
time, and reduces the memory requirement. The Split Bregman and AOS algorithms
are compared theoretically and experimentally.",['cs.CV']
Image Compositing for Segmentation of Surgical Tools without Manual Annotations,"Producing manual, pixel-accurate, image segmentation labels is tedious and
time-consuming. This is often a rate-limiting factor when large amounts of
labeled images are required, such as for training deep convolutional networks
for instrument-background segmentation in surgical scenes. No large datasets
comparable to industry standards in the computer vision community are available
for this task. To circumvent this problem, we propose to automate the creation
of a realistic training dataset by exploiting techniques stemming from special
effects and harnessing them to target training performance rather than visual
appeal. Foreground data is captured by placing sample surgical instruments over
a chroma key (a.k.a. green screen) in a controlled environment, thereby making
extraction of the relevant image segment straightforward. Multiple lighting
conditions and viewpoints can be captured and introduced in the simulation by
moving the instruments and camera and modulating the light source. Background
data is captured by collecting videos that do not contain instruments. In the
absence of pre-existing instrument-free background videos, minimal labeling
effort is required, just to select frames that do not contain surgical
instruments from videos of surgical interventions freely available online. We
compare different methods to blend instruments over tissue and propose a novel
data augmentation approach that takes advantage of the plurality of options. We
show that by training a vanilla U-Net on semi-synthetic data only and applying
a simple post-processing, we are able to match the results of the same network
trained on a publicly available manually labeled real dataset.",['cs.CV']
Uncertainty-based method for improving poorly labeled segmentation datasets,"The success of modern deep learning algorithms for image segmentation heavily
depends on the availability of large datasets with clean pixel-level
annotations (masks), where the objects of interest are accurately delineated.
Lack of time and expertise during data annotation leads to incorrect boundaries
and label noise. It is known that deep convolutional neural networks (DCNNs)
can memorize even completely random labels, resulting in poor accuracy. We
propose a framework to train binary segmentation DCNNs using sets of unreliable
pixel-level annotations. Erroneously labeled pixels are identified based on the
estimated aleatoric uncertainty of the segmentation and are relabeled to the
true value.","['cs.CV', 'cs.AI']"
Saliency-Aware Class-Agnostic Food Image Segmentation,"Advances in image-based dietary assessment methods have allowed nutrition
professionals and researchers to improve the accuracy of dietary assessment,
where images of food consumed are captured using smartphones or wearable
devices. These images are then analyzed using computer vision methods to
estimate energy and nutrition content of the foods. Food image segmentation,
which determines the regions in an image where foods are located, plays an
important role in this process. Current methods are data dependent, thus cannot
generalize well for different food types. To address this problem, we propose a
class-agnostic food image segmentation method. Our method uses a pair of eating
scene images, one before start eating and one after eating is completed. Using
information from both the before and after eating images, we can segment food
images by finding the salient missing objects without any prior information
about the food class. We model a paradigm of top down saliency which guides the
attention of the human visual system (HVS) based on a task to find the salient
missing objects in a pair of images. Our method is validated on food images
collected from a dietary study which showed promising results.","['cs.CV', 'cs.LG']"
L-SNet: from Region Localization to Scale Invariant Medical Image Segmentation,"Coarse-to-fine models and cascade segmentation architectures are widely
adopted to solve the problem of large scale variations in medical image
segmentation. However, those methods have two primary limitations: the
first-stage segmentation becomes a performance bottleneck; the lack of overall
differentiability makes the training process of two stages asynchronous and
inconsistent. In this paper, we propose a differentiable two-stage network
architecture to tackle these problems. In the first stage, a localization
network (L-Net) locates Regions of Interest (RoIs) in a detection fashion; in
the second stage, a segmentation network (S-Net) performs fine segmentation on
the recalibrated RoIs; a RoI recalibration module between L-Net and S-Net
eliminating the inconsistencies. Experimental results on the public dataset
show that our method outperforms state-of-the-art coarse-to-fine models with
negligible computation overheads.",['cs.CV']
Self-paced and self-consistent co-training for semi-supervised image segmentation,"Deep co-training has recently been proposed as an effective approach for
image segmentation when annotated data is scarce. In this paper, we improve
existing approaches for semi-supervised segmentation with a self-paced and
self-consistent co-training method. To help distillate information from
unlabeled images, we first design a self-paced learning strategy for
co-training that lets jointly-trained neural networks focus on
easier-to-segment regions first, and then gradually consider harder ones.This
is achieved via an end-to-end differentiable loss inthe form of a generalized
Jensen Shannon Divergence(JSD). Moreover, to encourage predictions from
different networks to be both consistent and confident, we enhance this
generalized JSD loss with an uncertainty regularizer based on entropy. The
robustness of individual models is further improved using a self-ensembling
loss that enforces their prediction to be consistent across different training
iterations. We demonstrate the potential of our method on three challenging
image segmentation problems with different image modalities, using small
fraction of labeled data. Results show clear advantages in terms of performance
compared to the standard co-training baselines and recently proposed
state-of-the-art approaches for semi-supervised segmentation",['cs.CV']
TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation,"Medical image segmentation is an essential prerequisite for developing
healthcare systems, especially for disease diagnosis and treatment planning. On
various medical image segmentation tasks, the u-shaped architecture, also known
as U-Net, has become the de-facto standard and achieved tremendous success.
However, due to the intrinsic locality of convolution operations, U-Net
generally demonstrates limitations in explicitly modeling long-range
dependency. Transformers, designed for sequence-to-sequence prediction, have
emerged as alternative architectures with innate global self-attention
mechanisms, but can result in limited localization abilities due to
insufficient low-level details. In this paper, we propose TransUNet, which
merits both Transformers and U-Net, as a strong alternative for medical image
segmentation. On one hand, the Transformer encodes tokenized image patches from
a convolution neural network (CNN) feature map as the input sequence for
extracting global contexts. On the other hand, the decoder upsamples the
encoded features which are then combined with the high-resolution CNN feature
maps to enable precise localization.
  We argue that Transformers can serve as strong encoders for medical image
segmentation tasks, with the combination of U-Net to enhance finer details by
recovering localized spatial information. TransUNet achieves superior
performances to various competing methods on different medical applications
including multi-organ segmentation and cardiac segmentation. Code and models
are available at https://github.com/Beckschen/TransUNet.",['cs.CV']
MudrockNet: Semantic Segmentation of Mudrock SEM Images through Deep Learning,"Segmentation and analysis of individual pores and grains of mudrocks from
scanning electron microscope images is non-trivial because of noise, imaging
artifacts, variation in pixel grayscale values across images, and overlaps in
grayscale values among different physical features such as silt grains, clay
grains, and pores in an image, which make their identification difficult.
Moreover, because grains and pores often have overlapping grayscale values,
direct application of threshold-based segmentation techniques is not
sufficient. Recent advances in the field of computer vision have made it easier
and faster to segment images and identify multiple occurrences of such features
in an image, provided that ground-truth data for training the algorithm is
available. Here, we propose a deep learning SEM image segmentation model,
MudrockNet based on Google's DeepLab-v3+ architecture implemented with the
TensorFlow library. The ground-truth data was obtained from an image-processing
workflow applied to scanning electron microscope images of uncemented muds from
the Kumano Basin offshore Japan at depths < 1.1 km. The trained deep learning
model obtained a pixel-accuracy about 90%, and predictions for the test data
obtained a mean intersection over union (IoU) of 0.6591 for silt grains and
0.6642 for pores. We also compared our model with the random forest classifier
using trainable Weka segmentation in ImageJ, and it was observed that
MudrockNet gave better predictions for both silt grains and pores. The size,
concentration, and spatial arrangement of the silt and clay grains can affect
the petrophysical properties of a mudrock, and an automated method to
accurately identify the different grains and pores in mudrocks can help improve
reservoir and seal characterization for petroleum exploration and anthropogenic
waste sequestration.","['cs.CV', 'physics.geo-ph', 'I.4.6; I.4.3']"
Co-Seg: An Image Segmentation Framework Against Label Corruption,"Supervised deep learning performance is heavily tied to the availability of
high-quality labels for training. Neural networks can gradually overfit
corrupted labels if directly trained on noisy datasets, leading to severe
performance degradation at test time. In this paper, we propose a novel deep
learning framework, namely Co-Seg, to collaboratively train segmentation
networks on datasets which include low-quality noisy labels. Our approach first
trains two networks simultaneously to sift through all samples and obtain a
subset with reliable labels. Then, an efficient yet easily-implemented label
correction strategy is applied to enrich the reliable subset. Finally, using
the updated dataset, we retrain the segmentation network to finalize its
parameters. Experiments in two noisy labels scenarios demonstrate that our
proposed model can achieve results comparable to those obtained from supervised
learning trained on the noise-free labels. In addition, our framework can be
easily implemented in any segmentation algorithm to increase its robustness to
noisy labels.","['cs.LG', 'cs.CV']"
Resolution enhancement in the recovery of underdrawings via style transfer by generative adversarial deep neural networks,"We apply generative adversarial convolutional neural networks to the problem
of style transfer to underdrawings and ghost-images in x-rays of fine art
paintings with a special focus on enhancing their spatial resolution. We build
upon a neural architecture developed for the related problem of synthesizing
high-resolution photo-realistic image from semantic label maps. Our neural
architecture achieves high resolution through a hierarchy of generators and
discriminator sub-networks, working throughout a range of spatial resolutions.
This coarse-to-fine generator architecture can increase the effective
resolution by a factor of eight in each spatial direction, or an overall
increase in number of pixels by a factor of 64. We also show that even just a
few examples of human-generated image segmentations can greatly improve --
qualitatively and quantitatively -- the generated images. We demonstrate our
method on works such as Leonardo's Madonna of the carnation and the
underdrawing in his Virgin of the rocks, which pose several special problems in
style transfer, including the paucity of representative works from which to
learn and transfer style information.","['cs.CV', 'eess.IV']"
Utilizing Uncertainty Estimation in Deep Learning Segmentation of Fluorescence Microscopy Images with Missing Markers,"Fluorescence microscopy images contain several channels, each indicating a
marker staining the sample. Since many different marker combinations are
utilized in practice, it has been challenging to apply deep learning based
segmentation models, which expect a predefined channel combination for all
training samples as well as at inference for future application. Recent work
circumvents this problem using a modality attention approach to be effective
across any possible marker combination. However, for combinations that do not
exist in a labeled training dataset, one cannot have any estimation of
potential segmentation quality if that combination is encountered during
inference. Without this, not only one lacks quality assurance but one also does
not know where to put any additional imaging and labeling effort. We herein
propose a method to estimate segmentation quality on unlabeled images by (i)
estimating both aleatoric and epistemic uncertainties of convolutional neural
networks for image segmentation, and (ii) training a Random Forest model for
the interpretation of uncertainty features via regression to their
corresponding segmentation metrics. Additionally, we demonstrate that including
these uncertainty measures during training can provide an improvement on
segmentation performance.","['cs.CV', 'cs.LG', 'eess.IV']"
Mask-based Data Augmentation for Semi-supervised Semantic Segmentation,"Semantic segmentation using convolutional neural networks (CNN) is a crucial
component in image analysis. Training a CNN to perform semantic segmentation
requires a large amount of labeled data, where the production of such labeled
data is both costly and labor intensive. Semi-supervised learning algorithms
address this issue by utilizing unlabeled data and so reduce the amount of
labeled data needed for training. In particular, data augmentation techniques
such as CutMix and ClassMix generate additional training data from existing
labeled data. In this paper we propose a new approach for data augmentation,
termed ComplexMix, which incorporates aspects of CutMix and ClassMix with
improved performance. The proposed approach has the ability to control the
complexity of the augmented data while attempting to be semantically-correct
and address the tradeoff between complexity and correctness. The proposed
ComplexMix approach is evaluated on a standard dataset for semantic
segmentation and compared to other state-of-the-art techniques. Experimental
results show that our method yields improvement over state-of-the-art methods
on standard datasets for semantic image segmentation.",['cs.CV']
Network-Agnostic Knowledge Transfer for Medical Image Segmentation,"Conventional transfer learning leverages weights of pre-trained networks, but
mandates the need for similar neural architectures. Alternatively, knowledge
distillation can transfer knowledge between heterogeneous networks but often
requires access to the original training data or additional generative
networks. Knowledge transfer between networks can be improved by being agnostic
to the choice of network architecture and reducing the dependence on original
training data. We propose a knowledge transfer approach from a teacher to a
student network wherein we train the student on an independent transferal
dataset, whose annotations are generated by the teacher. Experiments were
conducted on five state-of-the-art networks for semantic segmentation and seven
datasets across three imaging modalities. We studied knowledge transfer from a
single teacher, combination of knowledge transfer and fine-tuning, and
knowledge transfer from multiple teachers. The student model with a single
teacher achieved similar performance as the teacher; and the student model with
multiple teachers achieved better performance than the teachers. The salient
features of our algorithm include: 1)no need for original training data or
generative networks, 2) knowledge transfer between different architectures, 3)
ease of implementation for downstream tasks by using the downstream task
dataset as the transferal dataset, 4) knowledge transfer of an ensemble of
models, trained independently, into one student model. Extensive experiments
demonstrate that the proposed algorithm is effective for knowledge transfer and
easily tunable.",['cs.CV']
DSAL: Deeply Supervised Active Learning from Strong and Weak Labelers for Biomedical Image Segmentation,"Image segmentation is one of the most essential biomedical image processing
problems for different imaging modalities, including microscopy and X-ray in
the Internet-of-Medical-Things (IoMT) domain. However, annotating biomedical
images is knowledge-driven, time-consuming, and labor-intensive, making it
difficult to obtain abundant labels with limited costs. Active learning
strategies come into ease the burden of human annotation, which queries only a
subset of training data for annotation. Despite receiving attention, most of
active learning methods generally still require huge computational costs and
utilize unlabeled data inefficiently. They also tend to ignore the intermediate
knowledge within networks. In this work, we propose a deep active
semi-supervised learning framework, DSAL, combining active learning and
semi-supervised learning strategies. In DSAL, a new criterion based on deep
supervision mechanism is proposed to select informative samples with high
uncertainties and low uncertainties for strong labelers and weak labelers
respectively. The internal criterion leverages the disagreement of intermediate
features within the deep learning network for active sample selection, which
subsequently reduces the computational costs. We use the proposed criteria to
select samples for strong and weak labelers to produce oracle labels and pseudo
labels simultaneously at each active learning iteration in an ensemble learning
manner, which can be examined with IoMT Platform. Extensive experiments on
multiple medical image datasets demonstrate the superiority of the proposed
method over state-of-the-art active learning methods.","['cs.CV', 'cs.AI', 'eess.IV']"
Personal Fixations-Based Object Segmentation with Object Localization and Boundary Preservation,"As a natural way for human-computer interaction, fixation provides a
promising solution for interactive image segmentation. In this paper, we focus
on Personal Fixations-based Object Segmentation (PFOS) to address issues in
previous studies, such as the lack of appropriate dataset and the ambiguity in
fixations-based interaction. In particular, we first construct a new PFOS
dataset by carefully collecting pixel-level binary annotation data over an
existing fixation prediction dataset, such dataset is expected to greatly
facilitate the study along the line. Then, considering characteristics of
personal fixations, we propose a novel network based on Object Localization and
Boundary Preservation (OLBP) to segment the gazed objects. Specifically, the
OLBP network utilizes an Object Localization Module (OLM) to analyze personal
fixations and locates the gazed objects based on the interpretation. Then, a
Boundary Preservation Module (BPM) is designed to introduce additional boundary
information to guard the completeness of the gazed objects. Moreover, OLBP is
organized in the mixed bottom-up and top-down manner with multiple types of
deep supervision. Extensive experiments on the constructed PFOS dataset show
the superiority of the proposed OLBP network over 17 state-of-the-art methods,
and demonstrate the effectiveness of the proposed OLM and BPM components. The
constructed PFOS dataset and the proposed OLBP network are available at
https://github.com/MathLee/OLBPNet4PFOS.",['cs.CV']
Finger Vein Recognition by Generating Code,"Finger vein recognition has drawn increasing attention as one of the most
popular and promising biometrics due to its high distinguishes ability,
security and non-invasive procedure. The main idea of traditional schemes is to
directly extract features from finger vein images or patterns and then compare
features to find the best match. However, the features extracted from images
contain much redundant data, while the features extracted from patterns are
greatly influenced by image segmentation methods. To tack these problems, this
paper proposes a new finger vein recognition by generating code. The proposed
method does not require an image segmentation algorithm, is simple to calculate
and has a small amount of data. Firstly, the finger vein images were divided
into blocks to calculate the mean value. Then the centrosymmetric coding is
performed by using the generated eigenmatrix. The obtained codewords are
concatenated as the feature codewords of the image. The similarity between vein
codes is measured by the ratio of minimum Hamming distance to codeword length.
Extensive experiments on two public finger vein databases verify the
effectiveness of the proposed method. The results indicate that our method
outperforms the state-of-theart methods and has competitive potential in
performing the matching task.","['cs.CV', 'cs.IT', 'math.IT', 'Computing methodologies for image processing', 'I.4']"
Nonparametric clustering for image segmentation,"Image segmentation aims at identifying regions of interest within an image,
by grouping pixels according to their properties. This task resembles the
statistical one of clustering, yet many standard clustering methods fail to
meet the basic requirements of image segmentation: segment shapes are often
biased toward predetermined shapes and their number is rarely determined
automatically. Nonparametric clustering is, in principle, free from these
limitations and turns out to be particularly suitable for the task of image
segmentation. This is also witnessed by several operational analogies, as, for
instance, the resort to topological data analysis and spatial tessellation in
both the frameworks. We discuss the application of nonparametric clustering to
image segmentation and provide an algorithm specific for this task. Pixel
similarity is evaluated in terms of density of the color representation and the
adjacency structure of the pixels is exploited to introduce a simple, yet
effective method to identify image segments as disconnected high-density
regions. The proposed method works both to segment an image and to detect its
boundaries and can be seen as a generalization to color images of the class of
thresholding methods.","['cs.CV', 'eess.IV', 'stat.AP', '62P99']"
U-Noise: Learnable Noise Masks for Interpretable Image Segmentation,"Deep Neural Networks (DNNs) are widely used for decision making in a myriad
of critical applications, ranging from medical to societal and even judicial.
Given the importance of these decisions, it is crucial for us to be able to
interpret these models. We introduce a new method for interpreting image
segmentation models by learning regions of images in which noise can be applied
without hindering downstream model performance. We apply this method to
segmentation of the pancreas in CT scans, and qualitatively compare the quality
of the method to existing explainability techniques, such as Grad-CAM and
occlusion sensitivity. Additionally we show that, unlike other methods, our
interpretability model can be quantitatively evaluated based on the downstream
performance over obscured images.",['cs.CV']
PaddleSeg: A High-Efficient Development Toolkit for Image Segmentation,"Image Segmentation plays an essential role in computer vision and image
processing with various applications from medical diagnosis to autonomous car
driving. A lot of segmentation algorithms have been proposed for addressing
specific problems. In recent years, the success of deep learning techniques has
tremendously influenced a wide range of computer vision areas, and the modern
approaches of image segmentation based on deep learning are becoming prevalent.
In this article, we introduce a high-efficient development toolkit for image
segmentation, named PaddleSeg. The toolkit aims to help both developers and
researchers in the whole process of designing segmentation models, training
models, optimizing performance and inference speed, and deploying models.
Currently, PaddleSeg supports around 20 popular segmentation models and more
than 50 pre-trained models from real-time and high-accuracy levels. With
modular components and backbone networks, users can easily build over one
hundred models for different requirements. Furthermore, we provide
comprehensive benchmarks and evaluations to show that these segmentation
algorithms trained on our toolkit have more competitive accuracy. Also, we
provide various real industrial applications and practical cases based on
PaddleSeg. All codes and examples of PaddleSeg are available at
https://github.com/PaddlePaddle/PaddleSeg.",['cs.CV']
Affinity Fusion Graph-based Framework for Natural Image Segmentation,"This paper proposes an affinity fusion graph framework to effectively connect
different graphs with highly discriminating power and nonlinearity for natural
image segmentation. The proposed framework combines adjacency-graphs and kernel
spectral clustering based graphs (KSC-graphs) according to a new definition
named affinity nodes of multi-scale superpixels. These affinity nodes are
selected based on a better affiliation of superpixels, namely
subspace-preserving representation which is generated by sparse subspace
clustering based on subspace pursuit. Then a KSC-graph is built via a novel
kernel spectral clustering to explore the nonlinear relationships among these
affinity nodes. Moreover, an adjacency-graph at each scale is constructed,
which is further used to update the proposed KSC-graph at affinity nodes. The
fusion graph is built across different scales, and it is partitioned to obtain
final segmentation result. Experimental results on the Berkeley segmentation
dataset and Microsoft Research Cambridge dataset show the superiority of our
framework in comparison with the state-of-the-art methods. The code is
available at https://github.com/Yangzhangcst/AF-graph.",['cs.CV']
Rethinking Interactive Image Segmentation: Feature Space Annotation,"Despite the progress of interactive image segmentation methods, high-quality
pixel-level annotation is still time-consuming and laborious -- a bottleneck
for several deep learning applications. We take a step back to propose
interactive and simultaneous segment annotation from multiple images guided by
feature space projection and optimized by metric learning as the labeling
progresses. This strategy is in stark contrast to existing interactive
segmentation methodologies, which perform annotation in the image domain. We
show that our approach can surpass the accuracy of state-of-the-art methods in
foreground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, it
achieves 91.5\% accuracy in a known semantic segmentation dataset, Cityscapes,
being 74.75 times faster than the original annotation procedure. The appendix
presents additional qualitative results. Code and video demonstration will be
released upon publication.",['cs.CV']
Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation,"We address the problem of semantic nighttime image segmentation and improve
the state-of-the-art, by adapting daytime models to nighttime without using
nighttime annotations. Moreover, we design a new evaluation framework to
address the substantial uncertainty of semantics in nighttime images. Our
central contributions are: 1) a curriculum framework to gradually adapt
semantic segmentation models from day to night through progressively darker
times of day, exploiting cross-time-of-day correspondences between daytime
images from a reference map and dark images to guide the label inference in the
dark domains; 2) a novel uncertainty-aware annotation and evaluation framework
and metric for semantic segmentation, including image regions beyond human
recognition capability in the evaluation in a principled fashion; 3) the Dark
Zurich dataset, comprising 2416 unlabeled nighttime and 2920 unlabeled twilight
images with correspondences to their daytime counterparts plus a set of 201
nighttime images with fine pixel-level annotations created with our protocol,
which serves as a first benchmark for our novel evaluation. Experiments show
that our map-guided curriculum adaptation significantly outperforms
state-of-the-art methods on nighttime sets both for standard metrics and our
uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals
that selective invalidation of predictions can improve results on data with
ambiguous content such as our benchmark and profit safety-oriented applications
involving invalid inputs.",['cs.CV']
Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds,"Boundary information plays a significant role in 2D image segmentation, while
usually being ignored in 3D point cloud segmentation where ambiguous features
might be generated in feature extraction, leading to misclassification in the
transition area between two objects. In this paper, firstly, we propose a
Boundary Prediction Module (BPM) to predict boundary points. Based on the
predicted boundary, a boundary-aware Geometric Encoding Module (GEM) is
designed to encode geometric information and aggregate features with
discrimination in a neighborhood, so that the local features belonging to
different categories will not be polluted by each other. To provide extra
geometric information for boundary-aware GEM, we also propose a light-weight
Geometric Convolution Operation (GCO), making the extracted features more
distinguishing. Built upon the boundary-aware GEM, we build our network and
test it on benchmarks like ScanNet v2, S3DIS. Results show our methods can
significantly improve the baseline and achieve state-of-the-art performance.
Code is available at https://github.com/JchenXu/BoundaryAwareGEM.",['cs.CV']
Diminishing Uncertainty within the Training Pool: Active Learning for Medical Image Segmentation,"Active learning is a unique abstraction of machine learning techniques where
the model/algorithm could guide users for annotation of a set of data points
that would be beneficial to the model, unlike passive machine learning. The
primary advantage being that active learning frameworks select data points that
can accelerate the learning process of a model and can reduce the amount of
data needed to achieve full accuracy as compared to a model trained on a
randomly acquired data set. Multiple frameworks for active learning combined
with deep learning have been proposed, and the majority of them are dedicated
to classification tasks. Herein, we explore active learning for the task of
segmentation of medical imaging data sets. We investigate our proposed
framework using two datasets: 1.) MRI scans of the hippocampus, 2.) CT scans of
pancreas and tumors. This work presents a query-by-committee approach for
active learning where a joint optimizer is used for the committee. At the same
time, we propose three new strategies for active learning: 1.) increasing
frequency of uncertain data to bias the training data set; 2.) Using mutual
information among the input images as a regularizer for acquisition to ensure
diversity in the training dataset; 3.) adaptation of Dice log-likelihood for
Stein variational gradient descent (SVGD). The results indicate an improvement
in terms of data reduction by achieving full accuracy while only using 22.69 %
and 48.85 % of the available data for each dataset, respectively.",['cs.CV']
Deep Class-Specific Affinity-Guided Convolutional Network for Multimodal Unpaired Image Segmentation,"Multi-modal medical image segmentation plays an essential role in clinical
diagnosis. It remains challenging as the input modalities are often not
well-aligned spatially. Existing learning-based methods mainly consider sharing
trainable layers across modalities and minimizing visual feature discrepancies.
While the problem is often formulated as joint supervised feature learning,
multiple-scale features and class-specific representation have not yet been
explored. In this paper, we propose an affinity-guided fully convolutional
network for multimodal image segmentation. To learn effective representations,
we design class-specific affinity matrices to encode the knowledge of
hierarchical feature reasoning, together with the shared convolutional layers
to ensure the cross-modality generalization. Our affinity matrix does not
depend on spatial alignments of the visual features and thus allows us to train
with unpaired, multimodal inputs. We extensively evaluated our method on two
public multimodal benchmark datasets and outperform state-of-the-art methods.","['cs.CV', 'cs.AI']"
RatLesNetv2: A Fully Convolutional Network for Rodent Brain Lesion Segmentation,"We present a fully convolutional neural network (ConvNet), named RatLesNetv2,
for segmenting lesions in rodent magnetic resonance (MR) brain images.
RatLesNetv2 architecture resembles an autoencoder and it incorporates residual
blocks that facilitate its optimization. RatLesNetv2 is trained end to end on
three-dimensional images and it requires no preprocessing. We evaluated
RatLesNetv2 on an exceptionally large dataset composed of 916 T2-weighted rat
brain MRI scans of 671 rats at nine different lesion stages that were used to
study focal cerebral ischemia for drug development. In addition, we compared
its performance with three other ConvNets specifically designed for medical
image segmentation. RatLesNetv2 obtained similar to higher Dice coefficient
values than the other ConvNets and it produced much more realistic and compact
segmentations with notably fewer holes and lower Hausdorff distance. The Dice
scores of RatLesNetv2 segmentations also exceeded inter-rater agreement of
manual segmentations. In conclusion, RatLesNetv2 could be used for automated
lesion segmentation, reducing human workload and improving reproducibility.
RatLesNetv2 is publicly available at https://github.com/jmlipman/RatLesNetv2.","['cs.CV', 'eess.IV']"
ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation,"Unsupervised domain adaptation (UDA) for cross-modality medical image
segmentation has shown great progress by domain-invariant feature learning or
image appearance translation. Adapted feature learning usually cannot detect
domain shifts at the pixel level and is not able to achieve good results in
dense semantic segmentation tasks. Image appearance translation, e.g. CycleGAN,
translates images into different styles with good appearance, despite its
population, its semantic consistency is hardly to maintain and results in poor
cross-modality segmentation. In this paper, we propose intra- and
cross-modality semantic consistency (ICMSC) for UDA and our key insight is that
the segmentation of synthesised images in different styles should be
consistent. Specifically, our model consists of an image translation module and
a domain-specific segmentation module. The image translation module is a
standard CycleGAN, while the segmentation module contains two domain-specific
segmentation networks. The intra-modality semantic consistency (IMSC) forces
the reconstructed image after a cycle to be segmented in the same way as the
original input image, while the cross-modality semantic consistency (CMSC)
encourages the synthesized images after translation to be segmented exactly the
same as before translation. Comprehensive experimental results on
cross-modality hip joint bone segmentation show the effectiveness of our
proposed method, which achieves an average DICE of 81.61% on the acetabulum and
88.16% on the proximal femur, outperforming other state-of-the-art methods. It
is worth to note that without UDA, a model trained on CT for hip joint bone
segmentation is non-transferable to MRI and has almost zero-DICE segmentation.","['cs.CV', 'cs.AI']"
Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty,"In image segmentation, there is often more than one plausible solution for a
given input. In medical imaging, for example, experts will often disagree about
the exact location of object boundaries. Estimating this inherent uncertainty
and predicting multiple plausible hypotheses is of great interest in many
applications, yet this ability is lacking in most current deep learning
methods. In this paper, we introduce stochastic segmentation networks (SSNs),
an efficient probabilistic method for modelling aleatoric uncertainty with any
image segmentation network architecture. In contrast to approaches that produce
pixel-wise estimates, SSNs model joint distributions over entire label maps and
thus can generate multiple spatially coherent hypotheses for a single image. By
using a low-rank multivariate normal distribution over the logit space to model
the probability of the label map given the image, we obtain a spatially
consistent probability distribution that can be efficiently computed by a
neural network without any changes to the underlying architecture. We tested
our method on the segmentation of real-world medical data, including lung
nodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform
state-of-the-art for modelling correlated uncertainty in ambiguous images while
being much simpler, more flexible, and more efficient.","['cs.CV', 'cs.LG']"
Power-SLIC: Diagram-based superpixel generation,"Superpixel algorithms, which group pixels similar in color and other
low-level properties, are increasingly used for pre-processing in image
segmentation. Commonly important criteria for the computation of superpixels
are boundary adherence, speed, and regularity.
  Boundary adherence and regularity are typically contradictory goals. Most
recent algorithms have focused on improving boundary adherence. Motivated by
improving superpixel regularity, we propose a diagram-based superpixel
generation method called Power-SLIC.
  On the BSDS500 data set, Power-SLIC outperforms other state-of-the-art
algorithms in terms of compactness and boundary precision, and its boundary
adherence is the most robust against varying levels of Gaussian noise. In terms
of speed, Power-SLIC is competitive with SLIC.","['cs.CV', 'cs.CG', 'cs.LG']"
Cloud Cover Nowcasting with Deep Learning,"Nowcasting is a field of meteorology which aims at forecasting weather on a
short term of up to a few hours. In the meteorology landscape, this field is
rather specific as it requires particular techniques, such as data
extrapolation, where conventional meteorology is generally based on physical
modeling. In this paper, we focus on cloud cover nowcasting, which has various
application areas such as satellite shots optimisation and photovoltaic energy
production forecast.
  Following recent deep learning successes on multiple imagery tasks, we
applied deep convolutionnal neural networks on Meteosat satellite images for
cloud cover nowcasting. We present the results of several architectures
specialized in image segmentation and time series prediction. We selected the
best models according to machine learning metrics as well as meteorological
metrics. All selected architectures showed significant improvements over
persistence and the well-known U-Net surpasses AROME physical model.","['cs.CV', 'cs.AI', 'cs.LG']"
Computer Stereo Vision for Autonomous Driving,"As an important component of autonomous systems, autonomous car perception
has had a big leap with recent advances in parallel computing architectures.
With the use of tiny but full-feature embedded supercomputers, computer stereo
vision has been prevalently applied in autonomous cars for depth perception.
The two key aspects of computer stereo vision are speed and accuracy. They are
both desirable but conflicting properties, as the algorithms with better
disparity accuracy usually have higher computational complexity. Therefore, the
main aim of developing a computer stereo vision algorithm for resource-limited
hardware is to improve the trade-off between speed and accuracy. In this
chapter, we introduce both the hardware and software aspects of computer stereo
vision for autonomous car systems. Then, we discuss four autonomous car
perception tasks, including 1) visual feature detection, description and
matching, 2) 3D information acquisition, 3) object detection/recognition and 4)
semantic image segmentation. The principles of computer stereo vision and
parallel computing on multi-threading CPU and GPU architectures are then
detailed.",['cs.CV']
DA-RefineNet:A Dual Input Whole Slide Image Segmentation Algorithm Based on Attention,"Automatic medical image segmentation has wide applications for disease
diagnosing. However, it is much more challenging than natural optical image
segmentation due to the high-resolution of medical images and the corresponding
huge computation cost. The sliding window is a commonly used technique for
whole slide image (WSI) segmentation, however, for these methods based on the
sliding window, the main drawback is lacking global contextual information for
supervision. In this paper, we propose a dual-inputs attention network (denoted
as DA-RefineNet) for WSI segmentation, where both local fine-grained
information and global coarse information can be efficiently utilized.
Sufficient comparative experiments are conducted to evaluate the effectiveness
of the proposed method, the results prove that the proposed method can achieve
better performance on WSI segmentation compared to methods relying on
single-input.","['cs.CV', 'eess.IV']"
Unlabeled Data Guided Semi-supervised Histopathology Image Segmentation,"Automatic histopathology image segmentation is crucial to disease analysis.
Limited available labeled data hinders the generalizability of trained models
under the fully supervised setting. Semi-supervised learning (SSL) based on
generative methods has been proven to be effective in utilizing diverse image
characteristics. However, it has not been well explored what kinds of generated
images would be more useful for model training and how to use such images. In
this paper, we propose a new data guided generative method for histopathology
image segmentation by leveraging the unlabeled data distributions. First, we
design an image generation module. Image content and style are disentangled and
embedded in a clustering-friendly space to utilize their distributions. New
images are synthesized by sampling and cross-combining contents and styles.
Second, we devise an effective data selection policy for judiciously sampling
the generated images: (1) to make the generated training set better cover the
dataset, the clusters that are underrepresented in the original training set
are covered more; (2) to make the training process more effective, we identify
and oversample the images of ""hard cases"" in the data for which annotated
training data may be scarce. Our method is evaluated on glands and nuclei
datasets. We show that under both the inductive and transductive settings, our
SSL method consistently boosts the performance of common segmentation models
and attains state-of-the-art results.",['cs.CV']
CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks,"3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene
understanding, such as video analysis and volumetric image recognition.
However, 3D networks can easily lead to over-parameterization which incurs
expensive computation cost. In this paper, we propose Channel-wise Automatic
KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard
3D convolutions into a set of economic operations e.g., 1D, 2D convolutions.
Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which
enjoys the following benefits: 1) enabling operations deployed in every layer
to be heterogeneous, so that they can extract diverse and complementary
information to benefit the learning process; and 2) allowing for an efficient
and flexible replacement design, which can be generalized to both
spatial-temporal and volumetric data. Further, we propose a new search space
based on CAKES, so that the replacement configuration can be determined
automatically for simplifying 3D networks. CAKES shows superior performance to
other methods with similar model size, and it also achieves comparable
performance to state-of-the-art with much fewer parameters and computational
costs on tasks including 3D medical imaging segmentation and video action
recognition. Codes and models are available at
https://github.com/yucornetto/CAKES",['cs.CV']
Unsupervised Image Segmentation using Mutual Mean-Teaching,"Unsupervised image segmentation aims at assigning the pixels with similar
feature into a same cluster without annotation, which is an important task in
computer vision. Due to lack of prior knowledge, most of existing model usually
need to be trained several times to obtain suitable results. To address this
problem, we propose an unsupervised image segmentation model based on the
Mutual Mean-Teaching (MMT) framework to produce more stable results. In
addition, since the labels of pixels from two model are not matched, a label
alignment algorithm based on the Hungarian algorithm is proposed to match the
cluster labels. Experimental results demonstrate that the proposed model is
able to segment various types of images and achieves better performance than
the existing methods.","['cs.CV', 'cs.AI']"
mDALU: Multi-Source Domain Adaptation and Label Unification with Partial Datasets,"Object recognition advances very rapidly these days. One challenge is to
generalize existing methods to new domains, to more classes and/or to new data
modalities. In order to avoid annotating one dataset for each of these new
cases, one needs to combine and reuse existing datasets that may belong to
different domains, have partial annotations, and/or have different data
modalities. This paper treats this task as a multi-source domain adaptation and
label unification (mDALU) problem and proposes a novel method for it. Our
method consists of a partially-supervised adaptation stage and a
fully-supervised adaptation stage. In the former, partial knowledge is
transferred from multiple source domains to the target domain and fused
therein. Negative transfer between unmatched label space is mitigated via three
new modules: domain attention, uncertainty maximization and attention-guided
adversarial alignment. In the latter, knowledge is transferred in the unified
label space after a label completion process with pseudo-labels. We verify the
method on three different tasks, image classification, 2D semantic image
segmentation, and joint 2D-3D semantic segmentation. Extensive experiments show
that our method outperforms all competing methods significantly.",['cs.CV']
Pyramid-Focus-Augmentation: Medical Image Segmentation with Step-Wise Focus,"Segmentation of findings in the gastrointestinal tract is a challenging but
also an important task which is an important building stone for sufficient
automatic decision support systems. In this work, we present our solution for
the Medico 2020 task, which focused on the problem of colon polyp segmentation.
We present our simple but efficient idea of using an augmentation method that
uses grids in a pyramid-like manner (large to small) for segmentation. Our
results show that the proposed methods work as indented and can also lead to
comparable results when competing with other methods.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.MM']"
Meticulous Object Segmentation,"Compared with common image segmentation tasks targeted at low-resolution
images, higher resolution detailed image segmentation receives much less
attention. In this paper, we propose and study a task named Meticulous Object
Segmentation (MOS), which is focused on segmenting well-defined foreground
objects with elaborate shapes in high resolution images (e.g. 2k - 4k). To this
end, we propose the MeticulousNet which leverages a dedicated decoder to
capture the object boundary details. Specifically, we design a Hierarchical
Point-wise Refining (HierPR) block to better delineate object boundaries, and
reformulate the decoding process as a recursive coarse to fine refinement of
the object mask. To evaluate segmentation quality near object boundaries, we
propose the Meticulosity Quality (MQ) score considering both the mask coverage
and boundary precision. In addition, we collect a MOS benchmark dataset
including 600 high quality images with complex objects. We provide
comprehensive empirical evidence showing that MeticulousNet can reveal
pixel-accurate segmentation boundaries and is superior to state-of-the-art
methods for high resolution object segmentation tasks.",['cs.CV']
Domain Adaptation on Semantic Segmentation for Aerial Images,"Semantic segmentation has achieved significant advances in recent years.
While deep neural networks perform semantic segmentation well, their success
rely on pixel level supervision which is expensive and time-consuming. Further,
training using data from one domain may not generalize well to data from a new
domain due to a domain gap between data distributions in the different domains.
This domain gap is particularly evident in aerial images where visual
appearance depends on the type of environment imaged, season, weather, and time
of day when the environment is imaged. Subsequently, this distribution gap
leads to severe accuracy loss when using a pretrained segmentation model to
analyze new data with different characteristics. In this paper, we propose a
novel unsupervised domain adaptation framework to address domain shift in the
context of aerial semantic image segmentation. To this end, we solve the
problem of domain shift by learn the soft label distribution difference between
the source and target domains. Further, we also apply entropy minimization on
the target domain to produce high-confident prediction rather than using
high-confident prediction by pseudo-labeling. We demonstrate the effectiveness
of our domain adaptation framework using the challenge image segmentation
dataset of ISPRS, and show improvement over state-of-the-art methods in terms
of various metrics.",['cs.CV']
Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding,"Despite deep convolutional neural networks achieved impressive progress in
medical image computing and analysis, its paradigm of supervised learning
demands a large number of annotations for training to avoid overfitting and
achieving promising results. In clinical practices, massive semantic
annotations are difficult to acquire in some conditions where specialized
biomedical expert knowledge is required, and it is also a common condition
where only few annotated classes are available. In this work, we proposed a
novel method for few-shot medical image segmentation, which enables a
segmentation model to fast generalize to an unseen class with few training
images. We construct our few-shot image segmentor using a deep convolutional
network trained episodically. Motivated by the spatial consistency and
regularity in medical images, we developed an efficient global correlation
module to capture the correlation between a support and query image and
incorporate it into the deep network called global correlation network.
Moreover, we enhance discriminability of deep embedding to encourage clustering
of the feature domains of the same class while keep the feature domains of
different organs far apart. Ablation Study proved the effectiveness of the
proposed global correlation module and discriminative embedding loss. Extensive
experiments on anatomical abdomen images on both CT and MRI modalities are
performed to demonstrate the state-of-the-art performance of our proposed
model.",['cs.CV']
Central object segmentation by deep learning for fruits and other roundish objects,"We present CROP (Central Roundish Object Painter), which identifies and
paints the object at the center of an RGB image. Primarily CROP works for
roundish fruits in various illumination conditions, but surprisingly, it could
also deal with images of other organic or inorganic materials, or ones by
optical and electron microscopes, although CROP was trained solely by 172
images of fruits. The method involves image segmentation by deep learning, and
the architecture of the neural network is a deeper version of the original
U-Net. This technique could provide us with a means of automatically collecting
statistical data of fruit growth in farms. As an example, we describe our
experiment of processing 510 time series photos automatically to collect the
data on the size and the position of the target fruit. Our trained neural
network CROP and the above automatic programs are available on GitHub with
user-friendly interface programs.","['cs.CV', 'cs.LG']"
"Artist, Style And Year Classification Using Face Recognition And Clustering With Convolutional Neural Networks","Artist, year and style classification of fine-art paintings are generally
achieved using standard image classification methods, image segmentation, or
more recently, convolutional neural networks (CNNs). This works aims to use
newly developed face recognition methods such as FaceNet that use CNNs to
cluster fine-art paintings using the extracted faces in the paintings, which
are found abundantly. A dataset consisting of over 80,000 paintings from over
1000 artists is chosen, and three separate face recognition and clustering
tasks are performed. The produced clusters are analyzed by the file names of
the paintings and the clusters are named by their majority artist, year range,
and style. The clusters are further analyzed and their performance metrics are
calculated. The study shows promising results as the artist, year, and styles
are clustered with an accuracy of 58.8, 63.7, and 81.3 percent, while the
clusters have an average purity of 63.1, 72.4, and 85.9 percent.",['cs.CV']
Morphological Network: How Far Can We Go with Morphological Neurons?,"In recent years, the idea of using morphological operations as networks has
received much attention. Mathematical morphology provides very efficient and
useful image processing and image analysis tools based on basic operators like
dilation and erosion, defined in terms of kernels. Many other morphological
operations are built up using the dilation and erosion operations. Although the
learning of structuring elements such as dilation or erosion using the
backpropagation algorithm is not new, the order and the way these morphological
operations are used is not standard. In this paper, we have theoretically
analyzed the use of morphological operations for processing 1D feature vectors
and shown that this gets extended to the 2D case in a simple manner. Our
theoretical results show that a morphological block represents a sum of hinge
functions. Hinge functions are used in many places for classification and
regression tasks (Breiman (1993)). We have also proved a universal
approximation theorem -- a stack of two morphological blocks can approximate
any continuous function over arbitrary compact sets. To experimentally validate
the efficacy of this network in real-life applications, we have evaluated its
performance on satellite image classification datasets since morphological
operations are very sensitive to geometrical shapes and structures. We have
also shown results on a few tasks like segmentation of blood vessels from
fundus images, segmentation of lungs from chest x-ray and image dehazing. The
results are encouraging and further establishes the potential of morphological
networks.","['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']"
Towards Robust Medical Image Segmentation on Small-Scale Data with Incomplete Labels,"The data-driven nature of deep learning models for semantic segmentation
requires a large number of pixel-level annotations. However, large-scale and
fully labeled medical datasets are often unavailable for practical tasks.
Recently, partially supervised methods have been proposed to utilize images
with incomplete labels to mitigate the data scarcity problem in the medical
domain. As an emerging research area, the breakthroughs made by existing
methods rely on either large-scale data or complex model design, which makes
them 1) less practical for certain real-life tasks and 2) less robust for
small-scale data. It is time to step back and think about the robustness of
partially supervised methods and how to maximally utilize small-scale and
partially labeled data for medical image segmentation tasks. To bridge the
methodological gaps in label-efficient deep learning with partial supervision,
we propose RAMP, a simple yet efficient data augmentation framework for
partially supervised medical image segmentation by exploiting the assumption
that patients share anatomical similarities. We systematically evaluate RAMP
and the previous methods in various controlled multi-structure segmentation
tasks. Compared to the mainstream approaches, RAMP consistently improves the
performance of traditional segmentation networks on small-scale partially
labeled data and utilize additional image-wise weak annotations.","['cs.CV', 'cs.LG', 'eess.IV']"
Vox2Vox: 3D-GAN for Brain Tumour Segmentation,"Gliomas are the most common primary brain malignancies, with different
degrees of aggressiveness, variable prognosis and various heterogeneous
histological sub-regions, i.e., peritumoral edema, necrotic core, enhancing and
non-enhancing tumour core. Although brain tumours can easily be detected using
multi-modal MRI, accurate tumor segmentation is a challenging task. Hence,
using the data provided by the BraTS Challenge 2020, we propose a 3D
volume-to-volume Generative Adversarial Network for segmentation of brain
tumours. The model, called Vox2Vox, generates realistic segmentation outputs
from multi-channel 3D MR images, segmenting the whole, core and enhancing tumor
with mean values of 87.20%, 81.14%, and 78.67% as dice scores and 6.44mm,
24.36mm, and 18.95mm for Hausdorff distance 95 percentile for the BraTS testing
set after ensembling 10 Vox2Vox models obtained with a 10-fold
cross-validation.","['cs.CV', 'cs.LG', 'eess.IV']"
PGL: Prior-Guided Local Self-supervised Learning for 3D Medical Image Segmentation,"It has been widely recognized that the success of deep learning in image
segmentation relies overwhelmingly on a myriad amount of densely annotated
training data, which, however, are difficult to obtain due to the tremendous
labor and expertise required, particularly for annotating 3D medical images.
Although self-supervised learning (SSL) has shown great potential to address
this issue, most SSL approaches focus only on image-level global consistency,
but ignore the local consistency which plays a pivotal role in capturing
structural information for dense prediction tasks such as segmentation. In this
paper, we propose a PriorGuided Local (PGL) self-supervised model that learns
the region-wise local consistency in the latent feature space. Specifically, we
use the spatial transformations, which produce different augmented views of the
same image, as a prior to deduce the location relation between two views, which
is then used to align the feature maps of the same local region but being
extracted on two views. Next, we construct a local consistency loss to minimize
the voxel-wise discrepancy between the aligned feature maps. Thus, our PGL
model learns the distinctive representations of local regions, and hence is
able to retain structural information. This ability is conducive to downstream
segmentation tasks. We conducted an extensive evaluation on four public
computerized tomography (CT) datasets that cover 11 kinds of major human organs
and two tumors. The results indicate that using pre-trained PGL model to
initialize a downstream network leads to a substantial performance improvement
over both random initialization and the initialization with global
consistency-based models. Code and pre-trained weights will be made available
at: https://git.io/PGL.",['cs.CV']
Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity,"Infrared (IR) image segmentation is essential in many urban defence
applications, such as pedestrian surveillance, vehicle counting, security
monitoring, etc. Active contour model (ACM) is one of the most widely used
image segmentation tools at present, but the existing methods only utilize the
local or global single feature information of image to minimize the energy
function, which is easy to cause false segmentations in IR images. In this
paper, we propose a multi-feature driven active contour segmentation model to
handle IR images with intensity inhomogeneity. Firstly, an especially-designed
signed pressure force (SPF) function is constructed by combining the global
information calculated by global average gray information and the local
multi-feature information calculated by local entropy, local standard deviation
and gradient information. Then, we draw upon adaptive weight coefficient
calculated by local range to adjust the afore-mentioned global term and local
term. Next, the SPF function is substituted into the level set formulation
(LSF) for further evolution. Finally, the LSF converges after a finite number
of iterations, and the IR image segmentation result is obtained from the
corresponding convergence result. Experimental results demonstrate that the
presented method outperforms the state-of-the-art models in terms of precision
rate and overlapping rate in IR test images.",['cs.CV']
A comparative study of semi- and self-supervised semantic segmentation of biomedical microscopy data,"In recent years, Convolutional Neural Networks (CNNs) have become the
state-of-the-art method for biomedical image analysis. However, these networks
are usually trained in a supervised manner, requiring large amounts of labelled
training data. These labelled data sets are often difficult to acquire in the
biomedical domain. In this work, we validate alternative ways to train CNNs
with fewer labels for biomedical image segmentation using. We adapt two semi-
and self-supervised image classification methods and analyse their performance
for semantic segmentation of biomedical microscopy images.","['cs.CV', 'cs.AI', 'stat.AP']"
High-level Prior-based Loss Functions for Medical Image Segmentation: A Survey,"Today, deep convolutional neural networks (CNNs) have demonstrated state of
the art performance for supervised medical image segmentation, across various
imaging modalities and tasks. Despite early success, segmentation networks may
still generate anatomically aberrant segmentations, with holes or inaccuracies
near the object boundaries. To mitigate this effect, recent research works have
focused on incorporating spatial information or prior knowledge to enforce
anatomically plausible segmentation. If the integration of prior knowledge in
image segmentation is not a new topic in classical optimization approaches, it
is today an increasing trend in CNN based image segmentation, as shown by the
growing literature on the topic. In this survey, we focus on high level prior,
embedded at the loss function level. We categorize the articles according to
the nature of the prior: the object shape, size, topology, and the
inter-regions constraints. We highlight strengths and limitations of current
approaches, discuss the challenge related to the design and the integration of
prior-based losses, and the optimization strategies, and draw future research
directions.","['cs.CV', 'cs.LG']"
Segmentation overlapping wear particles with few labelled data and imbalance sample,"Ferrograph image segmentation is of significance for obtaining features of
wear particles. However, wear particles are usually overlapped in the form of
debris chains, which makes challenges to segment wear debris. An overlapping
wear particle segmentation network (OWPSNet) is proposed in this study to
segment the overlapped debris chains. The proposed deep learning model includes
three parts: a region segmentation network, an edge detection network and a
feature refine module. The region segmentation network is an improved U shape
network, and it is applied to separate the wear debris form background of
ferrograph image. The edge detection network is used to detect the edges of
wear particles. Then, the feature refine module combines low-level features and
high-level semantic features to obtain the final results. In order to solve the
problem of sample imbalance, we proposed a square dice loss function to
optimize the model. Finally, extensive experiments have been carried out on a
ferrograph image dataset. Results show that the proposed model is capable of
separating overlapping wear particles. Moreover, the proposed square dice loss
function can improve the segmentation results, especially for the segmentation
results of wear particle edge.",['cs.CV']
DoDNet: Learning to segment multi-organ and tumors from multiple partially labeled datasets,"Due to the intensive cost of labor and expertise in annotating 3D medical
images at a voxel level, most benchmark datasets are equipped with the
annotations of only one type of organs and/or tumors, resulting in the
so-called partially labeling issue. To address this, we propose a dynamic
on-demand network (DoDNet) that learns to segment multiple organs and tumors on
partially labeled datasets. DoDNet consists of a shared encoder-decoder
architecture, a task encoding module, a controller for generating dynamic
convolution filters, and a single but dynamic segmentation head. The
information of the current segmentation task is encoded as a task-aware prior
to tell the model what the task is expected to solve. Different from existing
approaches which fix kernels after training, the kernels in dynamic head are
generated adaptively by the controller, conditioned on both input image and
assigned task. Thus, DoDNet is able to segment multiple organs and tumors, as
done by multiple networks or a multi-head network, in a much efficient and
flexible manner. We have created a large-scale partially labeled dataset,
termed MOTS, and demonstrated the superior performance of our DoDNet over other
competitors on seven organ and tumor segmentation tasks. We also transferred
the weights pre-trained on MOTS to a downstream multi-organ segmentation task
and achieved state-of-the-art performance. This study provides a general 3D
medical image segmentation model that has been pre-trained on a large-scale
partially labelled dataset and can be extended (after fine-tuning) to
downstream volumetric medical data segmentation tasks. The dataset and code
areavailableat: https://git.io/DoDNet",['cs.CV']
Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation,"Segmentation of organs of interest in 3D medical images is necessary for
accurate diagnosis and longitudinal studies. Though recent advances using deep
learning have shown success for many segmentation tasks, large datasets are
required for high performance and the annotation process is both time consuming
and labor intensive. In this paper, we propose a 3D few shot segmentation
framework for accurate organ segmentation using limited training samples of the
target organ annotation. To achieve this, a U-Net like network is designed to
predict segmentation by learning the relationship between 2D slices of support
data and a query image, including a bidirectional gated recurrent unit (GRU)
that learns consistency of encoded features between adjacent slices. Also, we
introduce a transfer learning method to adapt the characteristics of the target
image and organ by updating the model before testing with arbitrary support and
query data sampled from the support data. We evaluate our proposed model using
three 3D CT datasets with annotations of different organs. Our model yielded
significantly improved performance over state-of-the-art few shot segmentation
models and was comparable to a fully supervised model trained with more target
training data.","['cs.CV', 'cs.AI']"
Dilated Convolutions with Lateral Inhibitions for Semantic Image Segmentation,"Dilated convolutions are widely used in deep semantic segmentation models as
they can enlarge the filters' receptive field without adding additional weights
nor sacrificing spatial resolution. However, as dilated convolutional filters
do not possess positional knowledge about the pixels on semantically meaningful
contours, they could lead to ambiguous predictions on object boundaries. In
addition, although dilating the filter can expand its receptive field, the
total number of sampled pixels remains unchanged, which usually comprises a
small fraction of the receptive field's total area. Inspired by the Lateral
Inhibition (LI) mechanisms in human visual systems, we propose the dilated
convolution with lateral inhibitions (LI-Convs) to overcome these limitations.
Introducing LI mechanisms improves the convolutional filter's sensitivity to
semantic object boundaries. Moreover, since LI-Convs also implicitly take the
pixels from the laterally inhibited zones into consideration, they can also
extract features at a denser scale. By integrating LI-Convs into the Deeplabv3+
architecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling
(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the Lateral
Inhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets
(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentation
models outperform the baseline on all of them, thus verify the effectiveness
and generality of the proposed LI-Convs.",['cs.CV']
Enforcing Perceptual Consistency on Generative Adversarial Networks by Using the Normalised Laplacian Pyramid Distance,"In recent years there has been a growing interest in image generation through
deep learning. While an important part of the evaluation of the generated
images usually involves visual inspection, the inclusion of human perception as
a factor in the training process is often overlooked. In this paper we propose
an alternative perceptual regulariser for image-to-image translation using
conditional generative adversarial networks (cGANs). To do so automatically
(avoiding visual inspection), we use the Normalised Laplacian Pyramid Distance
(NLPD) to measure the perceptual similarity between the generated image and the
original image. The NLPD is based on the principle of normalising the value of
coefficients with respect to a local estimate of mean energy at different
scales and has already been successfully tested in different experiments
involving human perception. We compare this regulariser with the originally
proposed L1 distance and note that when using NLPD the generated images contain
more realistic values for both local and global contrast. We found that using
NLPD as a regulariser improves image segmentation accuracy on generated images
as well as improving two no-reference image quality metrics.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging,"Medical image segmentation is one of the major challenges addressed by
machine learning methods. Yet, deep learning methods profoundly depend on a
large amount of annotated data, which is time-consuming and costly. Though,
semi-supervised learning methods approach this problem by leveraging an
abundant amount of unlabeled data along with a small amount of labeled data in
the training process. Recently, MixUp regularizer has been successfully
introduced to semi-supervised learning methods showing superior performance.
MixUp augments the model with new data points through linear interpolation of
the data at the input space. We argue that this option is limited. Instead, we
propose ROAM, a RandOm lAyer Mixup, which encourages the network to be less
confident for interpolated data points at randomly selected space. ROAM
generates more data points that have never seen before, and hence it avoids
over-fitting and enhances the generalization ability. We conduct extensive
experiments to validate our method on three publicly available datasets on
whole-brain image segmentation. ROAM achieves state-of-the-art (SOTA) results
in fully supervised (89.5%) and semi-supervised (87.0%) settings with a
relative improvement of up to 2.40% and 16.50%, respectively for the
whole-brain segmentation.",['cs.CV']
w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution,"Deep learning-based medical image segmentation technology aims at automatic
recognizing and annotating objects on the medical image. Non-local attention
and feature learning by multi-scale methods are widely used to model network,
which drives progress in medical image segmentation. However, those attention
mechanism methods have weakly non-local receptive fields' strengthened
connection for small objects in medical images. Then, the features of important
small objects in abstract or coarse feature maps may be deserted, which leads
to unsatisfactory performance. Moreover, the existing multi-scale methods only
simply focus on different sizes of view, whose sparse multi-scale features
collected are not abundant enough for small objects segmentation. In this work,
a multi-dimensional attention segmentation model with cascade multi-scale
convolution is proposed to predict accurate segmentation for small objects in
medical images. As the weight function, multi-dimensional attention modules
provide coefficient modification for significant/informative small objects
features. Furthermore, The cascade multi-scale convolution modules in each
skip-connection path are exploited to capture multi-scale features in different
semantic depth. The proposed method is evaluated on three datasets: KiTS19,
Pancreas CT of Decathlon-10, and MICCAI 2018 LiTS Challenge, demonstrating
better segmentation performances than the state-of-the-art baselines.",['cs.CV']
Image Segmentation Using Deep Learning: A Survey,"Image segmentation is a key topic in image processing and computer vision
with applications such as scene understanding, medical image analysis, robotic
perception, video surveillance, augmented reality, and image compression, among
many others. Various algorithms for image segmentation have been developed in
the literature. Recently, due to the success of deep learning models in a wide
range of vision applications, there has been a substantial amount of works
aimed at developing image segmentation approaches using deep learning models.
In this survey, we provide a comprehensive review of the literature at the time
of this writing, covering a broad spectrum of pioneering works for semantic and
instance-level segmentation, including fully convolutional pixel-labeling
networks, encoder-decoder architectures, multi-scale and pyramid based
approaches, recurrent networks, visual attention models, and generative models
in adversarial settings. We investigate the similarity, strengths and
challenges of these deep learning models, examine the most widely used
datasets, report performances, and discuss promising future research directions
in this area.","['cs.CV', 'cs.LG']"
A Simple Probabilistic Method for Deep Classification under Input-Dependent Label Noise,"Datasets with noisy labels are a common occurrence in practical applications
of classification methods. We propose a simple probabilistic method for
training deep classifiers under input-dependent (heteroscedastic) label noise.
We assume an underlying heteroscedastic generative process for noisy labels. To
make gradient based training feasible we use a temperature parameterized
softmax as a smooth approximation to the assumed generative process. We
illustrate that the softmax temperature controls a bias-variance trade-off for
the approximation. By tuning the softmax temperature, we improve accuracy,
log-likelihood and calibration on both image classification benchmarks with
controlled label noise as well as Imagenet-21k which has naturally occurring
label noise. For image segmentation, our method increases the mean IoU on the
PASCAL VOC and Cityscapes datasets by more than 1% over the state-of-the-art
model.","['cs.LG', 'stat.ML']"
Video Semantic Segmentation with Distortion-Aware Feature Correction,"Video semantic segmentation is active in recent years benefited from the
great progress of image semantic segmentation. For such a task, the per-frame
image segmentation is generally unacceptable in practice due to high
computation cost. To tackle this issue, many works use the flow-based feature
propagation to reuse the features of previous frames. However, the optical flow
estimation inevitably suffers inaccuracy and then causes the propagated
features distorted. In this paper, we propose distortion-aware feature
correction to alleviate the issue, which improves video segmentation
performance by correcting distorted propagated features. To be specific, we
firstly propose to transfer distortion patterns from feature into image space
and conduct effective distortion map prediction. Benefited from the guidance of
distortion maps, we proposed Feature Correction Module (FCM) to rectify
propagated features in the distorted areas. Our proposed method can
significantly boost the accuracy of video semantic segmentation at a low price.
The extensive experimental results on Cityscapes and CamVid show that our
method outperforms the recent state-of-the-art methods.",['cs.CV']
Interpretable and synergistic deep learning for visual explanation and statistical estimations of segmentation of disease features from medical images,"Deep learning (DL) models for disease classification or segmentation from
medical images are increasingly trained using transfer learning (TL) from
unrelated natural world images. However, shortcomings and utility of TL for
specialized tasks in the medical imaging domain remain unknown and are based on
assumptions that increasing training data will improve performance. We report
detailed comparisons, rigorous statistical analysis and comparisons of widely
used DL architecture for binary segmentation after TL with ImageNet
initialization (TII-models) with supervised learning with only medical
images(LMI-models) of macroscopic optical skin cancer, microscopic prostate
core biopsy and Computed Tomography (CT) DICOM images. Through visual
inspection of TII and LMI model outputs and their Grad-CAM counterparts, our
results identify several counter intuitive scenarios where automated
segmentation of one tumor by both models or the use of individual segmentation
output masks in various combinations from individual models leads to 10%
increase in performance. We also report sophisticated ensemble DL strategies
for achieving clinical grade medical image segmentation and model explanations
under low data regimes. For example; estimating performance, explanations and
replicability of LMI and TII models described by us can be used for situations
in which sparsity promotes better learning. A free GitHub repository of TII and
LMI models, code and more than 10,000 medical images and their Grad-CAM output
from this study can be used as starting points for advanced computational
medicine and DL research for biomedical discovery and applications.","['stat.ML', 'cs.LG']"
"Disentangle, align and fuse for multimodal and semi-supervised image segmentation","Magnetic resonance (MR) protocols rely on several sequences to assess
pathology and organ status properly. Despite advances in image analysis, we
tend to treat each sequence, here termed modality, in isolation. Taking
advantage of the common information shared between modalities (an organ's
anatomy) is beneficial for multi-modality processing and learning. However, we
must overcome inherent anatomical misregistrations and disparities in signal
intensity across the modalities to obtain this benefit. We present a method
that offers improved segmentation accuracy of the modality of interest (over a
single input model), by learning to leverage information present in other
modalities, even if few (semi-supervised) or no (unsupervised) annotations are
available for this specific modality. Core to our method is learning a
disentangled decomposition into anatomical and imaging factors. Shared
anatomical factors from the different inputs are jointly processed and fused to
extract more accurate segmentation masks. Image misregistrations are corrected
with a Spatial Transformer Network, which non-linearly aligns the anatomical
factors. The imaging factor captures signal intensity characteristics across
different modality data and is used for image reconstruction, enabling
semi-supervised learning. Temporal and slice pairing between inputs are learned
dynamically. We demonstrate applications in Late Gadolinium Enhanced (LGE) and
Blood Oxygenation Level Dependent (BOLD) cardiac segmentation, as well as in T2
abdominal segmentation. Code is available at
https://github.com/vios-s/multimodal_segmentation.",['cs.CV']
Global Image Segmentation Process using Machine Learning algorithm & Convolution Neural Network method for Self- Driving Vehicles,"In autonomous Vehicles technology Image segmentation was a major problem in
visual perception. This image segmentation process is mainly used in medical
applications. Here we adopted an image segmentation process to visual
perception tasks for predicting the agents on the surrounding environment,
identifying the road boundaries and tracking the line markings. Main objective
of the paper is to divide the input images using the image segmentation process
and Convolution Neural Network method for efficient results of visual
perception. For Sampling assume a local city data-set samples and validation
process done in Jupyter Notebook using Python language. We proposed this image
segmentation method planning to standard and further the development of
state-of-the art methods for visual inspection system understanding. The
experimental results achieves 73% mean IOU. Our method also achieves 90 FPS
inference speed and using a NVDIA GeForce GTX 1050 GPU.","['cs.CV', 'cs.GT']"
Distribution-aware Margin Calibration for Medical Image Segmentation,"The Jaccard index, also known as Intersection-over-Union (IoU score), is one
of the most critical evaluation metrics in medical image segmentation. However,
directly optimizing the mean IoU (mIoU) score over multiple objective classes
is an open problem. Although some algorithms have been proposed to optimize its
surrogates, there is no guarantee provided for their generalization ability. In
this paper, we present a novel data-distribution-aware margin calibration
method for a better generalization of the mIoU over the whole
data-distribution, underpinned by a rigid lower bound. This scheme ensures a
better segmentation performance in terms of IoU scores in practice. We evaluate
the effectiveness of the proposed margin calibration method on two medical
image segmentation datasets, showing substantial improvements of IoU scores
over other learning schemes using deep segmentation models.",['cs.CV']
Recyclable Waste Identification Using CNN Image Recognition and Gaussian Clustering,"Waste recycling is an important way of saving energy and materials in the
production process. In general cases recyclable objects are mixed with
unrecyclable objects, which raises a need for identification and
classification. This paper proposes a convolutional neural network (CNN) model
to complete both tasks. The model uses transfer learning from a pretrained
Resnet-50 CNN to complete feature extraction. A subsequent fully connected
layer for classification was trained on the augmented TrashNet dataset [1]. In
the application, sliding-window is used for image segmentation in the
pre-classification stage. In the post-classification stage, the labelled sample
points are integrated with Gaussian Clustering to locate the object. The
resulting model has achieved an overall detection rate of 48.4% in simulation
and final classification accuracy of 92.4%.","['cs.CV', 'cs.AI']"
Context-based Image Segment Labeling (CBISL),"Working with images, one often faces problems with incomplete or unclear
information. Image inpainting can be used to restore missing image regions but
focuses, however, on low-level image features such as pixel intensity, pixel
gradient orientation, and color. This paper aims to recover semantic image
features (objects and positions) in images. Based on published gated PixelCNNs,
we demonstrate a new approach referred to as quadro-directional PixelCNN to
recover missing objects and return probable positions for objects based on the
context. We call this approach context-based image segment labeling (CBISL).
The results suggest that our four-directional model outperforms one-directional
models (gated PixelCNN) and returns a human-comparable performance.","['cs.CV', '68T45', 'J.6; J.0']"
Highway Driving Dataset for Semantic Video Segmentation,"Scene understanding is an essential technique in semantic segmentation.
Although there exist several datasets that can be used for semantic
segmentation, they are mainly focused on semantic image segmentation with large
deep neural networks. Therefore, these networks are not useful for real time
applications, especially in autonomous driving systems. In order to solve this
problem, we make two contributions to semantic segmentation task. The first
contribution is that we introduce the semantic video dataset, the Highway
Driving dataset, which is a densely annotated benchmark for a semantic video
segmentation task. The Highway Driving dataset consists of 20 video sequences
having a 30Hz frame rate, and every frame is densely annotated. Secondly, we
propose a baseline algorithm that utilizes a temporal correlation. Together
with our attempt to analyze the temporal correlation, we expect the Highway
Driving dataset to encourage research on semantic video segmentation.",['cs.CV']
Contrastive learning of global and local features for medical image segmentation with limited annotations,"A key requirement for the success of supervised deep learning is a large
labeled dataset - a condition that is difficult to meet in medical image
analysis. Self-supervised learning (SSL) can help in this regard by providing a
strategy to pre-train a neural network with unlabeled data, followed by
fine-tuning for a downstream task with limited annotations. Contrastive
learning, a particular variant of SSL, is a powerful technique for learning
image-level representations. In this work, we propose strategies for extending
the contrastive learning framework for segmentation of volumetric medical
images in the semi-supervised setting with limited annotations, by leveraging
domain-specific and problem-specific cues. Specifically, we propose (1) novel
contrasting strategies that leverage structural similarity across volumetric
medical images (domain-specific cue) and (2) a local version of the contrastive
loss to learn distinctive representations of local regions that are useful for
per-pixel segmentation (problem-specific cue). We carry out an extensive
evaluation on three Magnetic Resonance Imaging (MRI) datasets. In the limited
annotation setting, the proposed method yields substantial improvements
compared to other self-supervision and semi-supervised learning techniques.
When combined with a simple data augmentation technique, the proposed method
reaches within 8% of benchmark performance using only two labeled MRI volumes
for training, corresponding to only 4% (for ACDC) of the training data used to
train the benchmark. The code is made public at
https://github.com/krishnabits001/domain_specific_cl.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-fine Framework and Its Adversarial Examples,"Although deep neural networks have been a dominant method for many 2D vision
tasks, it is still challenging to apply them to 3D tasks, such as medical image
segmentation, due to the limited amount of annotated 3D data and limited
computational resources. In this chapter, by rethinking the strategy to apply
3D Convolutional Neural Networks to segment medical images, we propose a novel
3D-based coarse-to-fine framework to efficiently tackle these challenges. The
proposed 3D-based framework outperforms their 2D counterparts by a large margin
since it can leverage the rich spatial information along all three axes. We
further analyze the threat of adversarial attacks on the proposed framework and
show how to defense against the attack. We conduct experiments on three
datasets, the NIH pancreas dataset, the JHMI pancreas dataset and the JHMI
pathological cyst dataset, where the first two and the last one contain healthy
and pathological pancreases respectively, and achieve the current
state-of-the-art in terms of Dice-Sorensen Coefficient (DSC) on all of them.
Especially, on the NIH pancreas segmentation dataset, we outperform the
previous best by an average of over $2\%$, and the worst case is improved by
$7\%$ to reach almost $70\%$, which indicates the reliability of our framework
in clinical applications.",['cs.CV']
Disentangling Human Error from the Ground Truth in Segmentation of Medical Images,"Recent years have seen increasing use of supervised learning methods for
segmentation tasks. However, the predictive performance of these algorithms
depends on the quality of labels. This problem is particularly pertinent in the
medical image domain, where both the annotation cost and inter-observer
variability are high. In a typical label acquisition process, different human
experts provide their estimates of the ""true"" segmentation labels under the
influence of their own biases and competence levels. Treating these noisy
labels blindly as the ground truth limits the performance that automatic
segmentation algorithms can achieve. In this work, we present a method for
jointly learning, from purely noisy observations alone, the reliability of
individual annotators and the true segmentation label distributions, using two
coupled CNNs. The separation of the two is achieved by encouraging the
estimated annotators to be maximally unreliable while achieving high fidelity
with the noisy training data. We first define a toy segmentation dataset based
on MNIST and study the properties of the proposed algorithm. We then
demonstrate the utility of the method on three public medical imaging
segmentation datasets with simulated (when necessary) and real diverse
annotations: 1) MSLSC (multiple-sclerosis lesions); 2) BraTS (brain tumours);
3) LIDC-IDRI (lung abnormalities). In all cases, our method outperforms
competing methods and relevant baselines particularly in cases where the number
of annotations is small and the amount of disagreement is large. The
experiments also show strong ability to capture the complex spatial
characteristics of annotators' mistakes.","['cs.CV', 'cs.LG']"
A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision,"Standard segmentation of medical images based on full-supervised
convolutional networks demands accurate dense annotations. Such learning
framework is built on laborious manual annotation with restrict demands for
expertise, leading to insufficient high-quality labels. To overcome such
limitation and exploit massive weakly labeled data, we relaxed the rigid
labeling requirement and developed a semi-supervised learning framework based
on a teacher-student fashion for organ and lesion segmentation with partial
dense-labeled supervision and supplementary loose bounding-box supervision
which are easier to acquire. Observing the geometrical relation of an organ and
its inner lesions in most cases, we propose a hierarchical organ-to-lesion
(O2L) attention module in a teacher segmentor to produce pseudo-labels. Then a
student segmentor is trained with combinations of manual-labeled and
pseudo-labeled annotations. We further proposed a localization branch realized
via an aggregation of high-level features in a deep decoder to predict
locations of organ and lesion, which enriches student segmentor with precise
localization information. We validated each design in our model on LiTS
challenge datasets by ablation study and showed its state-of-the-art
performance compared with recent methods. We show our model is robust to the
quality of bounding box and achieves comparable performance compared with
full-supervised learning methods.",['cs.CV']
What is Wrong with Continual Learning in Medical Image Segmentation?,"Continual learning protocols are attracting increasing attention from the
medical imaging community. In a continual setup, data from different sources
arrives sequentially and each batch is only available for a limited period.
Given the inherent privacy risks associated with medical data, this setup
reflects the reality of deployment for deep learning diagnostic radiology
systems. Many techniques exist to learn continuously for classification tasks,
and several have been adapted to semantic segmentation. Yet most have at least
one of the following flaws: a) they rely too heavily on domain identity
information during inference, or b) data as seen in early training stages does
not profit from training with later data. In this work, we propose an
evaluation framework that addresses both concerns, and introduce a fair
multi-model benchmark. We show that the benchmark outperforms two popular
continual learning methods for the task of T2-weighted MR prostate
segmentation.","['cs.CV', 'cs.LG']"
Color Image Segmentation Metrics,"An automatic image segmentation procedure is an inevitable part of many image
analyses and computer vision which deeply affect the rest of the system;
therefore, a set of interactive segmentation evaluation methods can
substantially simplify the system development process. This entry presents the
state of the art of quantitative evaluation metrics for color image
segmentation methods by performing an analytical and comparative review of the
measures. The decision-making process in selecting a suitable evaluation metric
is still very serious because each metric tends to favor a different
segmentation method for each benchmark dataset. Furthermore, a conceptual
comparison of these metrics is provided at a high level of abstraction and is
discussed for understanding the quantitative changes in different image
segmentation results.","['cs.CV', 'cs.LG', 'cs.MM', 'eess.IV', 'I.4.6; I.2.10; I.5.0; I.3.0; E.0']"
Gastric histopathology image segmentation using a hierarchical conditional random field,"For the Convolutional Neural Networks (CNNs) applied in the intelligent
diagnosis of gastric cancer, existing methods mostly focus on individual
characteristics or network frameworks without a policy to depict the integral
information. Mainly, Conditional Random Field (CRF), an efficient and stable
algorithm for analyzing images containing complicated contents, can
characterize spatial relation in images. In this paper, a novel Hierarchical
Conditional Random Field (HCRF) based Gastric Histopathology Image Segmentation
(GHIS) method is proposed, which can automatically localize abnormal (cancer)
regions in gastric histopathology images obtained by an optical microscope to
assist histopathologists in medical work. This HCRF model is built up with
higher order potentials, including pixel-level and patch-level potentials, and
graph-based post-processing is applied to further improve its segmentation
performance. Especially, a CNN is trained to build up the pixel-level
potentials and another three CNNs are fine-tuned to build up the patch-level
potentials for sufficient spatial segmentation information. In the experiment,
a hematoxylin and eosin (H&E) stained gastric histopathological dataset with
560 abnormal images are divided into training, validation and test sets with a
ratio of 1 : 1 : 2. Finally, segmentation accuracy, recall and specificity of
78.91%, 65.59%, and 81.33% are achieved on the test set. Our HCRF model
demonstrates high segmentation performance and shows its effectiveness and
future potential in the GHIS field.","['cs.CV', 'cs.LG']"
Weakly-supervised Learning For Catheter Segmentation in 3D Frustum Ultrasound,"Accurate and efficient catheter segmentation in 3D ultrasound (US) is
essential for cardiac intervention. Currently, the state-of-the-art
segmentation algorithms are based on convolutional neural networks (CNNs),
which achieved remarkable performances in a standard Cartesian volumetric data.
Nevertheless, these approaches suffer the challenges of low efficiency and GPU
unfriendly image size. Therefore, such difficulties and expensive hardware
requirements become a bottleneck to build accurate and efficient segmentation
models for real clinical application. In this paper, we propose a novel Frustum
ultrasound based catheter segmentation method. Specifically, Frustum ultrasound
is a polar coordinate based image, which includes same information of standard
Cartesian image but has much smaller size, which overcomes the bottleneck of
efficiency than conventional Cartesian images. Nevertheless, the irregular and
deformed Frustum images lead to more efforts for accurate voxel-level
annotation. To address this limitation, a weakly supervised learning framework
is proposed, which only needs 3D bounding box annotations overlaying the
region-of-interest to training the CNNs. Although the bounding box annotation
includes noise and inaccurate annotation to mislead to model, it is addressed
by the proposed pseudo label generated scheme. The labels of training voxels
are generated by incorporating class activation maps with line filtering, which
is iteratively updated during the training. Our experimental results show the
proposed method achieved the state-of-the-art performance with an efficiency of
0.25 second per volume. More crucially, the Frustum image segmentation provides
a much faster and cheaper solution for segmentation in 3D US image, which meet
the demands of clinical applications.",['cs.CV']
Human Perception-based Evaluation Criterion for Ultra-high Resolution Cell Membrane Segmentation,"Computer vision technology is widely used in biological and medical data
analysis and understanding. However, there are still two major bottlenecks in
the field of cell membrane segmentation, which seriously hinder further
research: lack of sufficient high-quality data and lack of suitable evaluation
criteria. In order to solve these two problems, this paper first proposes an
Ultra-high Resolution Image Segmentation dataset for the Cell membrane, called
U-RISC, the largest annotated Electron Microscopy (EM) dataset for the Cell
membrane with multiple iterative annotations and uncompressed high-resolution
raw data. During the analysis process of the U-RISC, we found that the current
popular segmentation evaluation criteria are inconsistent with human
perception. This interesting phenomenon is confirmed by a subjective experiment
involving twenty people. Furthermore, to resolve this inconsistency, we propose
a new evaluation criterion called Perceptual Hausdorff Distance (PHD) to
measure the quality of cell membrane segmentation results. Detailed performance
comparison and discussion of classic segmentation methods along with two
iterative manual annotation results under existing evaluation criteria and PHD
is given.",['cs.CV']
HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network,"This paper addresses representational block named Hierarchical-Split Block,
which can be taken as a plug-and-play block to upgrade existing convolutional
neural networks, improves model performance significantly in a network.
Hierarchical-Split Block contains many hierarchical split and concatenate
connections within one single residual block. We find multi-scale features is
of great importance for numerous vision tasks. Moreover, Hierarchical-Split
block is very flexible and efficient, which provides a large space of potential
network architectures for different applications. In this work, we present a
common backbone based on Hierarchical-Split block for tasks: image
classification, object detection, instance segmentation and semantic image
segmentation/parsing. Our approach shows significant improvements over all
these core tasks in comparison with the baseline. As shown in Figure1, for
image classification, our 50-layers network(HS-ResNet50) achieves 81.28% top-1
accuracy with competitive latency on ImageNet-1k dataset. It also outperforms
most state-of-the-art models. The source code and models will be available on:
https://github.com/PaddlePaddle/PaddleClas",['cs.CV']
PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training,"Road network and building footprint extraction is essential for many
applications such as updating maps, traffic regulations, city planning,
ride-hailing, disaster response \textit{etc}. Mapping road networks is
currently both expensive and labor-intensive. Recently, improvements in image
segmentation through the application of deep neural networks has shown
promising results in extracting road segments from large scale, high resolution
satellite imagery. However, significant challenges remain due to lack of enough
labeled training data needed to build models for industry grade applications.
In this paper, we propose a two-stage transfer learning technique to improve
robustness of semantic segmentation for satellite images that leverages noisy
pseudo ground truth masks obtained automatically (without human labor) from
crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid
Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation
that uses focal loss, poly learning rate, and context module. We demonstrate
the strengths of our approach through evaluations done on three popular
datasets over two tasks, namely, road extraction and building foot-print
detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building
footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet
and DeepGlobe road extraction dataset, respectively.","['cs.CV', 'cs.LG', 'eess.IV']"
DoFE: Domain-oriented Feature Embedding for Generalizable Fundus Image Segmentation on Unseen Datasets,"Deep convolutional neural networks have significantly boosted the performance
of fundus image segmentation when test datasets have the same distribution as
the training datasets. However, in clinical practice, medical images often
exhibit variations in appearance for various reasons, e.g., different scanner
vendors and image quality. These distribution discrepancies could lead the deep
networks to over-fit on the training datasets and lack generalization ability
on the unseen test datasets. To alleviate this issue, we present a novel
Domain-oriented Feature Embedding (DoFE) framework to improve the
generalization ability of CNNs on unseen target domains by exploring the
knowledge from multiple source domains. Our DoFE framework dynamically enriches
the image features with additional domain prior knowledge learned from
multi-source domains to make the semantic features more discriminative.
Specifically, we introduce a Domain Knowledge Pool to learn and memorize the
prior information extracted from multi-source domains. Then the original image
features are augmented with domain-oriented aggregated features, which are
induced from the knowledge pool based on the similarity between the input image
and multi-source domain images. We further design a novel domain code
prediction branch to infer this similarity and employ an attention-guided
mechanism to dynamically combine the aggregated features with the semantic
features. We comprehensively evaluate our DoFE framework on two fundus image
segmentation tasks, including the optic cup and disc segmentation and vessel
segmentation. Our DoFE framework generates satisfying segmentation results on
unseen datasets and surpasses other domain generalization and network
regularization methods.",['cs.CV']
Self-Supervision with Superpixels: Training Few-shot Medical Image Segmentation without Annotation,"Few-shot semantic segmentation (FSS) has great potential for medical imaging
applications. Most of the existing FSS techniques require abundant annotated
semantic classes for training. However, these methods may not be applicable for
medical images due to the lack of annotations. To address this problem we make
several contributions: (1) A novel self-supervised FSS framework for medical
images in order to eliminate the requirement for annotations during training.
Additionally, superpixel-based pseudo-labels are generated to provide
supervision; (2) An adaptive local prototype pooling module plugged into
prototypical networks, to solve the common challenging foreground-background
imbalance problem in medical image segmentation; (3) We demonstrate the general
applicability of the proposed approach for medical images using three different
tasks: abdominal organ segmentation for CT and MRI, as well as cardiac
segmentation for MRI. Our results show that, for medical image segmentation,
the proposed method outperforms conventional FSS methods which require manual
annotations for training.",['cs.CV']
MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps,"We present a novel region based active learning method for semantic image
segmentation, called MetaBox+. For acquisition, we train a meta regression
model to estimate the segment-wise Intersection over Union (IoU) of each
predicted segment of unlabeled images. This can be understood as an estimation
of segment-wise prediction quality. Queried regions are supposed to minimize to
competing targets, i.e., low predicted IoU values / segmentation quality and
low estimated annotation costs. For estimating the latter we propose a simple
but practical method for annotation cost estimation. We compare our method to
entropy based methods, where we consider the entropy as uncertainty of the
prediction. The comparison and analysis of the results provide insights into
annotation costs as well as robustness and variance of the methods. Numerical
experiments conducted with two different networks on the Cityscapes dataset
clearly demonstrate a reduction of annotation effort compared to random
acquisition. Noteworthily, we achieve 95%of the mean Intersection over Union
(mIoU), using MetaBox+ compared to when training with the full dataset, with
only 10.47% / 32.01% annotation effort for the two networks, respectively.",['cs.CV']
Linguistic Structure Guided Context Modeling for Referring Image Segmentation,"Referring image segmentation aims to predict the foreground mask of the
object referred by a natural language sentence. Multimodal context of the
sentence is crucial to distinguish the referent from the background. Existing
methods either insufficiently or redundantly model the multimodal context. To
tackle this problem, we propose a ""gather-propagate-distribute"" scheme to model
multimodal context by cross-modal interaction and implement this scheme as a
novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM
module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which
guides all the words to include valid multimodal context of the sentence while
excluding disturbing ones through three steps over the multimodal feature,
i.e., gathering, constrained propagation and distributing. Extensive
experiments on four benchmarks demonstrate that our method outperforms all the
previous state-of-the-arts.","['cs.CV', 'cs.CL']"
Depth-wise layering of 3d images using dense depth maps: a threshold based approach,"Image segmentation has long been a basic problem in computer vision.
Depth-wise Layering is a kind of segmentation that slices an image in a
depth-wise sequence unlike the conventional image segmentation problems dealing
with surface-wise decomposition. The proposed Depth-wise Layering technique
uses a single depth image of a static scene to slice it into multiple layers.
The technique employs a thresholding approach to segment rows of the dense
depth map into smaller partitions called Line-Segments in this paper. Then, it
uses the line-segment labelling method to identify number of objects and layers
of the scene independently. The final stage is to link objects of the scene to
their respective object-layers. We evaluate the efficiency of the proposed
technique by applying that on many images along with their dense depth maps.
The experiments have shown promising results of layering.","['cs.CV', 'eess.IV']"
Realistic Image Normalization for Multi-Domain Segmentation,"Image normalization is a building block in medical image analysis.
Conventional approaches are customarily utilized on a per-dataset basis. This
strategy, however, prevents the current normalization algorithms from fully
exploiting the complex joint information available across multiple datasets.
Consequently, ignoring such joint information has a direct impact on the
performance of segmentation algorithms. This paper proposes to revisit the
conventional image normalization approach by instead learning a common
normalizing function across multiple datasets. Jointly normalizing multiple
datasets is shown to yield consistent normalized images as well as an improved
image segmentation. To do so, a fully automated adversarial and task-driven
normalization approach is employed as it facilitates the training of realistic
and interpretable images while keeping performance on-par with the
state-of-the-art. The adversarial training of our network aims at finding the
optimal transfer function to improve both the segmentation accuracy and the
generation of realistic images. We evaluated the performance of our normalizer
on both infant and adult brains images from the iSEG, MRBrainS and ABIDE
datasets. Results reveal the potential of our normalization approach for
segmentation, with Dice improvements of up to 57.5% over our baseline. Our
method can also enhance data availability by increasing the number of samples
available when learning from multiple imaging domains.","['cs.LG', 'eess.IV', 'stat.ML']"
Deep Learning for Earth Image Segmentation based on Imperfect Polyline Labels with Annotation Errors,"In recent years, deep learning techniques (e.g., U-Net, DeepLab) have
achieved tremendous success in image segmentation. The performance of these
models heavily relies on high-quality ground truth segment labels.
Unfortunately, in many real-world problems, ground truth segment labels often
have geometric annotation errors due to manual annotation mistakes, GPS errors,
or visually interpreting background imagery at a coarse resolution. Such
location errors will significantly impact the training performance of existing
deep learning algorithms. Existing research on label errors either models
ground truth errors in label semantics (assuming label locations to be correct)
or models label location errors with simple square patch shifting. These
methods cannot fully incorporate the geometric properties of label location
errors. To fill the gap, this paper proposes a generic learning framework based
on the EM algorithm to update deep learning model parameters and infer hidden
true label locations simultaneously. Evaluations on a real-world hydrological
dataset in the streamline refinement application show that the proposed
framework outperforms baseline methods in classification accuracy (reducing the
number of false positives by 67% and reducing the number of false negatives by
55%).","['cs.CV', 'cs.LG', 'eess.IV']"
Referring Image Segmentation via Cross-Modal Progressive Comprehension,"Referring image segmentation aims at segmenting the foreground masks of the
entities that can well match the description given in the natural language
expression. Previous approaches tackle this problem using implicit feature
interaction and fusion between visual and linguistic modalities, but usually
fail to explore informative words of the expression to well align features from
the two modalities for accurately identifying the referred entity. In this
paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a
Text-Guided Feature Exchange (TGFE) module to effectively address the
challenging task. Concretely, the CMPC module first employs entity and
attribute words to perceive all the related entities that might be considered
by the expression. Then, the relational words are adopted to highlight the
correct entity as well as suppress other irrelevant ones by multimodal graph
reasoning. In addition to the CMPC module, we further leverage a simple yet
effective TGFE module to integrate the reasoned multimodal features from
different levels with the guidance of textual information. In this way,
features from multi-levels could communicate with each other and be refined
based on the textual context. We conduct extensive experiments on four popular
referring segmentation benchmarks and achieve new state-of-the-art
performances.","['cs.CV', 'cs.CL']"
RefVOS: A Closer Look at Referring Expressions for Video Object Segmentation,"The task of video object segmentation with referring expressions
(language-guided VOS) is to, given a linguistic phrase and a video, generate
binary masks for the object to which the phrase refers. Our work argues that
existing benchmarks used for this task are mainly composed of trivial cases, in
which referents can be identified with simple phrases. Our analysis relies on a
new categorization of the phrases in the DAVIS-2017 and Actor-Action datasets
into trivial and non-trivial REs, with the non-trivial REs annotated with seven
RE semantic categories. We leverage this data to analyze the results of RefVOS,
a novel neural network that obtains competitive results for the task of
language-guided image segmentation and state of the art results for
language-guided VOS. Our study indicates that the major challenges for the task
are related to understanding motion and static actions.",['cs.CV']
Semantic Segmentation With Multi Scale Spatial Attention For Self Driving Cars,"In this paper, we present a novel neural network using multi scale feature
fusion at various scales for accurate and efficient semantic image
segmentation. We used ResNet based feature extractor, dilated convolutional
layers in downsampling part, atrous convolutional layers in the upsampling part
and used concat operation to merge them. A new attention module is proposed to
encode more contextual information and enhance the receptive field of the
network. We present an in depth theoretical analysis of our network with
training and optimization details. Our network was trained and tested on the
Camvid dataset and Cityscapes dataset using mean accuracy per class and
Intersection Over Union (IOU) as the evaluation metrics. Our model outperforms
previous state of the art methods on semantic segmentation achieving mean IOU
value of 74.12 while running at >100 FPS.","['cs.CV', 'cs.LG']"
Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images,"Efficient and easy segmentation of images and volumes is of great practical
importance. Segmentation problems that motivate our approach originate from
microscopy imaging commonly used in materials science, medicine, and biology.
We formulate image segmentation as a probabilistic pixel classification
problem, and we apply segmentation as a step towards characterising image
content. Our method allows the user to define structures of interest by
interactively marking a subset of pixels. Thanks to the real-time feedback, the
user can place new markings strategically, depending on the current outcome.
The final pixel classification may be obtained from a very modest user input.
An important ingredient of our method is a graph that encodes image content.
This graph is built in an unsupervised manner during initialisation and is
based on clustering of image features. Since we combine a limited amount of
user-labelled data with the clustering information obtained from the unlabelled
parts of the image, our method fits in the general framework of semi-supervised
learning. We demonstrate how this can be a very efficient approach to
segmentation through pixel classification.",['cs.CV']
A Survey on Deep Learning Methods for Semantic Image Segmentation in Real-Time,"Semantic image segmentation is one of fastest growing areas in computer
vision with a variety of applications. In many areas, such as robotics and
autonomous vehicles, semantic image segmentation is crucial, since it provides
the necessary context for actions to be taken based on a scene understanding at
the pixel level. Moreover, the success of medical diagnosis and treatment
relies on the extremely accurate understanding of the data under consideration
and semantic image segmentation is one of the important tools in many cases.
Recent developments in deep learning have provided a host of tools to tackle
this problem efficiently and with increased accuracy. This work provides a
comprehensive analysis of state-of-the-art deep learning architectures in image
segmentation and, more importantly, an extensive list of techniques to achieve
fast inference and computational efficiency. The origins of these techniques as
well as their strengths and trade-offs are discussed with an in-depth analysis
of their impact in the area. The best-performing architectures are summarized
with a list of methods used to achieve these state-of-the-art results.","['cs.CV', 'stat.ML']"
Going to Extremes: Weakly Supervised Medical Image Segmentation,"Medical image annotation is a major hurdle for developing precise and robust
machine learning models. Annotation is expensive, time-consuming, and often
requires expert knowledge, particularly in the medical field. Here, we suggest
using minimal user interaction in the form of extreme point clicks to train a
segmentation model which, in effect, can be used to speed up medical image
annotation. An initial segmentation is generated based on the extreme points
utilizing the random walker algorithm. This initial segmentation is then used
as a noisy supervision signal to train a fully convolutional network that can
segment the organ of interest, based on the provided user clicks. Through
experimentation on several medical imaging datasets, we show that the
predictions of the network can be refined using several rounds of training with
the prediction from the same weakly annotated data. Further improvements are
shown utilizing the clicked points within a custom-designed loss and attention
mechanism. Our approach has the potential to speed up the process of generating
new training datasets for the development of new machine learning and deep
learning-based models for, but not exclusively, medical image analysis.",['cs.CV']
Interactive Learning for Semantic Segmentation in Earth Observation,"Dense pixel-wise classification maps output by deep neural networks are of
extreme importance for scene understanding. However, these maps are often
partially inaccurate due to a variety of possible factors. Therefore, we
propose to interactively refine them within a framework named DISCA (Deep Image
Segmentation with Continual Adaptation). It consists of continually adapting a
neural network to a target image using an interactive learning process with
sparse user annotations as ground-truth. We show through experiments on three
datasets using synthesized annotations the benefits of the approach, reaching
an IoU improvement up to 4.7% for ten sampled clicks. Finally, we exhibit that
our approach can be particularly rewarding when it is faced to additional
issues such as domain adaptation.",['cs.CV']
Segmentations-Leak: Membership Inference Attacks and Defenses in Semantic Image Segmentation,"Today's success of state of the art methods for semantic segmentation is
driven by large datasets. Data is considered an important asset that needs to
be protected, as the collection and annotation of such datasets comes at
significant efforts and associated costs. In addition, visual data might
contain private or sensitive information, that makes it equally unsuited for
public release. Unfortunately, recent work on membership inference in the
broader area of adversarial machine learning and inference attacks on machine
learning models has shown that even black box classifiers leak information on
the dataset that they were trained on. We show that such membership inference
attacks can be successfully carried out on complex, state of the art models for
semantic segmentation. In order to mitigate the associated risks, we also study
a series of defenses against such membership inference attacks and find
effective counter measures against the existing risks with little effect on the
utility of the segmentation method. Finally, we extensively evaluate our
attacks and defenses on a range of relevant real-world datasets: Cityscapes,
BDD100K, and Mapillary Vistas.",['cs.CV']
A Topological Loss Function for Deep-Learning based Image Segmentation using Persistent Homology,"We introduce a method for training neural networks to perform image or volume
segmentation in which prior knowledge about the topology of the segmented
object can be explicitly provided and then incorporated into the training
process. By using the differentiable properties of persistent homology, a
concept used in topological data analysis, we can specify the desired topology
of segmented objects in terms of their Betti numbers and then drive the
proposed segmentations to contain the specified topological features.
Importantly this process does not require any ground-truth labels, just prior
knowledge of the topology of the structure being segmented. We demonstrate our
approach in three experiments. Firstly we create a synthetic task in which
handwritten MNIST digits are de-noised, and show that using this kind of
topological prior knowledge in the training of the network significantly
improves the quality of the de-noised digits. Secondly we perform an experiment
in which the task is segmenting the myocardium of the left ventricle from
cardiac magnetic resonance images. We show that the incorporation of the prior
knowledge of the topology of this anatomy improves the resulting segmentations
in terms of both the topological accuracy and the Dice coefficient. Thirdly, we
extend the method to 3D volumes and demonstrate its performance on the task of
segmenting the placenta from ultrasound data, again showing that incorporating
topological priors improves performance on this challenging task. We find that
embedding explicit prior knowledge in neural network segmentation tasks is most
beneficial when the segmentation task is especially challenging and that it can
be used in either a semi-supervised or post-processing context to extract a
useful training gradient from images without pixelwise labels.","['cs.CV', 'eess.IV']"
UXNet: Searching Multi-level Feature Aggregation for 3D Medical Image Segmentation,"Aggregating multi-level feature representation plays a critical role in
achieving robust volumetric medical image segmentation, which is important for
the auxiliary diagnosis and treatment. Unlike the recent neural architecture
search (NAS) methods that typically searched the optimal operators in each
network layer, but missed a good strategy to search for feature aggregations,
this paper proposes a novel NAS method for 3D medical image segmentation, named
UXNet, which searches both the scale-wise feature aggregation strategies as
well as the block-wise operators in the encoder-decoder network. UXNet has
several appealing benefits. (1) It significantly improves flexibility of the
classical UNet architecture, which only aggregates feature representations of
encoder and decoder in equivalent resolution. (2) A continuous relaxation of
UXNet is carefully designed, enabling its searching scheme performed in an
efficient differentiable manner. (3) Extensive experiments demonstrate the
effectiveness of UXNet compared with recent NAS methods for medical image
segmentation. The architecture discovered by UXNet outperforms existing
state-of-the-art models in terms of Dice on several public 3D medical image
segmentation benchmarks, especially for the boundary locations and tiny
tissues. The searching computational complexity of UXNet is cheap, enabling to
search a network with the best performance less than 1.5 days on two TitanXP
GPUs.",['cs.CV']
LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation,"Deep Learning (DL) models are becoming larger, because the increase in model
size might offer significant accuracy gain. To enable the training of large
deep networks, data parallelism and model parallelism are two well-known
approaches for parallel training. However, data parallelism does not help
reduce memory footprint per device. In this work, we introduce Large deep 3D
ConvNets with Automated Model Parallelism (LAMP) and investigate the impact of
both input's and deep 3D ConvNets' size on segmentation accuracy. Through
automated model parallelism, it is feasible to train large deep 3D ConvNets
with a large input patch, even the whole image. Extensive experiments
demonstrate that, facilitated by the automated model parallelism, the
segmentation accuracy can be improved through increasing model size and input
context size, and large input yields significant inference speedup compared
with sliding window of small patches in the inference. Code is
available\footnote{https://monai.io/research/lamp-automated-model-parallelism}.","['cs.CV', 'cs.DC', 'cs.LG', 'cs.NE', 'eess.IV']"
Finite Group Equivariant Neural Networks for Games,"Games such as go, chess and checkers have multiple equivalent game states,
i.e. multiple board positions where symmetrical and opposite moves should be
made. These equivalences are not exploited by current state of the art neural
agents which instead must relearn similar information, thereby wasting
computing time. Group equivariant CNNs in existing work create networks which
can exploit symmetries to improve learning, however, they lack the
expressiveness to correctly reflect the move embeddings necessary for games. We
introduce Finite Group Neural Networks (FGNNs), a method for creating agents
with an innate understanding of these board positions. FGNNs are shown to
improve the performance of networks playing checkers (draughts), and can be
easily adapted to other games and learning problems. Additionally, FGNNs can be
created from existing network architectures. These include, for the first time,
those with skip connections and arbitrary layer types. We demonstrate that an
equivariant version of U-Net (FGNN-U-Net) outperforms the unmodified network in
image segmentation.","['cs.LG', 'stat.ML']"
Learning to segment microscopy images with lazy labels,"The need for labour intensive pixel-wise annotation is a major limitation of
many fully supervised learning methods for segmenting bioimages that can
contain numerous object instances with thin separations. In this paper, we
introduce a deep convolutional neural network for microscopy image
segmentation. Annotation issues are circumvented by letting the network being
trainable on coarse labels combined with only a very small number of images
with pixel-wise annotations. We call this new labelling strategy `lazy' labels.
Image segmentation is stratified into three connected tasks: rough inner region
detection, object separation and pixel-wise segmentation. These tasks are
learned in an end-to-end multi-task learning framework. The method is
demonstrated on two microscopy datasets, where we show that the model gives
accurate segmentation results even if exact boundary labels are missing for a
majority of annotated data. It brings more flexibility and efficiency for
training deep neural networks that are data hungry and is applicable to
biomedical images with poor contrast at the object boundaries or with diverse
textures and repeated patterns.","['cs.CV', 'cs.LG', 'stat.ML']"
Random Style Transfer based Domain Generalization Networks Integrating Shape and Spatial Information,"Deep learning (DL)-based models have demonstrated good performance in medical
image segmentation. However, the models trained on a known dataset often fail
when performed on an unseen dataset collected from different centers, vendors
and disease populations. In this work, we present a random style transfer
network to tackle the domain generalization problem for multi-vendor and center
cardiac image segmentation. Style transfer is used to generate training data
with a wider distribution/ heterogeneity, namely domain augmentation. As the
target domain could be unknown, we randomly generate a modality vector for the
target modality in the style transfer stage, to simulate the domain shift for
unknown domains. The model can be trained in a semi-supervised manner by
simultaneously optimizing a supervised segmentation and an unsupervised style
translation objective. Besides, the framework incorporates the spatial
information and shape prior of the target by introducing two regularization
terms. We evaluated the proposed framework on 40 subjects from the M\&Ms
challenge2020, and obtained promising performance in the segmentation for data
from unknown vendors and centers.","['cs.CV', 'eess.IV']"
Contour Sparse Representation with SDD Features for Object Recognition,"Slope difference distribution (SDD) is computed for the one-dimensional
curve. It is not only robust to calculate the partitioning point to separate
the curve logically, but also robust to calculate the clustering center of each
part of the separated curve. SDD has been proposed for image segmentation and
it outperforms all existing image segmentation methods. For verification
purpose, we have made the Matlab codes of comparing SDD method with existing
image segmentation methods freely available at Matlab Central. The contour of
the object is similar to the histogram of the image. Thus, feature detection by
SDD from the contour of the object is also feasible. In this letter, SDD
features are defined and they form the sparse representation of the object
contour. The reference model of each object is built based on the SDD features
and then model matching is used for on line object recognition. The
experimental results are very encouraging. For the gesture recognition, SDD
achieved 100% accuracy for two public datasets: the NUS dataset and the
near-infrared dataset. For the object recognition, SDD achieved 100% accuracy
for the Kimia 99 dataset.","['cs.CV', 'cs.LG', 'eess.IV']"
Deep Neural Network for 3D Surface Segmentation based on Contour Tree Hierarchy,"Given a 3D surface defined by an elevation function on a 2D grid as well as
non-spatial features observed at each pixel, the problem of surface
segmentation aims to classify pixels into contiguous classes based on both
non-spatial features and surface topology. The problem has important
applications in hydrology, planetary science, and biochemistry but is uniquely
challenging for several reasons. First, the spatial extent of class segments
follows surface contours in the topological space, regardless of their spatial
shapes and directions. Second, the topological structure exists in multiple
spatial scales based on different surface resolutions. Existing widely
successful deep learning models for image segmentation are often not applicable
due to their reliance on convolution and pooling operations to learn regular
structural patterns on a grid. In contrast, we propose to represent surface
topological structure by a contour tree skeleton, which is a polytree capturing
the evolution of surface contours at different elevation levels. We further
design a graph neural network based on the contour tree hierarchy to model
surface topological structure at different spatial scales. Experimental
evaluations based on real-world hydrological datasets show that our model
outperforms several baseline methods in classification accuracy.","['cs.CV', 'cs.LG']"
Mask-guided sample selection for Semi-Supervised Instance Segmentation,"Image segmentation methods are usually trained with pixel-level annotations,
which require significant human effort to collect. The most common solution to
address this constraint is to implement weakly-supervised pipelines trained
with lower forms of supervision, such as bounding boxes or scribbles. Another
option are semi-supervised methods, which leverage a large amount of unlabeled
data and a limited number of strongly-labeled samples. In this second setup,
samples to be strongly-annotated can be selected randomly or with an active
learning mechanism that chooses the ones that will maximize the model
performance. In this work, we propose a sample selection approach to decide
which samples to annotate for semi-supervised instance segmentation. Our method
consists in first predicting pseudo-masks for the unlabeled pool of samples,
together with a score predicting the quality of the mask. This score is an
estimate of the Intersection Over Union (IoU) of the segment with the ground
truth mask. We study which samples are better to annotate given the quality
score, and show how our approach outperforms a random selection, leading to
improved performance for semi-supervised instance segmentation with low
annotation budgets.",['cs.CV']
MCU-Net: A framework towards uncertainty representations for decision support system patient referrals in healthcare contexts,"Incorporating a human-in-the-loop system when deploying automated decision
support is critical in healthcare contexts to create trust, as well as provide
reliable performance on a patient-to-patient basis. Deep learning methods while
having high performance, do not allow for this patient-centered approach due to
the lack of uncertainty representation. Thus, we present a framework of
uncertainty representation evaluated for medical image segmentation, using
MCU-Net which combines a U-Net with Monte Carlo Dropout, evaluated with four
different uncertainty metrics. The framework augments this by adding a
human-in-the-loop aspect based on an uncertainty threshold for automated
referral of uncertain cases to a medical professional. We demonstrate that
MCU-Net combined with epistemic uncertainty and an uncertainty threshold tuned
for this application maximizes automated performance on an individual patient
level, yet refers truly uncertain cases. This is a step towards uncertainty
representations when deploying machine learning based decision support in
healthcare settings.","['cs.LG', 'cs.CV', 'stat.ML']"
LULC Segmentation of RGB Satellite Image Using FCN-8,"This work presents use of Fully Convolutional Network (FCN-8) for semantic
segmentation of high-resolution RGB earth surface satel-lite images into land
use land cover (LULC) categories. Specically, we propose a non-overlapping
grid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16
weights to segment satellite im-ages into four (forest, built-up, farmland and
water) classes. The FCN-8 semantically projects the discriminating features in
lower resolution learned by the encoder onto the pixel space in higher
resolution to get a dense classi cation. We experimented the proposed system
with Gaofen-2 image dataset, that contains 150 images of over 60 di erent
cities in china. For comparison, we used available ground-truth along with
images segmented using a widely used commeriial GIS software called
eCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8
obtains signi cantly improved performance, than the eCognition soft-ware. Our
model achieves average accuracy of 91.0% and average Inter-section over Union
(IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is
0.60. This paper also reports a detail analysis of errors occurred at the LULC
boundary.","['cs.CV', 'cs.LG', 'eess.IV']"
Universal Semantic Segmentation for Fisheye Urban Driving Images,"Semantic segmentation is a critical method in the field of autonomous
driving. When performing semantic image segmentation, a wider field of view
(FoV) helps to obtain more information about the surrounding environment,
making automatic driving safer and more reliable, which could be offered by
fisheye cameras. However, large public fisheye datasets are not available, and
the fisheye images captured by the fisheye camera with large FoV comes with
large distortion, so commonly-used semantic segmentation model cannot be
directly utilized. In this paper, a seven degrees of freedom (DoF) augmentation
method is proposed to transform rectilinear image to fisheye image in a more
comprehensive way. In the training process, rectilinear images are transformed
into fisheye images in seven DoF, which simulates the fisheye images taken by
cameras of different positions, orientations and focal lengths. The result
shows that training with the seven-DoF augmentation can improve the model's
accuracy and robustness against different distorted fisheye data. This
seven-DoF augmentation provides a universal semantic segmentation solution for
fisheye cameras in different autonomous driving applications. Also, we provide
specific parameter settings of the augmentation for autonomous driving. At
last, we tested our universal semantic segmentation model on real fisheye
images and obtained satisfactory results. The code and configurations are
released at https://github.com/Yaozhuwa/FisheyeSeg.","['cs.CV', 'cs.LG', 'stat.ML']"
DISIR: Deep Image Segmentation with Interactive Refinement,"This paper presents an interactive approach for multi-class segmentation of
aerial images. Precisely, it is based on a deep neural network which exploits
both RGB images and annotations. Starting from an initial output based on the
image only, our network then interactively refines this segmentation map using
a concatenation of the image and user annotations. Importantly, user
annotations modify the inputs of the network - not its weights - enabling a
fast and smooth process. Through experiments on two public aerial datasets, we
show that user annotations are extremely rewarding: each click corrects roughly
5000 pixels. We analyze the impact of different aspects of our framework such
as the representation of the annotations, the volume of training data or the
network architecture. Code is available at https://github.com/delair-ai/DISIR.","['cs.CV', 'cs.HC']"
PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data,"The 3D volumetric shape of the heart's left ventricle (LV) myocardium (MYO)
wall provides important information for diagnosis of cardiac disease and
invasive procedure navigation. Many cardiac image segmentation methods have
relied on detection of region-of-interest as a pre-requisite for shape
segmentation and modeling. With segmentation results, a 3D surface mesh and a
corresponding point cloud of the segmented cardiac volume can be reconstructed
for further analyses. Although state-of-the-art methods (e.g., U-Net) have
achieved decent performance on cardiac image segmentation in terms of accuracy,
these segmentation results can still suffer from imaging artifacts and noise,
which will lead to inaccurate shape modeling results. In this paper, we propose
a PC-U net that jointly reconstructs the point cloud of the LV MYO wall
directly from volumes of 2D CT slices and generates its segmentation masks from
the predicted 3D point cloud. Extensive experimental results show that by
incorporating a shape prior from the point cloud, the segmentation masks are
more accurate than the state-of-the-art U-Net results in terms of Dice's
coefficient and Hausdorff distance.The proposed joint learning framework of our
PC-U net is beneficial for automatic cardiac image analysis tasks because it
can obtain simultaneously the 3D shape and segmentation of the LV MYO walls.",['cs.CV']
TapLab: A Fast Framework for Semantic Video Segmentation Tapping into Compressed-Domain Knowledge,"Real-time semantic video segmentation is a challenging task due to the strict
requirements of inference speed. Recent approaches mainly devote great efforts
to reducing the model size for high efficiency. In this paper, we rethink this
problem from a different viewpoint: using knowledge contained in compressed
videos. We propose a simple and effective framework, dubbed TapLab, to tap into
resources from the compressed domain. Specifically, we design a fast feature
warping module using motion vectors for acceleration. To reduce the noise
introduced by motion vectors, we design a residual-guided correction module and
a residual-guided frame selection module using residuals. TapLab significantly
reduces redundant computations of the state-of-the-art fast semantic image
segmentation models, running 3 to 10 times faster with controllable accuracy
degradation. The experimental results show that TapLab achieves 70.6% mIoU on
the Cityscapes dataset at 99.8 FPS with a single GPU card for the 1024x2048
videos. A high-speed version even reaches the speed of 160+ FPS. Codes will be
available soon at https://github.com/Sixkplus/TapLab.",['cs.CV']
A Neural Markovian Multiresolution Image Labeling Algorithm,"This paper describes the results of formally evaluating the MCV (Markov
concurrent vision) image labeling algorithm which is a (semi-) hierarchical
algorithm commencing with a partition made up of single pixel regions and
merging regions or subsets of regions using a Markov random field (MRF) image
model. It is an example of a general approach to computer vision called
concurrent vision in which the operations of image segmentation and image
classification are carried out concurrently. While many image labeling
algorithms output a single partition, or segmentation, the MCV algorithm
outputs a sequence of partitions and this more elaborate structure may provide
information that is valuable for higher level vision systems. With certain
types of MRF the component of the system for image evaluation can be
implemented as a hardwired feed forward neural network. While being applicable
to images (i.e. 2D signals), the algorithm is equally applicable to 1D signals
(e.g. speech) or 3D signals (e.g. video sequences) (though its performance in
such domains remains to be tested). The algorithm is assessed using subjective
and objective criteria with very good results.",['cs.CV']
Feedback Attention for Cell Image Segmentation,"In this paper, we address cell image segmentation task by Feedback Attention
mechanism like feedback processing. Unlike conventional neural network models
of feedforward processing, we focused on the feedback processing in human brain
and assumed that the network learns like a human by connecting feature maps
from deep layers to shallow layers. We propose some Feedback Attentions which
imitate human brain and feeds back the feature maps of output layer to close
layer to the input. U-Net with Feedback Attention showed better result than the
conventional methods using only feedforward processing.",['cs.CV']
Sparse Coding Driven Deep Decision Tree Ensembles for Nuclear Segmentation in Digital Pathology Images,"In this paper, we propose an easily trained yet powerful representation
learning approach with performance highly competitive to deep neural networks
in a digital pathology image segmentation task. The method, called sparse
coding driven deep decision tree ensembles that we abbreviate as ScD2TE,
provides a new perspective on representation learning. We explore the
possibility of stacking several layers based on non-differentiable pairwise
modules and generate a densely concatenated architecture holding the
characteristics of feature map reuse and end-to-end dense learning. Under this
architecture, fast convolutional sparse coding is used to extract multi-level
features from the output of each layer. In this way, rich image appearance
models together with more contextual information are integrated by learning a
series of decision tree ensembles. The appearance and the high-level context
features of all the previous layers are seamlessly combined by concatenating
them to feed-forward as input, which in turn makes the outputs of subsequent
layers more accurate and the whole model efficient to train. Compared with deep
neural networks, our proposed ScD2TE does not require back-propagation
computation and depends on less hyper-parameters. ScD2TE is able to achieve a
fast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the
superiority of our segmentation technique by evaluating it on the multi-disease
state and multi-organ dataset where consistently higher performances were
obtained for comparison against several state-of-the-art deep learning methods
such as convolutional neural networks (CNN), fully convolutional networks
(FCN), etc.",['cs.CV']
Image segmentation via Cellular Automata,"In this paper, we propose a new approach for building cellular automata to
solve real-world segmentation problems. We design and train a cellular
automaton that can successfully segment high-resolution images. We consider a
colony that densely inhabits the pixel grid, and all cells are governed by a
randomized update that uses the current state, the color, and the state of the
$3\times 3$ neighborhood. The space of possible rules is defined by a small
neural network. The update rule is applied repeatedly in parallel to a large
random subset of cells and after convergence is used to produce segmentation
masks that are then back-propagated to learn the optimal update rules using
standard gradient descent methods. We demonstrate that such models can be
learned efficiently with only limited trajectory length and that they show
remarkable ability to organize the information to produce a globally consistent
segmentation result, using only local information exchange. From a practical
perspective, our approach allows us to build very efficient models -- our
smallest automaton uses less than 10,000 parameters to solve complex
segmentation tasks.","['cs.CV', 'cs.LG']"
Domain Adaptive Medical Image Segmentation via Adversarial Learning of Disease-Specific Spatial Patterns,"In medical imaging, the heterogeneity of multi-centre data impedes the
applicability of deep learning-based methods and results in significant
performance degradation when applying models in an unseen data domain, e.g. a
new centreor a new scanner. In this paper, we propose an unsupervised domain
adaptation framework for boosting image segmentation performance across
multiple domains without using any manual annotations from the new target
domains, but by re-calibrating the networks on few images from the target
domain. To achieve this, we enforce architectures to be adaptive to new data by
rejecting improbable segmentation patterns and implicitly learning through
semantic and boundary information, thus to capture disease-specific spatial
patterns in an adversarial optimization. The adaptation process needs
continuous monitoring, however, as we cannot assume the presence of
ground-truth masks for the target domain, we propose two new metrics to monitor
the adaptation process, and strategies to train the segmentation algorithm in a
stable fashion. We build upon well-established 2D and 3D architectures and
perform extensive experiments on three cross-centre brain lesion segmentation
tasks, involving multicentre public and in-house datasets. We demonstrate that
recalibrating the deep networks on a few unlabeled images from the target
domain improves the segmentation accuracy significantly.",['cs.CV']
Learning To Pay Attention To Mistakes,"In convolutional neural network based medical image segmentation, the
periphery of foreground regions representing malignant tissues may be
disproportionately assigned as belonging to the background class of healthy
tissues
\cite{attenUnet}\cite{AttenUnet2018}\cite{InterSeg}\cite{UnetFrontNeuro}\cite{LearnActiveContour}.
This leads to high false negative detection rates. In this paper, we propose a
novel attention mechanism to directly address such high false negative rates,
called Paying Attention to Mistakes. Our attention mechanism steers the models
towards false positive identification, which counters the existing bias towards
false negatives. The proposed mechanism has two complementary implementations:
(a) ""explicit"" steering of the model to attend to a larger Effective Receptive
Field on the foreground areas; (b) ""implicit"" steering towards false positives,
by attending to a smaller Effective Receptive Field on the background areas. We
validated our methods on three tasks: 1) binary dense prediction between
vehicles and the background using CityScapes; 2) Enhanced Tumour Core
segmentation with multi-modal MRI scans in BRATS2018; 3) segmenting stroke
lesions using ultrasound images in ISLES2018. We compared our methods with
state-of-the-art attention mechanisms in medical imaging, including
self-attention, spatial-attention and spatial-channel mixed attention. Across
all of the three different tasks, our models consistently outperform the
baseline models in Intersection over Union (IoU) and/or Hausdorff Distance
(HD). For instance, in the second task, the ""explicit"" implementation of our
mechanism reduces the HD of the best baseline by more than $26\%$, whilst
improving the IoU by more than $3\%$. We believe our proposed attention
mechanism can benefit a wide range of medical and computer vision tasks, which
suffer from over-detection of background.",['cs.CV']
ATSO: Asynchronous Teacher-Student Optimization for Semi-Supervised Medical Image Segmentation,"In medical image analysis, semi-supervised learning is an effective method to
extract knowledge from a small amount of labeled data and a large amount of
unlabeled data. This paper focuses on a popular pipeline known as self
learning, and points out a weakness named lazy learning that refers to the
difficulty for a model to learn from the pseudo labels generated by itself. To
alleviate this issue, we propose ATSO, an asynchronous version of
teacher-student optimization. ATSO partitions the unlabeled data into two
subsets and alternately uses one subset to fine-tune the model and updates the
label on the other subset. We evaluate ATSO on two popular medical image
segmentation datasets and show its superior performance in various
semi-supervised settings. With slight modification, ATSO transfers well to
natural image segmentation for autonomous driving data.",['cs.CV']
Scribble-based Domain Adaptation via Co-segmentation,"Although deep convolutional networks have reached state-of-the-art
performance in many medical image segmentation tasks, they have typically
demonstrated poor generalisation capability. To be able to generalise from one
domain (e.g. one imaging modality) to another, domain adaptation has to be
performed. While supervised methods may lead to good performance, they require
to fully annotate additional data which may not be an option in practice. In
contrast, unsupervised methods don't need additional annotations but are
usually unstable and hard to train. In this work, we propose a novel
weakly-supervised method. Instead of requiring detailed but time-consuming
annotations, scribbles on the target domain are used to perform domain
adaptation. This paper introduces a new formulation of domain adaptation based
on structured learning and co-segmentation. Our method is easy to train, thanks
to the introduction of a regularised loss. The framework is validated on
Vestibular Schwannoma segmentation (T1 to T2 scans). Our proposed method
outperforms unsupervised approaches and achieves comparable performance to a
fully-supervised approach.",['cs.CV']
Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field,"Polarimetric synthetic aperture radar (PolSAR) image segmentation is
currently of great importance in image processing for remote sensing
applications. However, it is a challenging task due to two main reasons.
Firstly, the label information is difficult to acquire due to high annotation
costs. Secondly, the speckle effect embedded in the PolSAR imaging process
remarkably degrades the segmentation performance. To address these two issues,
we present a contextual PolSAR image semantic segmentation method in this
paper.With a newly defined channelwise consistent feature set as input, the
three-dimensional discrete wavelet transform (3D-DWT) technique is employed to
extract discriminative multi-scale features that are robust to speckle noise.
Then Markov random field (MRF) is further applied to enforce label smoothness
spatially during segmentation. By simultaneously utilizing 3D-DWT features and
MRF priors for the first time, contextual information is fully integrated
during the segmentation to ensure accurate and smooth segmentation. To
demonstrate the effectiveness of the proposed method, we conduct extensive
experiments on three real benchmark PolSAR image data sets. Experimental
results indicate that the proposed method achieves promising segmentation
accuracy and preferable spatial consistency using a minimal number of labeled
pixels.",['cs.CV']
Uncertainty quantification in medical image segmentation with normalizing flows,"Medical image segmentation is inherently an ambiguous task due to factors
such as partial volumes and variations in anatomical definitions. While in most
cases the segmentation uncertainty is around the border of structures of
interest, there can also be considerable inter-rater differences. The class of
conditional variational autoencoders (cVAE) offers a principled approach to
inferring distributions over plausible segmentations that are conditioned on
input images. Segmentation uncertainty estimated from samples of such
distributions can be more informative than using pixel level probability
scores. In this work, we propose a novel conditional generative model that is
based on conditional Normalizing Flow (cFlow). The basic idea is to increase
the expressivity of the cVAE by introducing a cFlow transformation step after
the encoder. This yields improved approximations of the latent posterior
distribution, allowing the model to capture richer segmentation variations.
With this we show that the quality and diversity of samples obtained from our
conditional generative model is enhanced. Performance of our model, which we
call cFlow Net, is evaluated on two medical imaging datasets demonstrating
substantial improvements in both qualitative and quantitative measures when
compared to a recent cVAE based model.","['stat.ML', 'cs.CV', 'cs.LG']"
PhraseCut: Language-based Image Segmentation in the Wild,"We consider the problem of segmenting image regions given a natural language
phrase, and study it on a novel dataset of 77,262 images and 345,486
phrase-region pairs. Our dataset is collected on top of the Visual Genome
dataset and uses the existing annotations to generate a challenging set of
referring phrases for which the corresponding regions are manually annotated.
Phrases in our dataset correspond to multiple regions and describe a large
number of object and stuff categories as well as their attributes such as
color, shape, parts, and relationships with other entities in the image. Our
experiments show that the scale and diversity of concepts in our dataset poses
significant challenges to the existing state-of-the-art. We systematically
handle the long-tail nature of these concepts and present a modular approach to
combine category, attribute, and relationship cues that outperforms existing
approaches.",['cs.CV']
Meta-DRN: Meta-Learning for 1-Shot Image Segmentation,"Modern deep learning models have revolutionized the field of computer vision.
But, a significant drawback of most of these models is that they require a
large number of labelled examples to generalize properly. Recent developments
in few-shot learning aim to alleviate this requirement. In this paper, we
propose a novel lightweight CNN architecture for 1-shot image segmentation. The
proposed model is created by taking inspiration from well-performing
architectures for semantic segmentation and adapting it to the 1-shot domain.
We train our model using 4 meta-learning algorithms that have worked well for
image classification and compare the results. For the chosen dataset, our
proposed model has a 70% lower parameter count than the benchmark, while having
better or comparable mean IoU scores using all 4 of the meta-learning
algorithms.","['cs.CV', 'cs.LG', 'eess.IV']"
State-of-The-Art Fuzzy Active Contour Models for Image Segmentation,"Image segmentation is the initial step for every image analysis task. A large
variety of segmentation algorithm has been proposed in the literature during
several decades with some mixed success. Among them, the fuzzy energy based
active contour models get attention to the researchers during last decade which
results in development of various methods. A good segmentation algorithm should
perform well in a large number of images containing noise, blur, low contrast,
region in-homogeneity, etc. However, the performances of the most of the
existing fuzzy energy based active contour models have been evaluated typically
on the limited number of images. In this article, our aim is to review the
existing fuzzy active contour models from the theoretical point of view and
also evaluate them experimentally on a large set of images under the various
conditions. The analysis under a large variety of images provides objective
insight into the strengths and weaknesses of various fuzzy active contour
models. Finally, we discuss several issues and future research direction on
this particular topic.","['cs.CV', 'eess.IV']"
Neural Style Transfer for Remote Sensing,"The well-known technique outlined in the paper of Leon A. Gatys et al., A
Neural Algorithm of Artistic Style, has become a trending topic both in
academic literature and industrial applications. Neural Style Transfer (NST)
constitutes an essential tool for a wide range of applications, such as
artistic stylization of 2D images, user-assisted creation tools and production
tools for entertainment applications. The purpose of this study is to present a
method for creating artistic maps from satellite images, based on the NST
algorithm. This method includes three basic steps (i) application of semantic
image segmentation on the original satellite image, dividing its content into
classes (i.e. land, water), (ii) application of neural style transfer for each
class and (iii) creation of a collage, i.e. an artistic image consisting of a
combination of the two stylized image generated on the previous step.","['cs.CV', 'cs.LG', 'eess.IV']"
Differentiable Rendering: A Survey,"Deep neural networks (DNNs) have shown remarkable performance improvements on
vision-related tasks such as object detection or image segmentation. Despite
their success, they generally lack the understanding of 3D objects which form
the image, as it is not always possible to collect 3D information about the
scene or to easily annotate it. Differentiable rendering is a novel field which
allows the gradients of 3D objects to be calculated and propagated through
images. It also reduces the requirement of 3D data collection and annotation,
while enabling higher success rate in various applications. This paper reviews
existing literature and discusses the current state of differentiable
rendering, its applications and open research problems.","['cs.CV', 'cs.GR']"
Heatmap-based Vanishing Point boosts Lane Detection,"Vision-based lane detection (LD) is a key part of autonomous driving
technology, and it is also a challenging problem. As one of the important
constraints of scene composition, vanishing point (VP) may provide a useful
clue for lane detection. In this paper, we proposed a new multi-task fusion
network architecture for high-precision lane detection. Firstly, the ERFNet was
used as the backbone to extract the hierarchical features of the road image.
Then, the lanes were detected using image segmentation. Finally, combining the
output of lane detection and the hierarchical features extracted by the
backbone, the lane VP was predicted using heatmap regression. The proposed
fusion strategy was tested using the public CULane dataset. The experimental
results suggest that the lane detection accuracy of our method outperforms
those of state-of-the-art (SOTA) methods.","['cs.CV', '68T45 (Primary) 68T07 (Secondary)', 'I.4.6']"
Weakly Supervised Minirhizotron Image Segmentation with MIL-CAM,"We present a multiple instance learning class activation map (MIL-CAM)
approach for pixel-level minirhizotron image segmentation given weak
image-level labels. Minirhizotrons are used to image plant roots in situ.
Minirhizotron imagery is often composed of soil containing a few long and thin
root objects of small diameter. The roots prove to be challenging for existing
semantic image segmentation methods to discriminate. In addition to learning
from weak labels, our proposed MIL-CAM approach re-weights the root versus soil
pixels during analysis for improved performance due to the heavy imbalance
between soil and root pixels. The proposed approach outperforms other attention
map and multiple instance learning methods for localization of root objects in
minirhizotron imagery.",['cs.CV']
Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction,"We present a method combining affinity prediction with region agglomeration,
which improves significantly upon the state of the art of neuron segmentation
from electron microscopy (EM) in accuracy and scalability. Our method consists
of a 3D U-NET, trained to predict affinities between voxels, followed by
iterative region agglomeration. We train using a structured loss based on
MALIS, encouraging topologically correct segmentations obtained from affinity
thresholding. Our extension consists of two parts: First, we present a
quasi-linear method to compute the loss gradient, improving over the original
quadratic algorithm. Second, we compute the gradient in two separate passes to
avoid spurious gradient contributions in early training stages. Our predictions
are accurate enough that simple learning-free percentile-based agglomeration
outperforms more involved methods used earlier on inferior predictions. We
present results on three diverse EM datasets, achieving relative improvements
over previous results of 27%, 15%, and 250%. Our findings suggest that a single
method can be applied to both nearly isotropic block-face EM data and
anisotropic serial sectioned EM data. The runtime of our method scales linearly
with the size of the volume and achieves a throughput of about 2.6 seconds per
megavoxel, qualifying our method for the processing of very large datasets.",['cs.CV']
Deep learning for image segmentation: veritable or overhyped?,"Deep learning has achieved great success as a powerful classification tool
and also made great progress in sematic segmentation. As a result, many
researchers also believe that deep learning is the most powerful tool for pixel
level image segmentation. Could deep learning achieve the same pixel level
accuracy as traditional image segmentation techniques by mapping the features
of the object into a non-linear function? This paper gives a short survey of
the accuracies achieved by deep learning so far in image classification and
image segmentation. Compared to the high accuracies achieved by deep learning
in classifying limited categories in international vision challenges, the image
segmentation accuracies achieved by deep learning in the same challenges are
only about eighty percent. On the contrary, the image segmentation accuracies
achieved in international biomedical challenges are close to ninty five
percent. Why the difference is so big? Since the accuracies of the competitors
methods are only evaluated based on their submitted results instead of
reproducing the results by submitting the source codes or the software, are the
achieved accuracies verifiable or overhyped? We are going to find it out by
analyzing the working principle of deep learning. Finally, we compared the
accuracies of state of the art deep learning methods with a threshold selection
method quantitatively. Experimental results showed that the threshold selection
method could achieve significantly higher accuracy than deep learning methods
in image segmentation.","['cs.CV', 'cs.LG']"
End-to-End Trainable Deep Active Contour Models for Automated Image Segmentation: Delineating Buildings in Aerial Imagery,"The automated segmentation of buildings in remote sensing imagery is a
challenging task that requires the accurate delineation of multiple building
instances over typically large image areas. Manual methods are often laborious
and current deep-learning-based approaches fail to delineate all building
instances and do so with adequate accuracy. As a solution, we present Trainable
Deep Active Contours (TDACs), an automatic image segmentation framework that
intimately unites Convolutional Neural Networks (CNNs) and Active Contour
Models (ACMs). The Eulerian energy functional of the ACM component includes
per-pixel parameter maps that are predicted by the backbone CNN, which also
initializes the ACM. Importantly, both the ACM and CNN components are fully
implemented in TensorFlow and the entire TDAC architecture is end-to-end
automatically differentiable and backpropagation trainable without user
intervention. TDAC yields fast, accurate, and fully automatic simultaneous
delineation of arbitrarily many buildings in the image. We validate the model
on two publicly available aerial image datasets for building segmentation, and
our results demonstrate that TDAC establishes a new state-of-the-art
performance.",['cs.CV']
Greenhouse Segmentation on High-Resolution Optical Satellite Imagery using Deep Learning Techniques,"Greenhouse segmentation has pivotal importance for climate-smart agricultural
land-use planning. Deep learning-based approaches provide state-of-the-art
performance in natural image segmentation. However, semantic segmentation on
high-resolution optical satellite imagery is a challenging task because of the
complex environment. In this paper, a sound methodology is proposed for
pixel-wise classification on images acquired by the Azersky (SPOT-7) optical
satellite. In particular, customized variations of U-Net-like architectures are
employed to identify greenhouses. Two models are proposed which uniquely
incorporate dilated convolutions and skip connections, and the results are
compared to that of the baseline U-Net model. The dataset used consists of
pan-sharpened orthorectified Azersky images (red, green, blue,and near infrared
channels) with 1.5-meter resolution and annotation masks, collected from 15
regions in Azerbaijan where the greenhouses are densely congested. The images
cover the cumulative area of 1008 $km^2$ and annotation masks contain 47559
polygons in total. The $F_1, Kappa, AUC$, and $IOU$ scores are used for
performance evaluation. It is observed that the use of the deconvolutional
layers alone throughout the expansive path does not yield satisfactory results;
therefore, they are either replaced or coupled with bilinear interpolation. All
models benefit from the hard example mining (HEM) strategy. It is also reported
that the best accuracy of $93.29\%$ ($F_1\,score$) is recorded when the
weighted binary cross-entropy loss is coupled with the dice loss. Experimental
results showed that both of the proposed models outperformed the baseline U-Net
architecture such that the best model proposed scored $4.48\%$ higher in
comparison to the baseline architecture.","['cs.CV', 'eess.IV']"
Globally Optimal Segmentation of Mutually Interacting Surfaces using Deep Learning,"Segmentation of multiple surfaces in medical images is a challenging problem,
further complicated by the frequent presence of weak boundary and mutual
influence between adjacent objects. The traditional graph-based optimal surface
segmentation method has proven its effectiveness with its ability of capturing
various surface priors in a uniform graph model. However, its efficacy heavily
relies on handcrafted features that are used to define the surface cost for the
""goodness"" of a surface. Recently, deep learning (DL) is emerging as powerful
tools for medical image segmentation thanks to its superior feature learning
capability. Unfortunately, due to the scarcity of training data in medical
imaging, it is nontrivial for DL networks to implicitly learn the global
structure of the target surfaces, including surface interactions. In this work,
we propose to parameterize the surface cost functions in the graph model and
leverage DL to learn those parameters. The multiple optimal surfaces are then
simultaneously detected by minimizing the total surface cost while explicitly
enforcing the mutual surface interaction constraints. The optimization problem
is solved by the primal-dual Internal Point Method, which can be implemented by
a layer of neural networks, enabling efficient end-to-end training of the whole
network. Experiments on Spectral Domain Optical Coherence Tomography (SD-OCT)
retinal layer segmentation and Intravascular Ultrasound (IVUS) vessel wall
segmentation demonstrated very promising results. All source code is public to
facilitate further research at this direction.","['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']"
Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images,"Semi-supervised learning has attracted much attention in medical image
segmentation due to challenges in acquiring pixel-wise image annotations, which
is a crucial step for building high-performance deep learning methods. Most
existing semi-supervised segmentation approaches either tend to neglect
geometric constraint in object segments, leading to incomplete object coverage,
or impose strong shape prior that requires extra alignment. In this work, we
propose a novel shapeaware semi-supervised segmentation strategy to leverage
abundant unlabeled data and to enforce a geometric shape constraint on the
segmentation output. To achieve this, we develop a multi-task deep network that
jointly predicts semantic segmentation and signed distance map(SDM) of object
surfaces. During training, we introduce an adversarial loss between the
predicted SDMs of labeled and unlabeled data so that our network is able to
capture shape-aware features more effectively. Experiments on the Atrial
Segmentation Challenge dataset show that our method outperforms current
state-of-the-art approaches with improved shape estimation, which validates its
efficacy. Code is available at https://github.com/kleinzcy/SASSnet.",['cs.CV']
AinnoSeg: Panoramic Segmentation with High Perfomance,"Panoramic segmentation is a scene where image segmentation tasks is more
difficult. With the development of CNN networks, panoramic segmentation tasks
have been sufficiently developed.However, the current panoramic segmentation
algorithms are more concerned with context semantics, but the details of image
are not processed enough. Moreover, they cannot solve the problems which
contains the accuracy of occluded object segmentation,little object
segmentation,boundary pixel in object segmentation etc. Aiming to address these
issues, this paper presents some useful tricks. (a) By changing the basic
segmentation model, the model can take into account the large objects and the
boundary pixel classification of image details. (b) Modify the loss function so
that it can take into account the boundary pixels of multiple objects in the
image. (c) Use a semi-supervised approach to regain control of the training
process. (d) Using multi-scale training and reasoning. All these operations
named AinnoSeg, AinnoSeg can achieve state-of-art performance on the well-known
dataset ADE20K.","['cs.CV', 'cs.LG', 'eess.IV']"
Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering,"The usage of convolutional neural networks (CNNs) for unsupervised image
segmentation was investigated in this study. In the proposed approach, label
prediction and network parameter learning are alternately iterated to meet the
following criteria: (a) pixels of similar features should be assigned the same
label, (b) spatially continuous pixels should be assigned the same label, and
(c) the number of unique labels should be large. Although these criteria are
incompatible, the proposed approach minimizes the combination of similarity
loss and spatial continuity loss to find a plausible solution of label
assignment that balances the aforementioned criteria well. The contributions of
this study are four-fold. First, we propose a novel end-to-end network of
unsupervised image segmentation that consists of normalization and an argmax
function for differentiable clustering. Second, we introduce a spatial
continuity loss function that mitigates the limitations of fixed segment
boundaries possessed by previous work. Third, we present an extension of the
proposed method for segmentation with scribbles as user input, which showed
better accuracy than existing methods while maintaining efficiency. Finally, we
introduce another extension of the proposed method: unseen image segmentation
by using networks pre-trained with a few reference images without re-training
the networks. The effectiveness of the proposed approach was examined on
several benchmark datasets of image segmentation.",['cs.CV']
Naive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation,"Supervised learning in large discriminative models is a mainstay for modern
computer vision. Such an approach necessitates investing in large-scale
human-annotated datasets for achieving state-of-the-art results. In turn, the
efficacy of supervised learning may be limited by the size of the human
annotated dataset. This limitation is particularly notable for image
segmentation tasks, where the expense of human annotation is especially large,
yet large amounts of unlabeled data may exist. In this work, we ask if we may
leverage semi-supervised learning in unlabeled video sequences and extra images
to improve the performance on urban scene segmentation, simultaneously tackling
semantic, instance, and panoptic segmentation. The goal of this work is to
avoid the construction of sophisticated, learned architectures specific to
label propagation (e.g., patch matching and optical flow). Instead, we simply
predict pseudo-labels for the unlabeled data and train subsequent models with
both human-annotated and pseudo-labeled data. The procedure is iterated for
several times. As a result, our Naive-Student model, trained with such simple
yet effective iterative semi-supervised learning, attains state-of-the-art
results at all three Cityscapes benchmarks, reaching the performance of 67.8%
PQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable
step towards building a simple procedure to harness unlabeled video sequences
and extra images to surpass state-of-the-art performance on core computer
vision tasks.",['cs.CV']
Automatic Image Labelling at Pixel Level,"The performance of deep networks for semantic image segmentation largely
depends on the availability of large-scale training images which are labelled
at the pixel level. Typically, such pixel-level image labellings are obtained
manually by a labour-intensive process. To alleviate the burden of manual image
labelling, we propose an interesting learning approach to generate pixel-level
image labellings automatically. A Guided Filter Network (GFN) is first
developed to learn the segmentation knowledge from a source domain, and such
GFN then transfers such segmentation knowledge to generate coarse object masks
in the target domain. Such coarse object masks are treated as pseudo labels and
they are further integrated to optimize/refine the GFN iteratively in the
target domain. Our experiments on six image sets have demonstrated that our
proposed approach can generate fine-grained object masks (i.e., pixel-level
object labellings), whose quality is very comparable to the manually-labelled
ones. Our proposed approach can also achieve better performance on semantic
image segmentation than most existing weakly-supervised approaches.",['cs.CV']
Beyond Single Stage Encoder-Decoder Networks: Deep Decoders for Semantic Image Segmentation,"Single encoder-decoder methodologies for semantic segmentation are reaching
their peak in terms of segmentation quality and efficiency per number of
layers. To address these limitations, we propose a new architecture based on a
decoder which uses a set of shallow networks for capturing more information
content. The new decoder has a new topology of skip connections, namely
backward and stacked residual connections. In order to further improve the
architecture we introduce a weight function which aims to re-balance classes to
increase the attention of the networks to under-represented objects. We carried
out an extensive set of experiments that yielded state-of-the-art results for
the CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove the
effectiveness of our decoder, we conducted a set of experiments studying the
impact of our decoder to state-of-the-art segmentation techniques.
Additionally, we present a set of experiments augmenting semantic segmentation
with optical flow information, showing that motion clues can boost pure image
based semantic segmentation approaches.","['cs.CV', 'cs.RO']"
Superpixel-Guided Label Softening for Medical Image Segmentation,"Segmentation of objects of interest is one of the central tasks in medical
image analysis, which is indispensable for quantitative analysis. When
developing machine-learning based methods for automated segmentation, manual
annotations are usually used as the ground truth toward which the models learn
to mimic. While the bulky parts of the segmentation targets are relatively easy
to label, the peripheral areas are often difficult to handle due to ambiguous
boundaries and the partial volume effect, etc., and are likely to be labeled
with uncertainty. This uncertainty in labeling may, in turn, result in
unsatisfactory performance of the trained models. In this paper, we propose
superpixel-based label softening to tackle the above issue. Generated by
unsupervised over-segmentation, each superpixel is expected to represent a
locally homogeneous area. If a superpixel intersects with the annotation
boundary, we consider a high probability of uncertain labeling within this
area. Driven by this intuition, we soften labels in this area based on signed
distances to the annotation boundary and assign probability values within [0,
1] to them, in comparison with the original ""hard"", binary labels of either 0
or 1. The softened labels are then used to train the segmentation models
together with the hard labels. Experimental results on a brain MRI dataset and
an optical coherence tomography dataset demonstrate that this conceptually
simple and implementation-wise easy method achieves overall superior
segmentation performances to baseline and comparison methods for both 3D and 2D
medical images.",['cs.CV']
Anisotropic Mesh Adaptation for Image Segmentation Based on Mumford-Shah Functional,"As the resolution of digital images increase significantly, the processing of
images becomes more challenging in terms of accuracy and efficiency. In this
paper, we consider image segmentation by solving a partial differentiation
equation (PDE) model based on the Mumford-Shah functional. We develop a new
algorithm by combining anisotropic mesh adaptation for image representation and
finite element method for solving the PDE model. Comparing to traditional
algorithms solved by finite difference method, our algorithm provides faster
and better results without the need to resizing the images to lower quality. We
also extend the algorithm to segment images with multiple regions.","['cs.CV', 'cs.NA', 'math.NA']"
Autoregressive Unsupervised Image Segmentation,"In this work, we propose a new unsupervised image segmentation approach based
on mutual information maximization between different constructed views of the
inputs. Taking inspiration from autoregressive generative models that predict
the current pixel from past pixels in a raster-scan ordering created with
masked convolutions, we propose to use different orderings over the inputs
using various forms of masked convolutions to construct different views of the
data. For a given input, the model produces a pair of predictions with two
valid orderings, and is then trained to maximize the mutual information between
the two outputs. These outputs can either be low-dimensional features for
representation learning or output clusters corresponding to semantic labels for
clustering. While masked convolutions are used during training, in inference,
no masking is applied and we fall back to the standard convolution where the
model has access to the full input. The proposed method outperforms current
state-of-the-art on unsupervised image segmentation. It is simple and easy to
implement, and can be extended to other visual tasks and integrated seamlessly
into existing unsupervised learning methods requiring different views of the
data.",['cs.CV']
An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis,"Convolutional neural networks have shown to achieve superior performance on
image segmentation tasks. However, convolutional neural networks, operating as
black-box systems, generally do not provide a reliable measure about the
confidence of their decisions. This leads to various problems in industrial
settings, amongst others, inadequate levels of trust from users in the model's
outputs as well as a non-compliance with current policy guidelines (e.g., EU AI
Strategy). To address these issues, we use uncertainty measures based on
Monte-Carlo dropout in the context of a human-in-the-loop system to increase
the system's transparency and performance. In particular, we demonstrate the
benefits described above on a real-world multi-class image segmentation task of
wear analysis in the machining industry. Following previous work, we show that
the quality of a prediction correlates with the model's uncertainty.
Additionally, we demonstrate that a multiple linear regression using the
model's uncertainties as independent variables significantly explains the
quality of a prediction (\(R^2=0.718\)). Within the uncertainty-based
human-in-the-loop system, the multiple regression aims at identifying failed
predictions on an image-level. The system utilizes a human expert to label
these failed predictions manually. A simulation study demonstrates that the
uncertainty-based human-in-the-loop system increases performance for different
levels of human involvement in comparison to a random-based human-in-the-loop
system. To ensure generalizability, we show that the presented approach
achieves similar results on the publicly available Cityscapes dataset.",['cs.CV']
Tackling the Problem of Limited Data and Annotations in Semantic Segmentation,"In this work, the case of semantic segmentation on a small image dataset
(simulated by 1000 randomly selected images from PASCAL VOC 2012), where only
weak supervision signals (scribbles from user interaction) are available is
studied. Especially, to tackle the problem of limited data annotations in image
segmentation, transferring different pre-trained models and CRF based methods
are applied to enhance the segmentation performance. To this end, RotNet,
DeeperCluster, and Semi&Weakly Supervised Learning (SWSL) pre-trained models
are transferred and finetuned in a DeepLab-v2 baseline, and dense CRF is
applied both as a post-processing and loss regularization technique. The
results of my study show that, on this small dataset, using a pre-trained
ResNet50 SWSL model gives results that are 7.4% better than applying an
ImageNet pre-trained model; moreover, for the case of training on the full
PASCAL VOC 2012 training data, this pre-training approach increases the mIoU
results by almost 4%. On the other hand, dense CRF is shown to be very
effective as well, enhancing the results both as a loss regularization
technique in weakly supervised training and as a post-processing tool.",['cs.CV']
BUNET: Blind Medical Image Segmentation Based on Secure UNET,"The strict security requirements placed on medical records by various privacy
regulations become major obstacles in the age of big data. To ensure efficient
machine learning as a service schemes while protecting data confidentiality, in
this work, we propose blind UNET (BUNET), a secure protocol that implements
privacy-preserving medical image segmentation based on the UNET architecture.
In BUNET, we efficiently utilize cryptographic primitives such as homomorphic
encryption and garbled circuits (GC) to design a complete secure protocol for
the UNET neural architecture. In addition, we perform extensive architectural
search in reducing the computational bottleneck of GC-based secure activation
protocols with high-dimensional input data. In the experiment, we thoroughly
examine the parameter space of our protocol, and show that we can achieve up to
14x inference time reduction compared to the-state-of-the-art secure inference
technique on a baseline architecture with negligible accuracy degradation.","['cs.CV', 'cs.CR']"
On uncertainty estimation in active learning for image segmentation,"Uncertainty estimation is important for interpreting the trustworthiness of
machine learning models in many applications. This is especially critical in
the data-driven active learning setting where the goal is to achieve a certain
accuracy with minimum labeling effort. In such settings, the model learns to
select the most informative unlabeled samples for annotation based on its
estimated uncertainty. The highly uncertain predictions are assumed to be more
informative for improving model performance. In this paper, we explore
uncertainty calibration within an active learning framework for medical image
segmentation, an area where labels often are scarce. Various uncertainty
estimation methods and acquisition strategies (regions and full images) are
investigated. We observe that selecting regions to annotate instead of full
images leads to more well-calibrated models. Additionally, we experimentally
show that annotating regions can cut 50% of pixels that need to be labeled by
humans compared to annotating full images.","['cs.CV', 'cs.LG']"
Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation,"Medical image annotations are prohibitively time-consuming and expensive to
obtain. To alleviate annotation scarcity, many approaches have been developed
to efficiently utilize extra information, e.g.,semi-supervised learning further
exploring plentiful unlabeled data, domain adaptation including multi-modality
learning and unsupervised domain adaptation resorting to the prior knowledge
from additional modality. In this paper, we aim to investigate the feasibility
of simultaneously leveraging abundant unlabeled data and well-established
cross-modality data for annotation-efficient medical image segmentation. To
this end, we propose a novel semi-supervised domain adaptation approach, namely
Dual-Teacher, where the student model not only learns from labeled target data
(e.g., CT), but also explores unlabeled target data and labeled source data
(e.g., MR) by two teacher models. Specifically, the student model learns the
knowledge of unlabeled target data from intra-domain teacher by encouraging
prediction consistency, as well as the shape priors embedded in labeled source
data from inter-domain teacher via knowledge distillation. Consequently, the
student model can effectively exploit the information from all three data
resources and comprehensively integrate them to achieve improved performance.
We conduct extensive experiments on MM-WHS 2017 dataset and demonstrate that
our approach is able to concurrently utilize unlabeled data and cross-modality
data with superior performance, outperforming semi-supervised learning and
domain adaptation methods with a large margin.",['cs.CV']
Towards Unsupervised Learning for Instrument Segmentation in Robotic Surgery with Cycle-Consistent Adversarial Networks,"Surgical tool segmentation in endoscopic images is an important problem: it
is a crucial step towards full instrument pose estimation and it is used for
integration of pre- and intra-operative images into the endoscopic view. While
many recent approaches based on convolutional neural networks have shown great
results, a key barrier to progress lies in the acquisition of a large number of
manually-annotated images which is necessary for an algorithm to generalize and
work well in diverse surgical scenarios. Unlike the surgical image data itself,
annotations are difficult to acquire and may be of variable quality. On the
other hand, synthetic annotations can be automatically generated by using
forward kinematic model of the robot and CAD models of tools by projecting them
onto an image plane. Unfortunately, this model is very inaccurate and cannot be
used for supervised learning of image segmentation models. Since generated
annotations will not directly correspond to endoscopic images due to errors, we
formulate the problem as an unpaired image-to-image translation where the goal
is to learn the mapping between an input endoscopic image and a corresponding
annotation using an adversarial model. Our approach allows to train image
segmentation models without the need to acquire expensive annotations and can
potentially exploit large unlabeled endoscopic image collection outside the
annotated distributions of image/annotation data. We test our proposed method
on Endovis 2017 challenge dataset and show that it is competitive with
supervised segmentation methods.",['cs.CV']
Superpixel Segmentation using Dynamic and Iterative Spanning Forest,"As constituent parts of image objects, superpixels can improve several
higher-level operations. However, image segmentation methods might have their
accuracy seriously compromised for reduced numbers of superpixels. We have
investigated a solution based on the Iterative Spanning Forest (ISF) framework.
In this work, we present Dynamic ISF (DISF) -- a method based on the following
steps. (a) It starts from an image graph and a seed set with considerably more
pixels than the desired number of superpixels. (b) The seeds compete among
themselves, and each seed conquers its most closely connected pixels, resulting
in an image partition (spanning forest) with connected superpixels. In step
(c), DISF assigns relevance values to seeds based on superpixel analysis and
removes the most irrelevant ones. Steps (b) and (c) are repeated until the
desired number of superpixels is reached. DISF has the chance to reconstruct
relevant edges after each iteration, when compared to region merging
algorithms. As compared to other seed-based superpixel methods, DISF is more
likely to find relevant seeds. It also introduces dynamic arc-weight estimation
in the ISF framework for more effective superpixel delineation, and we
demonstrate all results on three datasets with distinct object properties.",['cs.CV']
A Multi-scale CNN-CRF Framework for Environmental Microorganism Image Segmentation,"To assist researchers to identify Environmental Microorganisms (EMs)
effectively, a Multiscale CNN-CRF (MSCC) framework for the EM image
segmentation is proposed in this paper. There are two parts in this framework:
The first is a novel pixel-level segmentation approach, using a newly
introduced Convolutional Neural Network (CNN), namely, ""mU-Net-B3"", with a
dense Conditional Random Field (CRF) postprocessing. The second is a VGG-16
based patch-level segmentation method with a novel ""buffer"" strategy, which
further improves the segmentation quality of the details of the EMs. In the
experiment, compared with the state-of-the-art methods on 420 EM images, the
proposed MSCC method reduces the memory requirement from 355 MB to 103 MB,
improves the overall evaluation indexes (Dice, Jaccard, Recall, Accuracy) from
85.24%, 77.42%, 82.27%, and 96.76% to 87.13%, 79.74%, 87.12%, and 96.91%,
respectively, and reduces the volume overlap error from 22.58% to 20.26%.
Therefore, the MSCC method shows great potential in the EM segmentation field.",['cs.CV']
Fully Hyperbolic Convolutional Neural Networks,"Convolutional Neural Networks (CNN) have recently seen tremendous success in
various computer vision tasks. However, their application to problems with high
dimensional input and output, such as high-resolution image and video
segmentation or 3D medical imaging, has been limited by various factors.
Primarily, in the training stage, it is necessary to store network activations
for back propagation. In these settings, the memory requirements associated
with storing activations can exceed what is feasible with current hardware,
especially for problems in 3D. Motivated by the propagation of signals over
physical networks, that are governed by the hyperbolic Telegraph equation, in
this work we introduce a fully conservative hyperbolic network for problems
with high dimensional input and output. We introduce a coarsening operation
that allows completely reversible CNNs by using a learnable Discrete Wavelet
Transform and its inverse to both coarsen and interpolate the network state and
change the number of channels. We show that fully reversible networks are able
to achieve results comparable to the state of the art in 4D time-lapse hyper
spectral image segmentation and full 3D video segmentation, with a much lower
memory footprint that is a constant independent of the network depth. We also
extend the use of such networks to Variational Auto Encoders with high
resolution input and output.","['cs.CV', 'cs.LG']"
Meta Corrupted Pixels Mining for Medical Image Segmentation,"Deep neural networks have achieved satisfactory performance in piles of
medical image analysis tasks. However the training of deep neural network
requires a large amount of samples with high-quality annotations. In medical
image segmentation, it is very laborious and expensive to acquire precise
pixel-level annotations. Aiming at training deep segmentation models on
datasets with probably corrupted annotations, we propose a novel Meta Corrupted
Pixels Mining (MCPM) method based on a simple meta mask network. Our method is
targeted at automatically estimate a weighting map to evaluate the importance
of every pixel in the learning of segmentation network. The meta mask network
which regards the loss value map of the predicted segmentation results as
input, is capable of identifying out corrupted layers and allocating small
weights to them. An alternative algorithm is adopted to train the segmentation
network and the meta mask network, simultaneously. Extensive experimental
results on LIDC-IDRI and LiTS datasets show that our method outperforms
state-of-the-art approaches which are devised for coping with corrupted
annotations.","['cs.CV', 'J.3']"
Medical Image Segmentation via Unsupervised Convolutional Neural Network,"For the majority of the learning-based segmentation methods, a large quantity
of high-quality training data is required. In this paper, we present a novel
learning-based segmentation model that could be trained semi- or un-
supervised. Specifically, in the unsupervised setting, we parameterize the
Active contour without edges (ACWE) framework via a convolutional neural
network (ConvNet), and optimize the parameters of the ConvNet using a
self-supervised method. In another setting (semi-supervised), the auxiliary
segmentation ground truth is used during training. We show that the method
provides fast and high-quality bone segmentation in the context of
single-photon emission computed tomography (SPECT) image.","['cs.CV', 'cs.LG', 'eess.IV']"
Anatomical Data Augmentation via Fluid-based Image Registration,"We introduce a fluid-based image augmentation method for medical image
analysis. In contrast to existing methods, our framework generates anatomically
meaningful images via interpolation from the geodesic subspace underlying given
samples. Our approach consists of three steps: 1) given a source image and a
set of target images, we construct a geodesic subspace using the Large
Deformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample
transformations from the resulting geodesic subspace; 3) we obtain deformed
images and segmentations via interpolation. Experiments on brain (LPBA) and
knee (OAI) data illustrate the performance of our approach on two tasks: 1)
data augmentation during training and testing for image segmentation; 2)
one-shot learning for single atlas image segmentation. We demonstrate that our
approach generates anatomically meaningful data and improves performance on
these tasks over competing approaches. Code is available at
https://github.com/uncbiag/easyreg.","['cs.CV', 'I.2.10']"
Complex Network Construction for Interactive Image Segmentation using Particle Competition and Cooperation: A New Approach,"In the interactive image segmentation task, the Particle Competition and
Cooperation (PCC) model is fed with a complex network, which is built from the
input image. In the network construction phase, a weight vector is needed to
define the importance of each element in the feature set, which consists of
color and location information of the corresponding pixels, thus demanding a
specialist's intervention. The present paper proposes the elimination of the
weight vector through modifications in the network construction phase. The
proposed model and the reference model, without the use of a weight vector,
were compared using 151 images extracted from the Grabcut dataset, the PASCAL
VOC dataset and the Alpha matting dataset. Each model was applied 30 times to
each image to obtain an error average. These simulations resulted in an error
rate of only 0.49\% when classifying pixels with the proposed model while the
reference model had an error rate of 3.14\%. The proposed method also presented
less error variation in the diversity of the evaluated images, when compared to
the reference model.","['cs.CV', 'cs.LG']"
Suggestive Annotation of Brain Tumour Images with Gradient-guided Sampling,"Machine learning has been widely adopted for medical image analysis in recent
years given its promising performance in image segmentation and classification
tasks. As a data-driven science, the success of machine learning, in particular
supervised learning, largely depends on the availability of manually annotated
datasets. For medical imaging applications, such annotated datasets are not
easy to acquire. It takes a substantial amount of time and resource to curate
an annotated medical image set. In this paper, we propose an efficient
annotation framework for brain tumour images that is able to suggest
informative sample images for human experts to annotate. Our experiments show
that training a segmentation model with only 19% suggestively annotated patient
scans from BraTS 2019 dataset can achieve a comparable performance to training
a model on the full dataset for whole tumour segmentation task. It demonstrates
a promising way to save manual annotation cost and improve data efficiency in
medical imaging applications.","['cs.CV', 'cs.LG', 'eess.IV']"
Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation,"Computer vision tasks such as semantic segmentation perform very well in good
weather conditions, but if the weather turns bad, they have problems to achieve
this performance in these conditions. One possibility to obtain more robust and
reliable results in adverse weather conditions is to use video-segmentation
approaches instead of commonly used single-image segmentation methods.
Video-segmentation approaches capture temporal information of the previous
video-frames in addition to current image information, and hence, they are more
robust against disturbances, especially if they occur in only a few frames of
the video-sequence. However, video-segmentation approaches, which are often
based on recurrent neural networks, cannot be applied in real-time applications
anymore, since their recurrent structures in the network are computational
expensive. For instance, the inference time of the LSTM-ICNet, in which
recurrent units are placed at proper positions in the single-segmentation
approach ICNet, increases up to 61 percent compared to the basic ICNet. Hence,
in this work, the LSTM-ICNet is sped up by modifying the recurrent units of the
network so that it becomes real-time capable again. Experiments on different
datasets and various weather conditions show that the inference time can be
decreased by about 23 percent by these modifications, while they achieve
similar performance than the LSTM-ICNet and outperform the single-segmentation
approach enormously in adverse weather conditions.","['cs.CV', 'cs.LG', 'eess.IV']"
Kullback-Leibler Divergence-Based Fuzzy $C$-Means Clustering Incorporating Morphological Reconstruction and Wavelet Frames for Image Segmentation,"Although spatial information of images usually enhance the robustness of the
Fuzzy C-Means (FCM) algorithm, it greatly increases the computational costs for
image segmentation. To achieve a sound trade-off between the segmentation
performance and the speed of clustering, we come up with a Kullback-Leibler
(KL) divergence-based FCM algorithm by incorporating a tight wavelet frame
transform and a morphological reconstruction operation. To enhance FCM's
robustness, an observed image is first filtered by using the morphological
reconstruction. A tight wavelet frame system is employed to decompose the
observed and filtered images so as to form their feature sets. Considering
these feature sets as data of clustering, an modified FCM algorithm is
proposed, which introduces a KL divergence term in the partition matrix into
its objective function. The KL divergence term aims to make membership degrees
of each image pixel closer to those of its neighbors, which brings that the
membership partition becomes more suitable and the parameter setting of FCM
becomes simplified. On the basis of the obtained partition matrix and
prototypes, the segmented feature set is reconstructed by minimizing the
inverse process of the modified objective function. To modify abnormal features
produced in the reconstruction process, each reconstructed feature is
reassigned to the closest prototype. As a result, the segmentation accuracy of
KL divergence-based FCM is further improved. What's more, the segmented image
is reconstructed by using a tight wavelet frame reconstruction operation.
Finally, supporting experiments coping with synthetic, medical and color images
are reported. Experimental results exhibit that the proposed algorithm works
well and comes with better segmentation performance than other comparative
algorithms. Moreover, the proposed algorithm requires less time than most of
the FCM-related algorithms.","['cs.CV', '62H30', 'I.4.6']"
Automated Pavement Crack Segmentation Using U-Net-based Convolutional Neural Network,"Automated pavement crack image segmentation is challenging because of
inherent irregular patterns, lighting conditions, and noise in images.
Conventional approaches require a substantial amount of feature engineering to
differentiate crack regions from non-affected regions. In this paper, we
propose a deep learning technique based on a convolutional neural network to
perform segmentation tasks on pavement crack images. Our approach requires
minimal feature engineering compared to other machine learning techniques. We
propose a U-Net-based network architecture in which we replace the encoder with
a pretrained ResNet-34 neural network. We use a ""one-cycle"" training schedule
based on cyclical learning rates to speed up the convergence. Our method
achieves an F1 score of 96% on the CFD dataset and 73% on the Crack500 dataset,
outperforming other algorithms tested on these datasets. We perform ablation
studies on various techniques that helped us get marginal performance boosts,
i.e., the addition of spatial and channel squeeze and excitation (SCSE)
modules, training with gradually increasing image sizes, and training various
neural network layers with different learning rates.",['cs.CV']
Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation,"Although having achieved great success in medical image segmentation, deep
learning-based approaches usually require large amounts of well-annotated data,
which can be extremely expensive in the field of medical image analysis.
Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised
learning and unsupervised domain adaptation both take the advantage of
unlabeled data, and they are closely related to each other. In this paper, we
propose uncertainty-aware multi-view co-training (UMCT), a unified framework
that addresses these two tasks for volumetric medical image segmentation. Our
framework is capable of efficiently utilizing unlabeled data for better
performance. We firstly rotate and permute the 3D volumes into multiple views
and train a 3D deep network on each view. We then apply co-training by
enforcing multi-view consistency on unlabeled data, where an uncertainty
estimation of each view is utilized to achieve accurate labeling. Experiments
on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset
show state-of-the-art performance of the proposed framework on semi-supervised
medical image segmentation. Under unsupervised domain adaptation settings, we
validate the effectiveness of this work by adapting our multi-organ
segmentation model to two pathological organs from the Medical Segmentation
Decathlon Datasets. Additionally, we show that our UMCT-DA model can even
effectively handle the challenging situation where labeled source data is
inaccessible, demonstrating strong potentials for real-world applications.",['cs.CV']
Interactive Deep Refinement Network for Medical Image Segmentation,"Deep learning techniques have successfully been employed in numerous computer
vision tasks including image segmentation. The techniques have also been
applied to medical image segmentation, one of the most critical tasks in
computer-aided diagnosis. Compared with natural images, the medical image is a
gray-scale image with low-contrast (even with some invisible parts). Because
some organs have similar intensity and texture with neighboring organs, there
is usually a need to refine automatic segmentation results. In this paper, we
propose an interactive deep refinement framework to improve the traditional
semantic segmentation networks such as U-Net and fully convolutional network.
In the proposed framework, we added a refinement network to traditional
segmentation network to refine the segmentation results.Experimental results
with public dataset revealed that the proposed method could achieve higher
accuracy than other state-of-the-art methods.",['cs.CV']
Region-of-interest guided Supervoxel Inpainting for Self-supervision,"Self-supervised learning has proven to be invaluable in making best use of
all of the available data in biomedical image segmentation. One particularly
simple and effective mechanism to achieve self-supervision is inpainting, the
task of predicting arbitrary missing areas based on the rest of an image. In
this work, we focus on image inpainting as the self-supervised proxy task, and
propose two novel structural changes to further enhance the performance of a
deep neural network. We guide the process of generating images to inpaint by
using supervoxel-based masking instead of random masking, and also by focusing
on the area to be segmented in the primary task, which we term as the
region-of-interest. We postulate that these additions force the network to
learn semantics that are more attuned to the primary task, and test our
hypotheses on two applications: brain tumour and white matter hyperintensities
segmentation. We empirically show that our proposed approach consistently
outperforms both supervised CNNs, without any self-supervision, and
conventional inpainting-based self-supervision methods on both large and small
training set sizes.",['cs.CV']
Post-DAE: Anatomically Plausible Segmentation via Post-Processing with Denoising Autoencoders,"We introduce Post-DAE, a post-processing method based on denoising
autoencoders (DAE) to improve the anatomical plausibility of arbitrary
biomedical image segmentation algorithms. Some of the most popular segmentation
methods (e.g. based on convolutional neural networks or random forest
classifiers) incorporate additional post-processing steps to ensure that the
resulting masks fulfill expected connectivity constraints. These methods
operate under the hypothesis that contiguous pixels with similar aspect should
belong to the same class. Even if valid in general, this assumption does not
consider more complex priors like topological restrictions or convexity, which
cannot be easily incorporated into these methods. Post-DAE leverages the latest
developments in manifold learning via denoising autoencoders. First, we learn a
compact and non-linear embedding that represents the space of anatomically
plausible segmentations. Then, given a segmentation mask obtained with an
arbitrary method, we reconstruct its anatomically plausible version by
projecting it onto the learnt manifold. The proposed method is trained using
unpaired segmentation mask, what makes it independent of intensity information
and image modality. We performed experiments in binary and multi-label
segmentation of chest X-ray and cardiac magnetic resonance images. We show how
erroneous and noisy segmentation masks can be improved using Post-DAE. With
almost no additional computation cost, our method brings erroneous
segmentations back to a feasible space.","['cs.CV', 'cs.LG', 'eess.IV']"
Large-scale detection and categorization of oil spills from SAR images with deep learning,"We propose a deep learning framework to detect and categorize oil spills in
synthetic aperture radar (SAR) images at a large scale. By means of a carefully
designed neural network model for image segmentation trained on an extensive
dataset, we are able to obtain state-of-the-art performance in oil spill
detection, achieving results that are comparable to results produced by human
operators. We also introduce a classification task, which is novel in the
context of oil spill detection in SAR. Specifically, after being detected, each
oil spill is also classified according to different categories pertaining to
its shape and texture characteristics. The classification results provide
valuable insights for improving the design of oil spill services by
world-leading providers. As the last contribution, we present our operational
pipeline and a visualization tool for large-scale data, which allows to detect
and analyze the historical presence of oil spills worldwide.","['cs.CV', 'cs.LG']"
Road surface detection and differentiation considering surface damages,"A challenge still to be overcome in the field of visual perception for
vehicle and robotic navigation on heavily damaged and unpaved roads is the task
of reliable path and obstacle detection. The vast majority of the researches
have as scenario roads in good condition, from developed countries. These works
cope with few situations of variation on the road surface and even fewer
situations presenting surface damages. In this paper we present an approach for
road detection considering variation in surface types, identifying paved and
unpaved surfaces and also detecting damage and other information on other road
surface that may be relevant to driving safety. We also present a new Ground
Truth with image segmentation, used in our approach and that allowed us to
evaluate our results. Our results show that it is possible to use passive
vision for these purposes, even using images captured with low cost cameras.","['cs.CV', 'I.5; I.5.4; I.4; I.4.6; I.4.8; I.4.9']"
"Deep Learning of Unified Region, Edge, and Contour Models for Automated Image Segmentation","Image segmentation is a fundamental and challenging problem in computer
vision with applications spanning multiple areas, such as medical imaging,
remote sensing, and autonomous vehicles. Recently, convolutional neural
networks (CNNs) have gained traction in the design of automated segmentation
pipelines. Although CNN-based models are adept at learning abstract features
from raw image data, their performance is dependent on the availability and
size of suitable training datasets. Additionally, these models are often unable
to capture the details of object boundaries and generalize poorly to unseen
classes. In this thesis, we devise novel methodologies that address these
issues and establish robust representation learning frameworks for
fully-automatic semantic segmentation in medical imaging and mainstream
computer vision. In particular, our contributions include (1) state-of-the-art
2D and 3D image segmentation networks for computer vision and medical image
analysis, (2) an end-to-end trainable image segmentation framework that unifies
CNNs and active contour models with learnable parameters for fast and robust
object delineation, (3) a novel approach for disentangling edge and texture
processing in segmentation networks, and (4) a novel few-shot learning model in
both supervised settings and semi-supervised settings where synergies between
latent and image spaces are leveraged to learn to segment images given limited
training data.",['cs.CV']
Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition,"This work introduces pyramidal convolution (PyConv), which is capable of
processing the input at multiple filter scales. PyConv contains a pyramid of
kernels, where each level involves different types of filters with varying size
and depth, which are able to capture different levels of details in the scene.
On top of these improved recognition capabilities, PyConv is also efficient
and, with our formulation, it does not increase the computational cost and
parameters compared to standard convolution. Moreover, it is very flexible and
extensible, providing a large space of potential network architectures for
different applications. PyConv has the potential to impact nearly every
computer vision task and, in this work, we present different architectures
based on PyConv for four main tasks on visual recognition: image
classification, video action classification/recognition, object detection and
semantic image segmentation/parsing. Our approach shows significant
improvements over all these core tasks in comparison with the baselines. For
instance, on image recognition, our 50-layers network outperforms in terms of
recognition performance on ImageNet dataset its counterpart baseline ResNet
with 152 layers, while having 2.39 times less parameters, 2.52 times lower
computational complexity and more than 3 times less layers. On image
segmentation, our novel framework sets a new state-of-the-art on the
challenging ADE20K benchmark for scene parsing. Code is available at:
https://github.com/iduta/pyconv","['cs.CV', 'cs.LG', 'eess.IV']"
Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift,"Deep convolutional neural networks (DCNNs) have contributed many
breakthroughs in segmentation tasks, especially in the field of medical
imaging. However, \textit{domain shift} and \textit{corrupted annotations},
which are two common problems in medical imaging, dramatically degrade the
performance of DCNNs in practice. In this paper, we propose a novel robust
cross-denoising framework using two peer networks to address domain shift and
corrupted label problems with a peer-review strategy. Specifically, each
network performs as a mentor, mutually supervised to learn from reliable
samples selected by the peer network to combat with corrupted labels. In
addition, a noise-tolerant loss is proposed to encourage the network to capture
the key location and filter the discrepancy under various noise-contaminant
labels. To further reduce the accumulated error, we introduce a
class-imbalanced cross learning using most confident predictions at the
class-level. Experimental results on REFUGE and Drishti-GS datasets for optic
disc (OD) and optic cup (OC) segmentation demonstrate the superior performance
of our proposed approach to the state-of-the-art methods.","['cs.CV', 'cs.LG', 'eess.IV']"
Cardiac Segmentation with Strong Anatomical Guarantees,"Convolutional neural networks (CNN) have had unprecedented success in medical
imaging and, in particular, in medical image segmentation. However, despite the
fact that segmentation results are closer than ever to the inter-expert
variability, CNNs are not immune to producing anatomically inaccurate
segmentations, even when built upon a shape prior. In this paper, we present a
framework for producing cardiac image segmentation maps that are guaranteed to
respect pre-defined anatomical criteria, while remaining within the
inter-expert variability. The idea behind our method is to use a well-trained
CNN, have it process cardiac images, identify the anatomically implausible
results and warp these results toward the closest anatomically valid cardiac
shape. This warping procedure is carried out with a constrained variational
autoencoder (cVAE) trained to learn a representation of valid cardiac shapes
through a smooth, yet constrained, latent space. With this cVAE, we can project
any implausible shape into the cardiac latent space and steer it toward the
closest correct shape. We tested our framework on short-axis MRI as well as
apical two and four-chamber view ultrasound images, two modalities for which
cardiac shapes are drastically different. With our method, CNNs can now produce
results that are both within the inter-expert variability and always
anatomically plausible without having to rely on a shape prior.","['cs.CV', 'cs.LG', 'eess.IV']"
Spectral Image Segmentation with Global Appearance Modeling,"We introduce a new spectral method for image segmentation that incorporates
long range relationships for global appearance modeling. The approach combines
two different graphs, one is a sparse graph that captures spatial relationships
between nearby pixels and another is a dense graph that captures pairwise
similarity between all pairs of pixels. We extend the spectral method for
Normalized Cuts to this setting by combining the transition matrices of Markov
chains associated with each graph. We also derive an efficient method that uses
importance sampling for sparsifying the dense graph of appearance
relationships. This leads to a practical algorithm for segmenting
high-resolution images. The resulting method can segment challenging images
without any filtering or pre-processing.","['cs.CV', 'cs.LG', 'eess.IV', 'I.4; I.5']"
Searching Learning Strategy with Reinforcement Learning for 3D Medical Image Segmentation,"Deep neural network (DNN) based approaches have been widely investigated and
deployed in medical image analysis. For example, fully convolutional neural
networks (FCN) achieve the state-of-the-art performance in several applications
of 2D/3D medical image segmentation. Even the baseline neural network models
(U-Net, V-Net, etc.) have been proven to be very effective and efficient when
the training process is set up properly. Nevertheless, to fully exploit the
potentials of neural networks, we propose an automated searching approach for
the optimal training strategy with reinforcement learning. The proposed
approach can be utilized for tuning hyper-parameters, and selecting necessary
data augmentation with certain probabilities. The proposed approach is
validated on several tasks of 3D medical image segmentation. The performance of
the baseline model is boosted after searching, and it can achieve comparable
accuracy to other manually-tuned state-of-the-art segmentation approaches.",['cs.CV']
Learning pose variations within shape population by constrained mixtures of factor analyzers,"Mining and learning the shape variability of underlying population has
benefited the applications including parametric shape modeling, 3D animation,
and image segmentation. The current statistical shape modeling method works
well on learning unstructured shape variations without obvious pose changes
(relative rotations of the body parts). Studying the pose variations within a
shape population involves segmenting the shapes into different articulated
parts and learning the transformations of the segmented parts. This paper
formulates the pose learning problem as mixtures of factor analyzers. The
segmentation is obtained by components posterior probabilities and the
rotations in pose variations are learned by the factor loading matrices. To
guarantee that the factor loading matrices are composed by rotation matrices,
constraints are imposed and the corresponding closed form optimal solution is
derived. Based on the proposed method, the pose variations are automatically
learned from the given shape populations. The method is applied in motion
animation where new poses are generated by interpolating the existing poses in
the training set. The obtained results are smooth and realistic.","['cs.CV', 'stat.ML']"
Multi-organ Segmentation over Partially Labeled Datasets with Multi-scale Feature Abstraction,"Shortage of fully annotated datasets has been a limiting factor in developing
deep learning based image segmentation algorithms and the problem becomes more
pronounced in multi-organ segmentation. In this paper, we propose a unified
training strategy that enables a novel multi-scale deep neural network to be
trained on multiple partially labeled datasets for multi-organ segmentation. In
addition, a new network architecture for multi-scale feature abstraction is
proposed to integrate pyramid input and feature analysis into a U-shape pyramid
structure. To bridge the semantic gap caused by directly merging features from
different scales, an equal convolutional depth mechanism is introduced.
Furthermore, we employ a deep supervision mechanism to refine the outputs in
different scales. To fully leverage the segmentation features from all the
scales, we design an adaptive weighting layer to fuse the outputs in an
automatic fashion. All these mechanisms together are integrated into a Pyramid
Input Pyramid Output Feature Abstraction Network (PIPO-FAN). Our proposed
method was evaluated on four publicly available datasets, including BTCV, LiTS,
KiTS and Spleen, where very promising performance has been achieved. The source
code of this work is publicly shared at https://github.com/DIAL-RPI/PIPO-FAN
for others to easily reproduce the work and build their own models with the
introduced mechanisms.",['cs.CV']
Elastic Boundary Projection for 3D Medical Image Segmentation,"We focus on an important yet challenging problem: using a 2D deep network to
deal with 3D segmentation for medical image analysis. Existing approaches
either applied multi-view planar (2D) networks or directly used volumetric (3D)
networks for this purpose, but both of them are not ideal: 2D networks cannot
capture 3D contexts effectively, and 3D networks are both memory-consuming and
less stable arguably due to the lack of pre-trained models.
  In this paper, we bridge the gap between 2D and 3D using a novel approach
named Elastic Boundary Projection (EBP). The key observation is that, although
the object is a 3D volume, what we really need in segmentation is to find its
boundary which is a 2D surface. Therefore, we place a number of pivot points in
the 3D space, and for each pivot, we determine its distance to the object
boundary along a dense set of directions. This creates an elastic shell around
each pivot which is initialized as a perfect sphere. We train a 2D deep network
to determine whether each ending point falls within the object, and gradually
adjust the shell so that it gradually converges to the actual shape of the
boundary and thus achieves the goal of segmentation. EBP allows boundary-based
segmentation without cutting a 3D volume into slices or patches, which stands
out from conventional 2D and 3D approaches. EBP achieves promising accuracy in
abdominal organ segmentation. Our code has been open-sourced
https://github.com/twni2016/Elastic-Boundary-Projection.",['cs.CV']
MSDU-net: A Multi-Scale Dilated U-net for Blur Detection,"Blur detection is the separation of blurred and clear regions of an image,
which is an important and challenging task in computer vision. In this work, we
regard blur detection as an image segmentation problem. Inspired by the success
of the U-net architecture for image segmentation, we design a Multi-Scale
Dilated convolutional neural network based on U-net, which we call MSDU-net.
The MSDU-net uses a group of multi-scale feature extractors with dilated
convolutions to extract texture information at different scales. The U-shape
architecture of the MSDU-net fuses the different-scale texture features and
generates a semantic feature which allows us to achieve better results on the
blur detection task. We show that using the MSDU-net we are able to outperform
other state of the art blur detection methods on two publicly available
benchmarks.",['cs.CV']
Bayesian optimization for modular black-box systems with switching costs,"Most existing black-box optimization methods assume that all variables in the
system being optimized have equal cost and can change freely at each iteration.
However, in many real world systems, inputs are passed through a sequence of
different operations or modules, making variables in earlier stages of
processing more costly to update. Such structure imposes a cost on switching
variables in early parts of a data processing pipeline. In this work, we
propose a new algorithm for switch cost-aware optimization called Lazy Modular
Bayesian Optimization (LaMBO). This method efficiently identifies the global
optimum while minimizing cost through a passive change of variables in early
modules. The method is theoretical grounded and achieves vanishing regret when
augmented with switching cost. We apply LaMBO to multiple synthetic functions
and a three-stage image segmentation pipeline used in a neuroscience
application, where we obtain promising improvements over prevailing cost-aware
Bayesian optimization algorithms. Our results demonstrate that LaMBO is an
effective strategy for black-box optimization that is capable of minimizing
switching costs in modular systems.","['cs.LG', 'stat.ML']"
Deep Semantic Segmentation of Natural and Medical Images: A Review,"The semantic image segmentation task consists of classifying each pixel of an
image into an instance, where each instance corresponds to a class. This task
is a part of the concept of scene understanding or better explaining the global
context of an image. In the medical image analysis domain, image segmentation
can be used for image-guided interventions, radiotherapy, or improved
radiological diagnostics. In this review, we categorize the leading deep
learning-based medical and non-medical image segmentation solutions into six
main groups of deep architectural, data synthesis-based, loss function-based,
sequenced models, weakly supervised, and multi-task methods and provide a
comprehensive review of the contributions in each of these groups. Further, for
each group, we analyze each variant of these groups and discuss the limitations
of the current approaches and present potential future research directions for
semantic image segmentation.","['cs.CV', 'cs.LG', 'eess.IV']"
ACNN: a Full Resolution DCNN for Medical Image Segmentation,"Deep Convolutional Neural Networks (DCNNs) are used extensively in medical
image segmentation and hence 3D navigation for robot-assisted Minimally
Invasive Surgeries (MISs). However, current DCNNs usually use down sampling
layers for increasing the receptive field and gaining abstract semantic
information. These down sampling layers decrease the spatial dimension of
feature maps, which can be detrimental to image segmentation. Atrous
convolution is an alternative for the down sampling layer. It increases the
receptive field whilst maintains the spatial dimension of feature maps. In this
paper, a method for effective atrous rate setting is proposed to achieve the
largest and fully-covered receptive field with a minimum number of atrous
convolutional layers. Furthermore, a new and full resolution DCNN - Atrous
Convolutional Neural Network (ACNN), which incorporates cascaded atrous
II-blocks, residual learning and Instance Normalization (IN) is proposed.
Application results of the proposed ACNN to Magnetic Resonance Imaging (MRI)
and Computed Tomography (CT) image segmentation demonstrate that the proposed
ACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net
and Deeplabv3+, but with reduced trainable parameters.","['cs.LG', 'stat.ML']"
Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks,"Transfer learning is widely used for training machine learning models. Here,
we study the role of transfer learning for training fully convolutional
networks (FCNs) for medical image segmentation. Our experiments show that
although transfer learning reduces the training time on the target task, the
improvement in segmentation accuracy is highly task/data-dependent. Larger
improvements in accuracy are observed when the segmentation task is more
challenging and the target training data is smaller. We observe that
convolutional filters of an FCN change little during training for medical image
segmentation, and still look random at convergence. We further show that quite
accurate FCNs can be built by freezing the encoder section of the network at
random values and only training the decoder section. At least for medical image
segmentation, this finding challenges the common belief that the encoder
section needs to learn data/task-specific representations. We examine the
evolution of FCN representations to gain a better insight into the effects of
transfer learning on the training dynamics. Our analysis shows that although
FCNs trained via transfer learning learn different representations than FCNs
trained with random initialization, the variability among FCNs trained via
transfer learning can be as high as that among FCNs trained with random
initialization. Moreover, feature reuse is not restricted to the early encoder
layers; rather, it can be more significant in deeper layers. These findings
offer new insights and suggest alternative ways of training FCNs for medical
image segmentation.","['cs.CV', 'cs.LG', 'eess.IV']"
Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation,"Image segmentation is a fundamental vision task and a crucial step for many
applications. In this paper, we propose a fast image segmentation method based
on a novel super boundary-to-pixel direction (super-BPD) and a customized
segmentation algorithm with super-BPD. Precisely, we define BPD on each pixel
as a two-dimensional unit vector pointing from its nearest boundary to the
pixel. In the BPD, nearby pixels from different regions have opposite
directions departing from each other, and adjacent pixels in the same region
have directions pointing to the other or each other (i.e., around medial
points). We make use of such property to partition an image into super-BPDs,
which are novel informative superpixels with robust direction similarity for
fast grouping into segmentation regions. Extensive experimental results on
BSDS500 and Pascal Context demonstrate the accuracy and efficency of the
proposed super-BPD in segmenting images. In practice, the proposed super-BPD
achieves comparable or superior performance with MCG while running at ~25fps
vs. 0.07fps. Super-BPD also exhibits a noteworthy transferability to unseen
scenes. The code is publicly available at
https://github.com/JianqiangWan/Super-BPD.",['cs.CV']
WaveSNet: Wavelet Integrated Deep Networks for Image Segmentation,"In deep networks, the lost data details significantly degrade the
performances of image segmentation. In this paper, we propose to apply Discrete
Wavelet Transform (DWT) to extract the data details during feature map
down-sampling, and adopt Inverse DWT (IDWT) with the extracted details during
the up-sampling to recover the details. We firstly transform DWT/IDWT as
general network layers, which are applicable to 1D/2D/3D data and various
wavelets like Haar, Cohen, and Daubechies, etc. Then, we design wavelet
integrated deep networks for image segmentation (WaveSNets) based on various
architectures, including U-Net, SegNet, and DeepLabv3+. Due to the
effectiveness of the DWT/IDWT in processing data details, experimental results
on CamVid, Pascal VOC, and Cityscapes show that our WaveSNets achieve better
segmentation performances than their vanilla versions.",['cs.CV']
Evolution of Image Segmentation using Deep Convolutional Neural Network: A Survey,"From the autonomous car driving to medical diagnosis, the requirement of the
task of image segmentation is everywhere. Segmentation of an image is one of
the indispensable tasks in computer vision. This task is comparatively
complicated than other vision tasks as it needs low-level spatial information.
Basically, image segmentation can be of two types: semantic segmentation and
instance segmentation. The combined version of these two basic tasks is known
as panoptic segmentation. In the recent era, the success of deep convolutional
neural networks (CNN) has influenced the field of segmentation greatly and gave
us various successful models to date. In this survey, we are going to take a
glance at the evolution of both semantic and instance segmentation work based
on CNN. We have also specified comparative architectural details of some
state-of-the-art models and discuss their training details to present a lucid
understanding of hyper-parameter tuning of those models. We have also drawn a
comparison among the performance of those models on different datasets. Lastly,
we have given a glimpse of some state-of-the-art panoptic segmentation models.",['cs.CV']
Deep Learning-Based Automated Image Segmentation for Concrete Petrographic Analysis,"The standard petrography test method for measuring air voids in concrete
(ASTM C457) requires a meticulous and long examination of sample phase
composition under a stereomicroscope. The high expertise and specialized
equipment discourage this test for routine concrete quality control. Though the
task can be alleviated with the aid of color-based image segmentation,
additional surface color treatment is required. Recently, deep learning
algorithms using convolutional neural networks (CNN) have achieved
unprecedented segmentation performance on image testing benchmarks. In this
study, we investigated the feasibility of using CNN to conduct concrete
segmentation without the use of color treatment. The CNN demonstrated a strong
potential to process a wide range of concretes, including those not involved in
model training. The experimental results showed that CNN outperforms the
color-based segmentation by a considerable margin, and has comparable accuracy
to human experts. Furthermore, the segmentation time is reduced to mere
seconds.","['cs.CV', 'cs.CY', 'cs.LG']"
Improving Calibration and Out-of-Distribution Detection in Medical Image Segmentation with Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) have shown to be powerful medical image
segmentation models. In this study, we address some of the main unresolved
issues regarding these models. Specifically, training of these models on small
medical image datasets is still challenging, with many studies promoting
techniques such as transfer learning. Moreover, these models are infamous for
producing over-confident predictions and for failing silently when presented
with out-of-distribution (OOD) data at test time. In this paper, we advocate
for multi-task learning, i.e., training a single model on several different
datasets, spanning several different organs of interest and different imaging
modalities. We show that not only a single CNN learns to automatically
recognize the context and accurately segment the organ of interest in each
context, but also that such a joint model often has more accurate and
better-calibrated predictions than dedicated models trained separately on each
dataset. Our experiments show that multi-task learning can outperform transfer
learning in medical image segmentation tasks. For detecting OOD data, we
propose a method based on spectral analysis of CNN feature maps. We show that
different datasets, representing different imaging modalities and/or different
organs of interest, have distinct spectral signatures, which can be used to
identify whether or not a test image is similar to the images used to train a
model. We show that this approach is far more accurate than OOD detection based
on prediction uncertainty. The methods proposed in this paper contribute
significantly to improving the accuracy and reliability of CNN-based medical
image segmentation models.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
Traditional Method Inspired Deep Neural Network for Edge Detection,"Recently, Deep-Neural-Network (DNN) based edge prediction is progressing
fast. Although the DNN based schemes outperform the traditional edge detectors,
they have much higher computational complexity. It could be that the DNN based
edge detectors often adopt the neural net structures designed for high-level
computer vision tasks, such as image segmentation and object recognition. Edge
detection is a rather local and simple job, the over-complicated architecture
and massive parameters may be unnecessary. Therefore, we propose a traditional
method inspired framework to produce good edges with minimal complexity. We
simplify the network architecture to include Feature Extractor, Enrichment, and
Summarizer, which roughly correspond to gradient, low pass filter, and pixel
connection in the traditional edge detection schemes. The proposed structure
can effectively reduce the complexity and retain the edge prediction quality.
Our TIN2 (Traditional Inspired Network) model has an accuracy higher than the
recent BDCN2 (Bi-Directional Cascade Network) but with a smaller model.",['cs.CV']
Robust Image Segmentation Quality Assessment,"Deep learning based image segmentation methods have achieved great success,
even having human-level accuracy in some applications. However, due to the
black box nature of deep learning, the best method may fail in some situations.
Thus predicting segmentation quality without ground truth would be very crucial
especially in clinical practice. Recently, people proposed to train neural
networks to estimate the quality score by regression. Although it can achieve
promising prediction accuracy, the network suffers robustness problem, e.g. it
is vulnerable to adversarial attacks. In this paper, we propose to alleviate
this problem by utilizing the difference between the input image and the
reconstructed image, which is conditioned on the segmentation to be assessed,
to lower the chance to overfit to the undesired image features from the
original input image, and thus to increase the robustness. Results on ACDC17
dataset demonstrated our method is promising.",['cs.CV']
Learning to segment from misaligned and partial labels,"To extract information at scale, researchers increasingly apply semantic
segmentation techniques to remotely-sensed imagery. While fully-supervised
learning enables accurate pixel-wise segmentation, compiling the exhaustive
datasets required is often prohibitively expensive. As a result, many non-urban
settings lack the ground-truth needed for accurate segmentation. Existing open
source infrastructure data for these regions can be inexact and non-exhaustive.
Open source infrastructure annotations like OpenStreetMaps (OSM) are
representative of this issue: while OSM labels provide global insights to road
and building footprints, noisy and partial annotations limit the performance of
segmentation algorithms that learn from them. In this paper, we present a novel
and generalizable two-stage framework that enables improved pixel-wise image
segmentation given misaligned and missing annotations. First, we introduce the
Alignment Correction Network to rectify incorrectly registered open source
labels. Next, we demonstrate a segmentation model -- the Pointer Segmentation
Network -- that uses corrected labels to predict infrastructure footprints
despite missing annotations. We test sequential performance on the AIRS
dataset, achieving a mean intersection-over-union score of 0.79; more
importantly, model performance remains stable as we decrease the fraction of
annotations present. We demonstrate the transferability of our method to lower
quality data, by applying the Alignment Correction Network to OSM labels to
correct building footprints; we also demonstrate the accuracy of the Pointer
Segmentation Network in predicting cropland boundaries in California from
medium resolution data. Overall, our methodology is robust for multiple
applications with varied amounts of training data present, thus offering a
method to extract reliable information from noisy, partial data.","['cs.CV', 'cs.CY', 'I.4.6; J.2']"
Multi-task deep learning for image segmentation using recursive approximation tasks,"Fully supervised deep neural networks for segmentation usually require a
massive amount of pixel-level labels which are manually expensive to create. In
this work, we develop a multi-task learning method to relax this constraint. We
regard the segmentation problem as a sequence of approximation subproblems that
are recursively defined and in increasing levels of approximation accuracy. The
subproblems are handled by a framework that consists of 1) a segmentation task
that learns from pixel-level ground truth segmentation masks of a small
fraction of the images, 2) a recursive approximation task that conducts partial
object regions learning and data-driven mask evolution starting from partial
masks of each object instance, and 3) other problem oriented auxiliary tasks
that are trained with sparse annotations and promote the learning of dedicated
features. Most training images are only labeled by (rough) partial masks, which
do not contain exact object boundaries, rather than by their full segmentation
masks. During the training phase, the approximation task learns the statistics
of these partial masks, and the partial regions are recursively increased
towards object boundaries aided by the learned information from the
segmentation task in a fully data-driven fashion. The network is trained on an
extremely small amount of precisely segmented images and a large set of coarse
labels. Annotations can thus be obtained in a cheap way. We demonstrate the
efficiency of our approach in three applications with microscopy images and
ultrasound images.","['cs.CV', 'cs.LG', 'eess.IV']"
Gleason Grading of Histology Prostate Images through Semantic Segmentation via Residual U-Net,"Worldwide, prostate cancer is one of the main cancers affecting men. The
final diagnosis of prostate cancer is based on the visual detection of Gleason
patterns in prostate biopsy by pathologists. Computer-aided-diagnosis systems
allow to delineate and classify the cancerous patterns in the tissue via
computer-vision algorithms in order to support the physicians' task. The
methodological core of this work is a U-Net convolutional neural network for
image segmentation modified with residual blocks able to segment cancerous
tissue according to the full Gleason system. This model outperforms other
well-known architectures, and reaches a pixel-level Cohen's quadratic Kappa of
0.52, at the level of previous image-level works in the literature, but
providing also a detailed localisation of the patterns.","['cs.CV', 'eess.IV']"
A New Validity Index for Fuzzy-Possibilistic C-Means Clustering,"In some complicated datasets, due to the presence of noisy data points and
outliers, cluster validity indices can give conflicting results in determining
the optimal number of clusters. This paper presents a new validity index for
fuzzy-possibilistic c-means clustering called Fuzzy-Possibilistic (FP) index,
which works well in the presence of clusters that vary in shape and density.
Moreover, FPCM like most of the clustering algorithms is susceptible to some
initial parameters. In this regard, in addition to the number of clusters, FPCM
requires a priori selection of the degree of fuzziness and the degree of
typicality. Therefore, we presented an efficient procedure for determining
their optimal values. The proposed approach has been evaluated using several
synthetic and real-world datasets. Final computational results demonstrate the
capabilities and reliability of the proposed approach compared with several
well-known fuzzy validity indices in the literature. Furthermore, to clarify
the ability of the proposed method in real applications, the proposed method is
implemented in microarray gene expression data clustering and medical image
segmentation.","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML', 'I.5.1; I.2.1; I.5.3; J.3; G.1.6']"
Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation,"Convex Shapes (CS) are common priors for optic disc and cup segmentation in
eye fundus images. It is important to design proper techniques to represent
convex shapes. So far, it is still a problem to guarantee that the output
objects from a Deep Neural Convolution Networks (DCNN) are convex shapes. In
this work, we propose a technique which can be easily integrated into the
commonly used DCNNs for image segmentation and guarantee that outputs are
convex shapes. This method is flexible and it can handle multiple objects and
allow some of the objects to be convex. Our method is based on the dual
representation of the sigmoid activation function in DCNNs. In the dual space,
the convex shape prior can be guaranteed by a simple quadratic constraint on a
binary representation of the shapes. Moreover, our method can also integrate
spatial regularization and some other shape prior using a soft thresholding
dynamics (STD) method. The regularization can make the boundary curves of the
segmentation objects to be simultaneously smooth and convex. We design a very
stable active set projection algorithm to numerically solve our model. This
algorithm can form a new plug-and-play DCNN layer called CS-STD whose outputs
must be a nearly binary segmentation of convex objects. In the CS-STD block,
the convexity information can be propagated to guide the DCNN in both forward
and backward propagation during training and prediction process. As an
application example, we apply the convexity prior layer to the retinal fundus
images segmentation by taking the popular DeepLabV3+ as a backbone network.
Experimental results on several public datasets show that our method is
efficient and outperforms the classical DCNN segmentation methods.",['cs.CV']
ECG-DelNet: Delineation of Ambulatory Electrocardiograms with Mixed Quality Labeling Using Neural Networks,"Electrocardiogram (ECG) detection and delineation are key steps for numerous
tasks in clinical practice, as ECG is the most performed non-invasive test for
assessing cardiac condition. State-of-the-art algorithms employ digital signal
processing (DSP), which require laborious rule adaptation to new morphologies.
In contrast, deep learning (DL) algorithms, especially for classification, are
gaining weight in academic and industrial settings. However, the lack of model
explainability and small databases hinder their applicability. We demonstrate
DL can be successfully applied to low interpretative tasks by embedding ECG
detection and delineation onto a segmentation framework. For this purpose, we
adapted and validated the most used neural network architecture for image
segmentation, the U-Net, to one-dimensional data. The model was trained using
PhysioNet's QT database, comprised of 105 ambulatory ECG recordings, for
single- and multi-lead scenarios. To alleviate data scarcity, data
regularization techniques such as pre-training with low-quality data labels,
performing ECG-based data augmentation and applying strong model regularizers
to the architecture were attempted. Other variations in the model's capacity
(U-Net's depth and width), alongside the application of state-of-the-art
additions, were evaluated. These variations were exhaustively validated in a
5-fold cross-validation manner. The best performing configuration reached
precisions of 90.12%, 99.14% and 98.25% and recalls of 98.73%, 99.94% and
99.88% for the P, QRS and T waves, respectively, on par with DSP-based
approaches. Despite being a data-hungry technique trained on a small dataset,
DL-based approaches demonstrate to be a viable alternative to traditional
DSP-based ECG processing techniques.","['cs.LG', 'eess.SP', 'stat.ML']"
Transformation Consistent Self-ensembling Model for Semi-supervised Medical Image Segmentation,"Deep convolutional neural networks have achieved remarkable progress on a
variety of medical image computing tasks. A common problem when applying
supervised deep learning methods to medical images is the lack of labeled data,
which is very expensive and time-consuming to be collected. In this paper, we
present a novel semi-supervised method for medical image segmentation, where
the network is optimized by the weighted combination of a common supervised
loss for labeled inputs only and a regularization loss for both labeled and
unlabeled data. To utilize the unlabeled data, our method encourages the
consistent predictions of the network-in-training for the same input under
different regularizations. Aiming for the semi-supervised segmentation problem,
we enhance the effect of regularization for pixel-level predictions by
introducing a transformation, including rotation and flipping, consistent
scheme in our self-ensembling model. With the aim of semi-supervised
segmentation tasks, we introduce a transformation consistent strategy in our
self-ensembling model to enhance the regularization effect for pixel-level
predictions. We have extensively validated the proposed semi-supervised method
on three typical yet challenging medical image segmentation tasks: (i) skin
lesion segmentation from dermoscopy images on International Skin Imaging
Collaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundus
images on Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liver
segmentation from volumetric CT scans on Liver Tumor Segmentation Challenge
(LiTS) dataset. Compared to the state-of-the-arts, our proposed method shows
superior segmentation performance on challenging 2D/3D medical images,
demonstrating the effectiveness of our semi-supervised method for medical image
segmentation.",['cs.CV']
Lake Ice Monitoring with Webcams and Crowd-Sourced Images,"Lake ice is a strong climate indicator and has been recognised as part of the
Essential Climate Variables (ECV) by the Global Climate Observing System
(GCOS). The dynamics of freezing and thawing, and possible shifts of freezing
patterns over time, can help in understanding the local and global climate
systems. One way to acquire the spatio-temporal information about lake ice
formation, independent of clouds, is to analyse webcam images. This paper
intends to move towards a universal model for monitoring lake ice with freely
available webcam data. We demonstrate good performance, including the ability
to generalise across different winters and different lakes, with a
state-of-the-art Convolutional Neural Network (CNN) model for semantic image
segmentation, Deeplab v3+. Moreover, we design a variant of that model, termed
Deep-U-Lab, which predicts sharper, more correct segmentation boundaries. We
have tested the model's ability to generalise with data from multiple camera
views and two different winters. On average, it achieves
intersection-over-union (IoU) values of ~71% across different cameras and ~69%
across different winters, greatly outperforming prior work. Going even further,
we show that the model even achieves 60% IoU on arbitrary images scraped from
photo-sharing web sites. As part of the work, we introduce a new benchmark
dataset of webcam images, Photi-LakeIce, from multiple cameras and two
different winters, along with pixel-wise ground truth annotations.","['cs.CV', 'eess.IV']"
Meta-Learning Initializations for Image Segmentation,"We extend first-order model agnostic meta-learning algorithms (including
FOMAML and Reptile) to image segmentation, present a novel neural network
architecture built for fast learning which we call EfficientLab, and leverage a
formal definition of the test error of meta-learning algorithms to decrease
error on out of distribution tasks. We show state of the art results on the
FSS-1000 dataset by meta-training EfficientLab with FOMAML and using Bayesian
optimization to infer the optimal test-time adaptation routine hyperparameters.
We also construct a small benchmark dataset, FP-k, for the empirical study of
how meta-learning systems perform in both few- and many-shot settings. On the
FP-k dataset, we show that meta-learned initializations provide value for
canonical few-shot image segmentation but their performance is quickly matched
by conventional transfer learning with performance being equal beyond 10
labeled examples. Our code, meta-learned model, and the FP-k dataset are
available at https://github.com/ml4ai/mliis .","['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']"
Partly Supervised Multitask Learning,"Semi-supervised learning has recently been attracting attention as an
alternative to fully supervised models that require large pools of labeled
data. Moreover, optimizing a model for multiple tasks can provide better
generalizability than single-task learning. Leveraging self-supervision and
adversarial training, we propose a novel general purpose semi-supervised,
multiple-task model---namely, self-supervised, semi-supervised, multitask
learning (S$^4$MTL)---for accomplishing two important tasks in medical imaging,
segmentation and diagnostic classification. Experimental results on chest and
spine X-ray datasets suggest that our S$^4$MTL model significantly outperforms
semi-supervised single task, semi/fully-supervised multitask, and
fully-supervised single task models, even with a 50\% reduction of class and
segmentation labels. We hypothesize that our proposed model can be effective in
tackling limited annotation problems for joint training, not only in medical
imaging domains, but also for general-purpose vision tasks.",['cs.CV']
SimPropNet: Improved Similarity Propagation for Few-shot Image Segmentation,"Few-shot segmentation (FSS) methods perform image segmentation for a
particular object class in a target (query) image, using a small set of
(support) image-mask pairs. Recent deep neural network based FSS methods
leverage high-dimensional feature similarity between the foreground features of
the support images and the query image features. In this work, we demonstrate
gaps in the utilization of this similarity information in existing methods, and
present a framework - SimPropNet, to bridge those gaps. We propose to jointly
predict the support and query masks to force the support features to share
characteristics with the query features. We also propose to utilize
similarities in the background regions of the query and support images using a
novel foreground-background attentive fusion mechanism. Our method achieves
state-of-the-art results for one-shot and five-shot segmentation on the
PASCAL-5i dataset. The paper includes detailed analysis and ablation studies
for the proposed improvements and quantitative comparisons with contemporary
methods.",['cs.CV']
ACCL: Adversarial constrained-CNN loss for weakly supervised medical image segmentation,"We propose adversarial constrained-CNN loss, a new paradigm of
constrained-CNN loss methods, for weakly supervised medical image segmentation.
In the new paradigm, prior knowledge is encoded and depicted by reference
masks, and is further employed to impose constraints on segmentation outputs
through adversarial learning with reference masks. Unlike pseudo label methods
for weakly supervised segmentation, such reference masks are used to train a
discriminator rather than a segmentation network, and thus are not required to
be paired with specific images. Our new paradigm not only greatly facilitates
imposing prior knowledge on network's outputs, but also provides stronger and
higher-order constraints, i.e., distribution approximation, through adversarial
learning. Extensive experiments involving different medical modalities,
different anatomical structures, different topologies of the object of
interest, different levels of prior knowledge and weakly supervised annotations
with different annotation ratios is conducted to evaluate our ACCL method.
Consistently superior segmentation results over the size constrained-CNN loss
method have been achieved, some of which are close to the results of full
supervision, thus fully verifying the effectiveness and generalization of our
method. Specifically, we report an average Dice score of 75.4% with an average
annotation ratio of 0.65%, surpassing the prior art, i.e., the size
constrained-CNN loss method, by a large margin of 11.4%. Our codes are made
publicly available at https://github.com/PengyiZhang/ACCL.",['cs.CV']
Importance Driven Continual Learning for Segmentation Across Domains,"The ability of neural networks to continuously learn and adapt to new tasks
while retaining prior knowledge is crucial for many applications. However,
current neural networks tend to forget previously learned tasks when trained on
new ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of
Continual Learning (CL) is to alleviate this problem, which is particularly
relevant for medical applications, where it may not be feasible to store and
access previously used sensitive patient data. In this work, we propose a
Continual Learning approach for brain segmentation, where a single network is
consecutively trained on samples from different domains. We build upon an
importance driven approach and adapt it for medical image segmentation.
Particularly, we introduce learning rate regularization to prevent the loss of
the network's knowledge. Our results demonstrate that directly restricting the
adaptation of important network parameters clearly reduces Catastrophic
Forgetting for segmentation across domains.",['cs.CV']
Feedback U-net for Cell Image Segmentation,"Human brain is a layered structure, and performs not only a feedforward
process from a lower layer to an upper layer but also a feedback process from
an upper layer to a lower layer. The layer is a collection of neurons, and
neural network is a mathematical model of the function of neurons. Although
neural network imitates the human brain, everyone uses only feedforward process
from the lower layer to the upper layer, and feedback process from the upper
layer to the lower layer is not used. Therefore, in this paper, we propose
Feedback U-Net using Convolutional LSTM which is the segmentation method using
Convolutional LSTM and feedback process. The output of U-net gave feedback to
the input, and the second round is performed. By using Convolutional LSTM, the
features in the second round are extracted based on the features acquired in
the first round. On both of the Drosophila cell image and Mouse cell image
datasets, our method outperformed conventional U-Net which uses only
feedforward process.",['cs.CV']
A Spatially Constrained Deep Convolutional Neural Network for Nerve Fiber Segmentation in Corneal Confocal Microscopic Images using Inaccurate Annotations,"Semantic image segmentation is one of the most important tasks in medical
image analysis. Most state-of-the-art deep learning methods require a large
number of accurately annotated examples for model training. However, accurate
annotation is difficult to obtain especially in medical applications. In this
paper, we propose a spatially constrained deep convolutional neural network
(DCNN) to achieve smooth and robust image segmentation using inaccurately
annotated labels for training. In our proposed method, image segmentation is
formulated as a graph optimization problem that is solved by a DCNN model
learning process. The cost function to be optimized consists of a unary term
that is calculated by cross entropy measurement and a pairwise term that is
based on enforcing a local label consistency. The proposed method has been
evaluated based on corneal confocal microscopic (CCM) images for nerve fiber
segmentation, where accurate annotations are extremely difficult to be
obtained. Based on both the quantitative result of a synthetic dataset and
qualitative assessment of a real dataset, the proposed method has achieved
superior performance in producing high quality segmentation results even with
inaccurate labels for training.",['cs.CV']
LSM: Learning Subspace Minimization for Low-level Vision,"We study the energy minimization problem in low-level vision tasks from a
novel perspective. We replace the heuristic regularization term with a
learnable subspace constraint, and preserve the data term to exploit domain
knowledge derived from the first principle of a task. This learning subspace
minimization (LSM) framework unifies the network structures and the parameters
for many low-level vision tasks, which allows us to train a single network for
multiple tasks simultaneously with completely shared parameters, and even
generalizes the trained network to an unseen task as long as its data term can
be formulated. We demonstrate our LSM framework on four low-level tasks
including interactive image segmentation, video segmentation, stereo matching,
and optical flow, and validate the network on various datasets. The experiments
show that the proposed LSM generates state-of-the-art results with smaller
model size, faster training convergence, and real-time inference.",['cs.CV']
C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation,"3D convolution neural networks (CNN) have been proved very successful in
parsing organs or tumours in 3D medical images, but it remains sophisticated
and time-consuming to choose or design proper 3D networks given different task
contexts. Recently, Neural Architecture Search (NAS) is proposed to solve this
problem by searching for the best network architecture automatically. However,
the inconsistency between search stage and deployment stage often exists in NAS
algorithms due to memory constraints and large search space, which could become
more serious when applying NAS to some memory and time consuming tasks, such as
3D medical image segmentation. In this paper, we propose coarse-to-fine neural
architecture search (C2FNAS) to automatically search a 3D segmentation network
from scratch without inconsistency on network size or input size. Specifically,
we divide the search procedure into two stages: 1) the coarse stage, where we
search the macro-level topology of the network, i.e. how each convolution
module is connected to other modules; 2) the fine stage, where we search at
micro-level for operations in each cell based on previous searched macro-level
topology. The coarse-to-fine manner divides the search procedure into two
consecutive stages and meanwhile resolves the inconsistency. We evaluate our
method on 10 public datasets from Medical Segmentation Decalthon (MSD)
challenge, and achieve state-of-the-art performance with the network searched
using one dataset, which demonstrates the effectiveness and generalization of
our searched models.",['cs.CV']
Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means,"Segmentation partitions an image into different regions containing pixels
with similar attributes. A standard non-contextual variant of Fuzzy C-means
clustering algorithm (FCM), considering its simplicity is generally used in
image segmentation. Using FCM has its disadvantages like it is dependent on the
initial guess of the number of clusters and highly sensitive to noise.
Satisfactory visual segments cannot be obtained using FCM. Particle Swarm
Optimization (PSO) belongs to the class of evolutionary algorithms and has good
convergence speed and fewer parameters compared to Genetic Algorithms (GAs). An
optimized version of PSO can be combined with FCM to act as a proper
initializer for the algorithm thereby reducing its sensitivity to initial
guess. A hybrid PSO algorithm named Adaptive Particle Swarm Optimization (APSO)
which improves in the calculation of various hyper parameters like inertia
weight, learning factors over standard PSO, using insights from swarm
behaviour, leading to improvement in cluster quality can be used. This paper
presents a new image segmentation algorithm called Adaptive Particle Swarm
Optimization and Fuzzy C-means Clustering Algorithm (APSOF), which is based on
Adaptive Particle Swarm Optimization (APSO) and Fuzzy C-means clustering.
Experimental results show that APSOF algorithm has edge over FCM in correctly
identifying the optimum cluster centers, there by leading to accurate
classification of the image pixels. Hence, APSOF algorithm has superior
performance in comparison with classic Particle Swarm Optimization (PSO) and
Fuzzy C-means clustering algorithm (FCM) for image segmentation.","['cs.CV', '68U10', 'I.4.6']"
A generic ensemble based deep convolutional neural network for semi-supervised medical image segmentation,"Deep learning based image segmentation has achieved the state-of-the-art
performance in many medical applications such as lesion quantification, organ
detection, etc. However, most of the methods rely on supervised learning, which
require a large set of high-quality labeled data. Data annotation is generally
an extremely time-consuming process. To address this problem, we propose a
generic semi-supervised learning framework for image segmentation based on a
deep convolutional neural network (DCNN). An encoder-decoder based DCNN is
initially trained using a few annotated training samples. This initially
trained model is then copied into sub-models and improved iteratively using
random subsets of unlabeled data with pseudo labels generated from models
trained in the previous iteration. The number of sub-models is gradually
decreased to one in the final iteration. We evaluate the proposed method on a
public grand-challenge dataset for skin lesion segmentation. Our method is able
to significantly improve beyond fully supervised model learning by
incorporating unlabeled data.",['cs.CV']
Generator evaluator-selector net for panoptic image segmentation and splitting unfamiliar objects into parts,"In machine learning and other fields, suggesting a good solution to a problem
is usually a harder task than evaluating the quality of such a solution. This
asymmetry is the basis for a large number of selection oriented methods that
use a generator system to guess a set of solutions and an evaluator system to
rank and select the best solutions. This work examines the use of this approach
to the problem of panoptic image segmentation and class agnostic parts
segmentation. The generator/evaluator approach for this case consists of two
independent convolutional neural nets: a generator net that suggests variety
segments corresponding to objects, stuff and parts regions in the image, and an
evaluator net that chooses the best segments to be merged into the segmentation
map. The result is a trial and error evolutionary approach in which a generator
that guesses segments with low average accuracy, but with wide variability, can
still produce good results when coupled with an accurate evaluator. The
generator consists of a Pointer net that receives an image and a point in the
image, and predicts the region of the segment containing the point. Generating
and evaluating each segment separately is essential in this case since it
demands exponentially fewer guesses compared to a system that guesses and
evaluates the full segmentation map in each try. The classification of the
selected segments is done by an independent region-specific classification net.
This allows the segmentation to be class agnostic and hence, capable of
segmenting unfamiliar categories that were not part of the training set. The
method was examined on the COCO Panoptic segmentation benchmark and gave
results comparable to those of the basic semantic segmentation and Mask-RCNN
methods. In addition, the system was used for the task of splitting objects of
unseen classes (that did not appear in the training set) into parts.","['cs.CV', 'cs.LG']"
Mimic and Fool: A Task Agnostic Adversarial Attack,"At present, adversarial attacks are designed in a task-specific fashion.
However, for downstream computer vision tasks such as image captioning, image
segmentation etc., the current deep learning systems use an image classifier
like VGG16, ResNet50, Inception-v3 etc. as a feature extractor. Keeping this in
mind, we propose Mimic and Fool, a task agnostic adversarial attack. Given a
feature extractor, the proposed attack finds an adversarial image which can
mimic the image feature of the original image. This ensures that the two images
give the same (or similar) output regardless of the task. We randomly select
1000 MSCOCO validation images for experimentation. We perform experiments on
two image captioning models, Show and Tell, Show Attend and Tell and one VQA
model, namely, end-to-end neural module network (N2NMN). The proposed attack
achieves success rate of 74.0%, 81.0% and 87.1% for Show and Tell, Show Attend
and Tell and N2NMN respectively. We also propose a slight modification to our
attack to generate natural-looking adversarial images. In addition, we also
show the applicability of the proposed attack for invertible architecture.
Since Mimic and Fool only requires information about the feature extractor of
the model, it can be considered as a gray-box attack.",['cs.CV']
Semi-supervised few-shot learning for medical image segmentation,"Recent years have witnessed the great progress of deep neural networks on
semantic segmentation, particularly in medical imaging. Nevertheless, training
high-performing models require large amounts of pixel-level ground truth masks,
which can be prohibitive to obtain in the medical domain. Furthermore, training
such models in a low-data regime highly increases the risk of overfitting.
Recent attempts to alleviate the need for large annotated datasets have
developed training strategies under the few-shot learning paradigm, which
addresses this shortcoming by learning a novel class from only a few labeled
examples. In this context, a segmentation model is trained on episodes, which
represent different segmentation problems, each of them trained with a very
small labeled dataset. In this work, we propose a novel few-shot learning
framework for semantic segmentation, where unlabeled images are also made
available at each episode. To handle this new learning paradigm, we propose to
include surrogate tasks that can leverage very powerful supervisory signals
--derived from the data itself-- for semantic feature learning. We show that
including unlabeled surrogate tasks in the episodic training leads to more
powerful feature representations, which ultimately results in better
generability to unseen tasks. We demonstrate the efficiency of our method in
the task of skin lesion segmentation in two publicly available datasets.
Furthermore, our approach is general and model-agnostic, which can be combined
with different deep architectures.",['cs.CV']
CNN in CT Image Segmentation: Beyound Loss Function for Expoliting Ground Truth Images,"Exploiting more information from ground truth (GT) images now is a new
research direction for further improving CNN's performance in CT image
segmentation. Previous methods focus on devising the loss function for
fulfilling such a purpose. However, it is rather difficult to devise a general
and optimization-friendly loss function. We here present a novel and practical
method that exploits GT images beyond the loss function. Our insight is that
feature maps of two CNNs trained respectively on GT and CT images should be
similar on some metric space, because they both are used to describe the same
objects for the same purpose. We hence exploit GT images by enforcing such two
CNNs' feature maps to be consistent. We assess the proposed method on two data
sets, and compare its performance to several competitive methods. Extensive
experimental results show that the proposed method is effective, outperforming
all the compared methods.",['cs.CV']
Fair Latency-Aware Metric for real-time video segmentation networks,"As supervised semantic segmentation is reaching satisfying results, many
recent papers focused on making segmentation network architectures faster,
smaller and more efficient. In particular, studies often aim to reach the stage
to which they can claim to be ""real-time"". Achieving this goal is especially
relevant in the context of real-time video operations for autonomous vehicles
and robots, or medical imaging during surgery.
  The common metric used for assessing these methods is so far the same as the
ones used for image segmentation without time constraint: mean Intersection
over Union (mIoU). In this paper, we argue that this metric is not relevant
enough for real-time video as it does not take into account the processing time
(latency) of the network. We propose a similar but more relevant metric called
FLAME for video-segmentation networks, that compares the output segmentation of
the network with the ground truth segmentation of the current video frame at
the time when the network finishes the processing.
  We perform experiments to compare a few networks using this metric and
propose a simple addition to network training to enhance results according to
that metric.",['cs.CV']
Real-Time High-Performance Semantic Image Segmentation of Urban Street Scenes,"Deep Convolutional Neural Networks (DCNNs) have recently shown outstanding
performance in semantic image segmentation. However, state-of-the-art
DCNN-based semantic segmentation methods usually suffer from high computational
complexity due to the use of complex network architectures. This greatly limits
their applications in the real-world scenarios that require real-time
processing. In this paper, we propose a real-time high-performance DCNN-based
method for robust semantic segmentation of urban street scenes, which achieves
a good trade-off between accuracy and speed. Specifically, a Lightweight
Baseline Network with Atrous convolution and Attention (LBN-AA) is firstly used
as our baseline network to efficiently obtain dense feature maps. Then, the
Distinctive Atrous Spatial Pyramid Pooling (DASPP), which exploits the
different sizes of pooling operations to encode the rich and distinctive
semantic information, is developed to detect objects at multiple scales.
Meanwhile, a Spatial detail-Preserving Network (SPN) with shallow convolutional
layers is designed to generate high-resolution feature maps preserving the
detailed spatial information. Finally, a simple but practical Feature Fusion
Network (FFN) is used to effectively combine both shallow and deep features
from the semantic branch (DASPP) and the spatial branch (SPN), respectively.
Extensive experimental results show that the proposed method respectively
achieves the accuracy of 73.6% and 68.0% mean Intersection over Union (mIoU)
with the inference speed of 51.0 fps and 39.3 fps on the challenging Cityscapes
and CamVid test datasets (by only using a single NVIDIA TITAN X card). This
demonstrates that the proposed method offers excellent performance at the
real-time speed for semantic segmentation of urban street scenes.",['cs.CV']
C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds,"Flow-based generative models have highly desirable properties like exact
log-likelihood evaluation and exact latent-variable inference, however they are
still in their infancy and have not received as much attention as alternative
generative models. In this paper, we introduce C-Flow, a novel conditioning
scheme that brings normalizing flows to an entirely new scenario with great
possibilities for multi-modal data modeling. C-Flow is based on a parallel
sequence of invertible mappings in which a source flow guides the target flow
at every step, enabling fine-grained control over the generation process. We
also devise a new strategy to model unordered 3D point clouds that, in
combination with the conditioning scheme, makes it possible to address 3D
reconstruction from a single image and its inverse problem of rendering an
image given a point cloud. We demonstrate our conditioning method to be very
adaptable, being also applicable to image manipulation, style transfer and
multi-modal image-to-image mapping in a diversity of domains, including RGB
images, segmentation maps, and edge masks.",['cs.CV']
CaDIS: Cataract Dataset for Image Segmentation,"Video feedback provides a wealth of information about surgical procedures and
is the main sensory cue for surgeons. Scene understanding is crucial to
computer assisted interventions (CAI) and to post-operative analysis of the
surgical procedure. A fundamental building block of such capabilities is the
identification and localization of surgical instruments and anatomical
structures through semantic segmentation. Deep learning has advanced semantic
segmentation techniques in the recent years but is inherently reliant on the
availability of labeled datasets for model training. This paper introduces a
dataset for semantic segmentation of cataract surgery videos. The annotated
images are part of the publicly available CATARACTS challenge dataset. In
addition, we benchmark the performance of several state-of-the-art deep
learning models for semantic segmentation on the presented dataset. The dataset
is publicly available at https://cataracts.grand-challenge.org/CaDIS/ .",['cs.CV']
A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation,"The Know Your Customer (KYC) and Anti Money Laundering (AML) are worldwide
practices to online customer identification based on personal identification
documents, similarity and liveness checking, and proof of address. To answer
the basic regulation question: are you whom you say you are? The customer needs
to upload valid identification documents (ID). This task imposes some
computational challenges since these documents are diverse, may present
different and complex backgrounds, some occlusion, partial rotation, poor
quality, or damage. Advanced text and document segmentation algorithms were
used to process the ID images. In this context, we investigated a method based
on U-Net to detect the document edges and text regions in ID images. Besides
the promising results on image segmentation, the U-Net based approach is
computationally expensive for a real application, since the image segmentation
is a customer device task. We propose a model optimization based on Octave
Convolutions to qualify the method to situations where storage, processing, and
time resources are limited, such as in mobile and robotic applications. We
conducted the evaluation experiments in two new datasets CDPhotoDataset and
DTDDataset, which are composed of real ID images of Brazilian documents. Our
results showed that the proposed models are efficient to document segmentation
tasks and portable.","['cs.CV', 'eess.IV']"
Automated Design of Deep Learning Methods for Biomedical Image Segmentation,"Biomedical imaging is a driver of scientific discovery and core component of
medical care, currently stimulated by the field of deep learning. While
semantic segmentation algorithms enable 3D image analysis and quantification in
many applications, the design of respective specialised solutions is
non-trivial and highly dependent on dataset properties and hardware conditions.
We propose nnU-Net, a deep learning framework that condenses the current domain
knowledge and autonomously takes the key decisions required to transfer a basic
architecture to different datasets and segmentation tasks. Without manual
tuning, nnU-Net surpasses most specialised deep learning pipelines in 19 public
international competitions and sets a new state of the art in the majority of
the 49 tasks. The results demonstrate a vast hidden potential in the systematic
adaptation of deep learning methods to different datasets. We make nnU-Net
publicly available as an open-source tool that can effectively be used
out-of-the-box, rendering state of the art segmentation accessible to
non-experts and catalyzing scientific progress as a framework for automated
method design.",['cs.CV']
RSS-Net: Weakly-Supervised Multi-Class Semantic Segmentation with FMCW Radar,"This paper presents an efficient annotation procedure and an application
thereof to end-to-end, rich semantic segmentation of the sensed environment
using FMCW scanning radar. We advocate radar over the traditional sensors used
for this task as it operates at longer ranges and is substantially more robust
to adverse weather and illumination conditions. We avoid laborious manual
labelling by exploiting the largest radar-focused urban autonomy dataset
collected to date, correlating radar scans with RGB cameras and LiDAR sensors,
for which semantic segmentation is an already consolidated procedure. The
training procedure leverages a state-of-the-art natural image segmentation
system which is publicly available and as such, in contrast to previous
approaches, allows for the production of copious labels for the radar stream by
incorporating four camera and two LiDAR streams. Additionally, the losses are
computed taking into account labels to the radar sensor horizon by accumulating
LiDAR returns along a pose-chain ahead and behind of the current vehicle
position. Finally, we present the network with multi-channel radar scan inputs
in order to deal with ephemeral and dynamic scene objects.","['cs.CV', 'cs.RO']"
Improving land cover segmentation across satellites using domain adaptation,"Land use and land cover mapping are essential to various fields of study,
including forestry, agriculture, and urban management. Using earth observation
satellites both facilitate and accelerate the task. Lately, deep learning
methods have proven to be excellent at automating the mapping via semantic
image segmentation. However, because deep neural networks require large amounts
of labeled data, it is not easy to exploit the full potential of satellite
imagery. Additionally, the land cover tends to differ in appearance from one
region to another; therefore, having labeled data from one location does not
necessarily help in mapping others. Furthermore, satellite images come in
various multispectral bands (the bands could range from RGB to over twelve
bands). In this paper, we aim at using domain adaptation to solve the
aforementioned problems. We applied a well-performing domain adaptation
approach on datasets we have built using RGB images from Sentinel-2,
WorldView-2, and Pleiades-1 satellites with Corine Land Cover as ground-truth
labels. We have also used the DeepGlobe land cover dataset. Experiments show a
significant improvement over results obtained without the use of domain
adaptation. In some cases, an improvement of over 20% MIoU. At times it even
manages to correct errors in the ground-truth labels.",['cs.CV']
BiLingUNet: Image Segmentation by Modulating Top-Down and Bottom-Up Visual Processing with Referring Expressions,"We present BiLingUNet, a state-of-the-art model for image segmentation using
referring expressions. BiLingUNet uses language to customize visual filters and
outperforms approaches that concatenate a linguistic representation to the
visual input. We find that using language to modulate both bottom-up and
top-down visual processing works better than just making the top-down
processing language-conditional. We argue that common 1x1 language-conditional
filters cannot represent relational concepts and experimentally demonstrate
that wider filters work better. Our model achieves state-of-the-art performance
on four referring expression datasets.","['cs.CV', 'cs.CL', 'cs.LG']"
Deep Grouping Model for Unified Perceptual Parsing,"The perceptual-based grouping process produces a hierarchical and
compositional image representation that helps both human and machine vision
systems recognize heterogeneous visual concepts. Examples can be found in the
classical hierarchical superpixel segmentation or image parsing works. However,
the grouping process is largely overlooked in modern CNN-based image
segmentation networks due to many challenges, including the inherent
incompatibility between the grid-shaped CNN feature map and the
irregular-shaped perceptual grouping hierarchy. Overcoming these challenges, we
propose a deep grouping model (DGM) that tightly marries the two types of
representations and defines a bottom-up and a top-down process for feature
exchanging. When evaluating the model on the recent Broden+ dataset for the
unified perceptual parsing task, it achieves state-of-the-art results while
having a small computational overhead compared to other contextual-based
segmentation models. Furthermore, the DGM has better interpretability compared
with modern CNN methods.",['cs.CV']
CRNet: Cross-Reference Networks for Few-Shot Segmentation,"Over the past few years, state-of-the-art image segmentation algorithms are
based on deep convolutional neural networks. To render a deep network with the
ability to understand a concept, humans need to collect a large amount of
pixel-level annotated data to train the models, which is time-consuming and
tedious. Recently, few-shot segmentation is proposed to solve this problem.
Few-shot segmentation aims to learn a segmentation model that can be
generalized to novel classes with only a few training images. In this paper, we
propose a cross-reference network (CRNet) for few-shot segmentation. Unlike
previous works which only predict the mask in the query image, our proposed
model concurrently make predictions for both the support image and the query
image. With a cross-reference mechanism, our network can better find the
co-occurrent objects in the two images, thus helping the few-shot segmentation
task. We also develop a mask refinement module to recurrently refine the
prediction of the foreground regions. For the $k$-shot learning, we propose to
finetune parts of networks to take advantage of multiple labeled support
images. Experiments on the PASCAL VOC 2012 dataset show that our network
achieves state-of-the-art performance.",['cs.CV']
Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection,"We present a generalized and scalable method, called Gen-LaneNet, to detect
3D lanes from a single image. The method, inspired by the latest
state-of-the-art 3D-LaneNet, is a unified framework solving image encoding,
spatial transform of features and 3D lane prediction in a single network.
However, we propose unique designs for Gen-LaneNet in two folds. First, we
introduce a new geometry-guided lane anchor representation in a new coordinate
frame and apply a specific geometric transformation to directly calculate real
3D lane points from the network output. We demonstrate that aligning the lane
points with the underlying top-view features in the new coordinate frame is
critical towards a generalized method in handling unfamiliar scenes. Second, we
present a scalable two-stage framework that decouples the learning of image
segmentation subnetwork and geometry encoding subnetwork. Compared to
3D-LaneNet, the proposed Gen-LaneNet drastically reduces the amount of 3D lane
labels required to achieve a robust solution in real-world application.
Moreover, we release a new synthetic dataset and its construction strategy to
encourage the development and evaluation of 3D lane detection methods. In
experiments, we conduct extensive ablation study to substantiate the proposed
Gen-LaneNet significantly outperforms 3D-LaneNet in average precision(AP) and
F-score.",['cs.CV']
LT-Net: Label Transfer by Learning Reversible Voxel-wise Correspondence for One-shot Medical Image Segmentation,"We introduce a one-shot segmentation method to alleviate the burden of manual
annotation for medical images. The main idea is to treat one-shot segmentation
as a classical atlas-based segmentation problem, where voxel-wise
correspondence from the atlas to the unlabelled data is learned. Subsequently,
segmentation label of the atlas can be transferred to the unlabelled data with
the learned correspondence. However, since ground truth correspondence between
images is usually unavailable, the learning system must be well-supervised to
avoid mode collapse and convergence failure. To overcome this difficulty, we
resort to the forward-backward consistency, which is widely used in
correspondence problems, and additionally learn the backward correspondences
from the warped atlases back to the original atlas. This cycle-correspondence
learning design enables a variety of extra, cycle-consistency-based supervision
signals to make the training process stable, while also boost the performance.
We demonstrate the superiority of our method over both deep learning-based
one-shot segmentation methods and a classical multi-atlas segmentation method
via thorough experiments.",['cs.CV']
"Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours","One usage of medical ultrasound imaging is to visualize and characterize
human tongue shape and motion during a real-time speech to study healthy or
impaired speech production. Due to the low-contrast characteristic and noisy
nature of ultrasound images, it might require expertise for non-expert users to
recognize tongue gestures in applications such as visual training of a second
language. Moreover, quantitative analysis of tongue motion needs the tongue
dorsum contour to be extracted, tracked, and visualized. Manual tongue contour
extraction is a cumbersome, subjective, and error-prone task. Furthermore, it
is not a feasible solution for real-time applications. The growth of deep
learning has been vigorously exploited in various computer vision tasks,
including ultrasound tongue contour tracking. In the current methods, the
process of tongue contour extraction comprises two steps of image segmentation
and post-processing. This paper presents a new novel approach of automatic and
real-time tongue contour tracking using deep neural networks. In the proposed
method, instead of the two-step procedure, landmarks of the tongue surface are
tracked. This novel idea enables researchers in this filed to benefits from
available previously annotated databases to achieve high accuracy results. Our
experiment disclosed the outstanding performances of the proposed technique in
terms of generalization, performance, and accuracy.","['cs.CV', 'cs.CL', 'cs.LG', 'eess.IV']"
EDC3: Ensemble of Deep-Classifiers using Class-specific Copula functions to Improve Semantic Image Segmentation,"In the literature, many fusion techniques are registered for the segmentation
of images, but they primarily focus on observed output or belief score or
probability score of the output classes. In the present work, we have utilized
inter source statistical dependency among different classifiers for ensembling
of different deep learning techniques for semantic segmentation of images. For
this purpose, in the present work, a class-wise Copula-based ensembling method
is newly proposed for solving the multi-class segmentation problem.
Experimentally, it is observed that the performance has improved more for
semantic image segmentation using the proposed class-specific Copula function
than the traditionally used single Copula function for the problem. The
performance is also compared with three state-of-the-art ensembling methods.",['cs.CV']
Deep Learning for Ranking Response Surfaces with Applications to Optimal Stopping Problems,"In this paper, we propose deep learning algorithms for ranking response
surfaces, with applications to optimal stopping problems in financial
mathematics. The problem of ranking response surfaces is motivated by
estimating optimal feedback policy maps in stochastic control problems, aiming
to efficiently find the index associated to the minimal response across the
entire continuous input space $\mathcal{X} \subseteq \mathbb{R}^d$. By
considering points in $\mathcal{X}$ as pixels and indices of the minimal
surfaces as labels, we recast the problem as an image segmentation problem,
which assigns a label to every pixel in an image such that pixels with the same
label share certain characteristics. This provides an alternative method for
efficiently solving the problem instead of using sequential design in our
previous work [R. Hu and M. Ludkovski, SIAM/ASA Journal on Uncertainty
Quantification, 5 (2017), 212--239].
  Deep learning algorithms are scalable, parallel and model-free, i.e., no
parametric assumptions needed on the response surfaces. Considering ranking
response surfaces as image segmentation allows one to use a broad class of deep
neural networks, e.g., UNet, SegNet, DeconvNet, which have been widely applied
and numerically proved to possess high accuracy in the field. We also
systematically study the dependence of deep learning algorithms on the input
data generated on uniform grids or by sequential design sampling, and observe
that the performance of deep learning is {\it not} sensitive to the noise and
locations (close to/away from boundaries) of training data. We present a few
examples including synthetic ones and the Bermudan option pricing problem to
show the efficiency and accuracy of this method.","['stat.ML', 'cs.LG', 'q-fin.CP', '60G40, 65C60, 68T99']"
Out-of-Distribution Detection in Multi-Label Datasets using Latent Space of $$-VAE,"Learning Enabled Components (LECs) are widely being used in a variety of
perception based autonomy tasks like image segmentation, object detection,
end-to-end driving, etc. These components are trained with large image datasets
with multimodal factors like weather conditions, time-of-day, traffic-density,
etc. The LECs learn from these factors during training, and while testing if
there is variation in any of these factors, the components get confused
resulting in low confidence predictions. The images with factors not seen
during training is commonly referred to as Out-of-Distribution (OOD). For safe
autonomy it is important to identify the OOD images, so that a suitable
mitigation strategy can be performed. Classical one-class classifiers like SVM
and SVDD are used to perform OOD detection. However, the multiple labels
attached to the images in these datasets, restricts the direct application of
these techniques. We address this problem using the latent space of the
$\beta$-Variational Autoencoder ($\beta$-VAE). We use the fact that compact
latent space generated by an appropriately selected $\beta$-VAE will encode the
information about these factors in a few latent variables, and that can be used
for computationally inexpensive detection. We evaluate our approach on the
nuScenes dataset, and our results shows the latent space of $\beta$-VAE is
sensitive to encode changes in the values of the generative factor.","['cs.CV', 'cs.LG']"
PADDIT: Probabilistic Augmentation of Data using Diffeomorphic Image Transformation,"For proper generalization performance of convolutional neural networks (CNNs)
in medical image segmentation, the learnt features should be invariant under
particular non-linear shape variations of the input. To induce invariance in
CNNs to such transformations, we propose Probabilistic Augmentation of Data
using Diffeomorphic Image Transformation (PADDIT) -- a systematic framework for
generating realistic transformations that can be used to augment data for
training CNNs. We show that CNNs trained with PADDIT outperforms CNNs trained
without augmentation and with generic augmentation in segmenting white matter
hyperintensities from T1 and FLAIR brain MRI scans.",['cs.CV']
StructBoost: Boosting Methods for Predicting Structured Output Variables,"Boosting is a method for learning a single accurate predictor by linearly
combining a set of less accurate weak learners. Recently, structured learning
has found many applications in computer vision. Inspired by structured support
vector machines (SSVM), here we propose a new boosting algorithm for structured
output prediction, which we refer to as StructBoost. StructBoost supports
nonlinear structured learning by combining a set of weak structured learners.
As SSVM generalizes SVM, our StructBoost generalizes standard boosting
approaches such as AdaBoost, or LPBoost to structured learning. The resulting
optimization problem of StructBoost is more challenging than SSVM in the sense
that it may involve exponentially many variables and constraints. In contrast,
for SSVM one usually has an exponential number of constraints and a
cutting-plane method is used. In order to efficiently solve StructBoost, we
formulate an equivalent $ 1 $-slack formulation and solve it using a
combination of cutting planes and column generation. We show the versatility
and usefulness of StructBoost on a range of problems such as optimizing the
tree loss for hierarchical multi-class classification, optimizing the Pascal
overlap criterion for robust visual tracking and learning conditional random
field parameters for image segmentation.",['cs.LG']
Improving Training on Noisy Stuctured Labels,"Fine-grained annotations---e.g. dense image labels, image segmentation and
text tagging---are useful in many ML applications but they are labor-intensive
to generate. Moreover there are often systematic, structured errors in these
fine-grained annotations. For example, a car might be entirely unannotated in
the image, or the boundary between a car and street might only be coarsely
annotated. Standard ML training on data with such structured errors produces
models with biases and poor performance. In this work, we propose a novel
framework of Error-Correcting Networks (ECN) to address the challenge of
learning in the presence structured error in fine-grained annotations. Given a
large noisy dataset with commonly occurring structured errors, and a much
smaller dataset with more accurate annotations, ECN is able to substantially
improve the prediction of fine-grained annotations compared to standard
approaches for training on noisy data. It does so by learning to leverage the
structures in the annotations and in the noisy labels. Systematic experiments
on image segmentation and text tagging demonstrate the strong performance of
ECN in improving training on noisy structured labels.","['cs.LG', 'cs.CL', 'stat.ML']"
Improving Learning Effectiveness For Object Detection and Classification in Cluttered Backgrounds,"Usually, Neural Networks models are trained with a large dataset of images in
homogeneous backgrounds. The issue is that the performance of the network
models trained could be significantly degraded in a complex and heterogeneous
environment. To mitigate the issue, this paper develops a framework that
permits to autonomously generate a training dataset in heterogeneous cluttered
backgrounds. It is clear that the learning effectiveness of the proposed
framework should be improved in complex and heterogeneous environments,
compared with the ones with the typical dataset. In our framework, a
state-of-the-art image segmentation technique called DeepLab is used to extract
objects of interest from a picture and Chroma-key technique is then used to
merge the extracted objects of interest into specific heterogeneous
backgrounds. The performance of the proposed framework is investigated through
empirical tests and compared with that of the model trained with the COCO
dataset. The results show that the proposed framework outperforms the model
compared. This implies that the learning effectiveness of the framework
developed is superior to the models with the typical dataset.","['cs.CV', 'cs.LG', 'eess.IV']"
Towards Interpretable Semantic Segmentation via Gradient-weighted Class Activation Mapping,"Convolutional neural networks have become state-of-the-art in a wide range of
image recognition tasks. The interpretation of their predictions, however, is
an active area of research. Whereas various interpretation methods have been
suggested for image classification, the interpretation of image segmentation
still remains largely unexplored. To that end, we propose SEG-GRAD-CAM, a
gradient-based method for interpreting semantic segmentation. Our method is an
extension of the widely-used Grad-CAM method, applied locally to produce
heatmaps showing the relevance of individual pixels for semantic segmentation.","['cs.CV', 'cs.LG', 'eess.IV']"
Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.","['cs.CV', 'cs.LG', 'stat.ML']"
Dam Burst: A region-merging-based image segmentation method,"Until now, all single level segmentation algorithms except CNN-based ones
lead to over segmentation. And CNN-based segmentation algorithms have their own
problems. To avoid over segmentation, multiple thresholds of criteria are
adopted in region merging process to produce hierarchical segmentation results.
However, there still has extreme over segmentation in the low level of the
hierarchy, and outstanding tiny objects are merged to their large adjacencies
in the high level of the hierarchy. This paper proposes a region-merging-based
image segmentation method that we call it Dam Burst. As a single level
segmentation algorithm, this method avoids over segmentation and retains
details by the same time. It is named because of that it simulates a flooding
from underground destroys dams between water-pools. We treat edge detection
results as strengthening structure of a dam if it is on the dam. To simulate a
flooding from underground, regions are merged by ascending order of the average
gra-dient inside the region.",['cs.CV']
MixModule: Mixed CNN Kernel Module for Medical Image Segmentation,"Convolutional neural networks (CNNs) have been successfully applied to
medical image classification, segmentation, and related tasks. Among the many
CNNs architectures, U-Net and its improved versions based are widely used and
achieve state-of-the-art performance these years. These improved architectures
focus on structural improvements and the size of the convolution kernel is
generally fixed. In this paper, we propose a module that combines the benefits
of multiple kernel sizes and we apply the proposed module to U-Net and its
variants. We test our module on three segmentation benchmark datasets and
experimental results show significant improvement.",['cs.CV']
A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization,"Image segmentation is a fundamental research topic in image processing and
computer vision. In the last decades, researchers developed a large number of
segmentation algorithms for various applications. Amongst these algorithms, the
Normalized cut (Ncut) segmentation method is widely applied due to its good
performance. The Ncut segmentation model is an optimization problem whose
energy is defined on a specifically designed graph. Thus, the segmentation
results of the existing Ncut method are largely dependent on a pre-constructed
similarity measure on the graph since this measure is usually given empirically
by users. This flaw will lead to some undesirable segmentation results. In this
paper, we propose a Ncut-based segmentation algorithm by integrating an
adaptive similarity measure and spatial regularization. The proposed model
combines the Parzen-Rosenblatt window method, non-local weights entropy, Ncut
energy, and regularizer of phase field in a variational framework. Our method
can adaptively update the similarity measure function by estimating some
parameters. This adaptive procedure enables the proposed algorithm finding a
better similarity measure for classification than the Ncut method. We provide
some mathematical interpretation of the proposed adaptive similarity from
multi-viewpoints such as statistics and convex optimization. In addition, the
regularizer of phase field can guarantee that the proposed algorithm has a
robust performance in the presence of noise, and it can also rectify the
similarity measure with a spatial priori. The well-posed theory such as the
existence of the minimizer for the proposed model is given in the paper.
Compared with some existing segmentation methods such as the traditional
Ncut-based model and the classical Chan-Vese model, the numerical experiments
show that our method can provide promising segmentation results.",['cs.CV']
3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training,"While making a tremendous impact in various fields, deep neural networks
usually require large amounts of labeled data for training which are expensive
to collect in many applications, especially in the medical domain. Unlabeled
data, on the other hand, is much more abundant. Semi-supervised learning
techniques, such as co-training, could provide a powerful tool to leverage
unlabeled data. In this paper, we propose a novel framework, uncertainty-aware
multi-view co-training (UMCT), to address semi-supervised learning on 3D data,
such as volumetric data from medical imaging. In our work, co-training is
achieved by exploiting multi-viewpoint consistency of 3D data. We generate
different views by rotating or permuting the 3D data and utilize asymmetrical
3D kernels to encourage diversified features in different sub-networks. In
addition, we propose an uncertainty-weighted label fusion mechanism to estimate
the reliability of each view's prediction with Bayesian deep learning. As one
view requires the supervision from other views in co-training, our
self-adaptive approach computes a confidence score for the prediction of each
unlabeled sample in order to assign a reliable pseudo label. Thus, our approach
can take advantage of unlabeled data during training. We show the effectiveness
of our proposed semi-supervised method on several public datasets from medical
image segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile,
a fully-supervised method based on our approach achieved state-of-the-art
performances on both the LiTS liver tumor segmentation and the Medical
Segmentation Decathlon (MSD) challenge, demonstrating the robustness and value
of our framework, even when fully supervised training is feasible.",['cs.CV']
Robust Multimodal Brain Tumor Segmentation via Feature Disentanglement and Gated Fusion,"Accurate medical image segmentation commonly requires effective learning of
the complementary information from multimodal data. However, in clinical
practice, we often encounter the problem of missing imaging modalities. We
tackle this challenge and propose a novel multimodal segmentation framework
which is robust to the absence of imaging modalities. Our network uses feature
disentanglement to decompose the input modalities into the modality-specific
appearance code, which uniquely sticks to each modality, and the
modality-invariant content code, which absorbs multimodal information for the
segmentation task. With enhanced modality-invariance, the disentangled content
code from each modality is fused into a shared representation which gains
robustness to missing data. The fusion is achieved via a learning-based
strategy to gate the contribution of different modalities at different
locations. We validate our method on the important yet challenging multimodal
brain tumor segmentation task with the BRATS challenge dataset. With
competitive performance to the state-of-the-art approaches for full modality,
our method achieves outstanding robustness under various missing modality(ies)
situations, significantly exceeding the state-of-the-art method by over 16% in
average for Dice on whole tumor segmentation.",['cs.CV']
Meta Segmentation Network for Ultra-Resolution Medical Images,"Despite recent progress on semantic segmentation, there still exist huge
challenges in medical ultra-resolution image segmentation. The methods based on
multi-branch structure can make a good balance between computational burdens
and segmentation accuracy. However, the fusion structure in these methods
require to be designed elaborately to achieve desirable result, which leads to
model redundancy. In this paper, we propose Meta Segmentation Network (MSN) to
solve this challenging problem. With the help of meta-learning, the fusion
module of MSN is quite simple but effective. MSN can fast generate the weights
of fusion layers through a simple meta-learner, requiring only a few training
samples and epochs to converge. In addition, to avoid learning all branches
from scratch, we further introduce a particular weight sharing mechanism to
realize a fast knowledge adaptation and share the weights among multiple
branches, resulting in the performance improvement and significant parameters
reduction. The experimental results on two challenging ultra-resolution medical
datasets BACH and ISIC show that MSN achieves the best performance compared
with the state-of-the-art methods.",['cs.CV']
Non-local U-Net for Biomedical Image Segmentation,"Deep learning has shown its great promise in various biomedical image
segmentation tasks. Existing models are typically based on U-Net and rely on an
encoder-decoder architecture with stacked local operators to aggregate
long-range information gradually. However, only using the local operators
limits the efficiency and effectiveness. In this work, we propose the non-local
U-Nets, which are equipped with flexible global aggregation blocks, for
biomedical image segmentation. These blocks can be inserted into U-Net as
size-preserving processes, as well as down-sampling and up-sampling layers. We
perform thorough experiments on the 3D multimodality isointense infant brain MR
image segmentation task to evaluate the non-local U-Nets. Results show that our
proposed models achieve top performances with fewer parameters and faster
computation.","['cs.CV', 'cs.LG', 'stat.AP', 'stat.ML']"
Neural arbitrary style transfer for portrait images using the attention mechanism,"Arbitrary style transfer is the task of synthesis of an image that has never
been seen before, using two given images: content image and style image. The
content image forms the structure, the basic geometric lines and shapes of the
resulting image, while the style image sets the color and texture of the
result. The word ""arbitrary"" in this context means the absence of any one
pre-learned style. So, for example, convolutional neural networks capable of
transferring a new style only after training or retraining on a new amount of
data are not con-sidered to solve such a problem, while networks based on the
attention mech-anism that are capable of performing such a transformation
without retraining - yes. An original image can be, for example, a photograph,
and a style image can be a painting of a famous artist. The resulting image in
this case will be the scene depicted in the original photograph, made in the
stylie of this picture. Recent arbitrary style transfer algorithms make it
possible to achieve good re-sults in this task, however, in processing portrait
images of people, the result of such algorithms is either unacceptable due to
excessive distortion of facial features, or weakly expressed, not bearing the
characteristic features of a style image. In this paper, we consider an
approach to solving this problem using the combined architecture of deep neural
networks with a attention mechanism that transfers style based on the contents
of a particular image segment: with a clear predominance of style over the form
for the background part of the im-age, and with the prevalence of content over
the form in the image part con-taining directly the image of a person.","['cs.CV', 'cs.LG', 'eess.IV']"
Calibrated Surrogate Maximization of Linear-fractional Utility in Binary Classification,"Complex classification performance metrics such as the F${}_\beta$-measure
and Jaccard index are often used, in order to handle class-imbalanced cases
such as information retrieval and image segmentation. These performance metrics
are not decomposable, that is, they cannot be expressed in a per-example
manner, which hinders a straightforward application of M-estimation widely used
in supervised learning. In this paper, we consider linear-fractional metrics,
which are a family of classification performance metrics that encompasses many
standard ones such as the F${}_\beta$-measure and Jaccard index, and propose
methods to directly maximize performances under those metrics. A clue to tackle
their direct optimization is a calibrated surrogate utility, which is a
tractable lower bound of the true utility function representing a given metric.
We characterize sufficient conditions which make the surrogate maximization
coincide with the maximization of the true utility. Simulation results on
benchmark datasets validate the effectiveness of our calibrated surrogate
maximization especially if the sample sizes are extremely small.","['cs.LG', 'stat.ML']"
PointRend: Image Segmentation as Rendering,"We present a new method for efficient high-quality image segmentation of
objects and scenes. By analogizing classical computer graphics methods for
efficient rendering with over- and undersampling challenges faced in pixel
labeling tasks, we develop a unique perspective of image segmentation as a
rendering problem. From this vantage, we present the PointRend (Point-based
Rendering) neural network module: a module that performs point-based
segmentation predictions at adaptively selected locations based on an iterative
subdivision algorithm. PointRend can be flexibly applied to both instance and
semantic segmentation tasks by building on top of existing state-of-the-art
models. While many concrete implementations of the general idea are possible,
we show that a simple design already achieves excellent results. Qualitatively,
PointRend outputs crisp object boundaries in regions that are over-smoothed by
previous methods. Quantitatively, PointRend yields significant gains on COCO
and Cityscapes, for both instance and semantic segmentation. PointRend's
efficiency enables output resolutions that are otherwise impractical in terms
of memory or computation compared to existing approaches. Code has been made
available at
https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.",['cs.CV']
Reinforced active learning for image segmentation,"Learning-based approaches for semantic segmentation have two inherent
challenges. First, acquiring pixel-wise labels is expensive and time-consuming.
Second, realistic segmentation datasets are highly unbalanced: some categories
are much more abundant than others, biasing the performance to the most
represented ones. In this paper, we are interested in focusing human labelling
effort on a small subset of a larger pool of data, minimizing this effort while
maximizing performance of a segmentation model on a hold-out set. We present a
new active learning strategy for semantic segmentation based on deep
reinforcement learning (RL). An agent learns a policy to select a subset of
small informative image regions -- opposed to entire images -- to be labeled,
from a pool of unlabeled data. The region selection decision is made based on
predictions and uncertainties of the segmentation model being trained. Our
method proposes a new modification of the deep Q-network (DQN) formulation for
active learning, adapting it to the large-scale nature of semantic segmentation
problems. We test the proof of concept in CamVid and provide results in the
large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN
approach requires roughly 30% less additional labeled data than our most
competitive baseline to reach the same performance. Moreover, we find that our
method asks for more labels of under-represented categories compared to the
baselines, improving their performance and helping to mitigate class imbalance.",['cs.CV']
Multi-scale self-guided attention for medical image segmentation,"Even though convolutional neural networks (CNNs) are driving progress in
medical image segmentation, standard models still have some drawbacks. First,
the use of multi-scale approaches, i.e., encoder-decoder architectures, leads
to a redundant use of information, where similar low-level features are
extracted multiple times at multiple scales. Second, long-range feature
dependencies are not efficiently modeled, resulting in non-optimal
discriminative feature representations associated with each semantic class. In
this paper we attempt to overcome these limitations with the proposed
architecture, by capturing richer contextual dependencies based on the use of
guided self-attention mechanisms. This approach is able to integrate local
features with their corresponding global dependencies, as well as highlight
interdependent channel maps in an adaptive manner. Further, the additional loss
between different modules guides the attention mechanisms to neglect irrelevant
information and focus on more discriminant regions of the image by emphasizing
relevant feature associations. We evaluate the proposed model in the context of
semantic segmentation on three different datasets: abdominal organs,
cardiovascular structures and brain tumors. A series of ablation experiments
support the importance of these attention modules in the proposed architecture.
In addition, compared to other state-of-the-art segmentation networks our model
yields better segmentation performance, increasing the accuracy of the
predictions while reducing the standard deviation. This demonstrates the
efficiency of our approach to generate precise and reliable automatic
segmentations of medical images. Our code is made publicly available at
https://github.com/sinAshish/Multi-Scale-Attention",['cs.CV']
Building Networks for Image Segmentation using Particle Competition and Cooperation,"Particle competition and cooperation (PCC) is a graph-based semi-supervised
learning approach. When PCC is applied to interactive image segmentation tasks,
pixels are converted into network nodes, and each node is connected to its
k-nearest neighbors, according to the distance between a set of features
extracted from the image. Building a proper network to feed PCC is crucial to
achieve good segmentation results. However, some features may be more important
than others to identify the segments, depending on the characteristics of the
image to be segmented. In this paper, an index to evaluate candidate networks
is proposed. Thus, building the network becomes a problem of optimizing some
feature weights based on the proposed index. Computer simulations are performed
on some real-world images from the Microsoft GrabCut database, and the
segmentation results related in this paper show the effectiveness of the
proposed method.","['cs.CV', 'cs.LG', 'cs.NE']"
Residual-Sparse Fuzzy $C$-Means Clustering Incorporating Morphological Reconstruction and Wavelet frames,"Instead of directly utilizing an observed image including some outliers,
noise or intensity inhomogeneity, the use of its ideal value (e.g. noise-free
image) has a favorable impact on clustering. Hence, the accurate estimation of
the residual (e.g. unknown noise) between the observed image and its ideal
value is an important task. To do so, we propose an $\ell_0$
regularization-based Fuzzy $C$-Means (FCM) algorithm incorporating a
morphological reconstruction operation and a tight wavelet frame transform. To
achieve a sound trade-off between detail preservation and noise suppression,
morphological reconstruction is used to filter an observed image. By combining
the observed and filtered images, a weighted sum image is generated. Since a
tight wavelet frame system has sparse representations of an image, it is
employed to decompose the weighted sum image, thus forming its corresponding
feature set. Taking it as data for clustering, we present an improved FCM
algorithm by imposing an $\ell_0$ regularization term on the residual between
the feature set and its ideal value, which implies that the favorable
estimation of the residual is obtained and the ideal value participates in
clustering. Spatial information is also introduced into clustering since it is
naturally encountered in image segmentation. Furthermore, it makes the
estimation of the residual more reliable. To further enhance the segmentation
effects of the improved FCM algorithm, we also employ the morphological
reconstruction to smoothen the labels generated by clustering. Finally, based
on the prototypes and smoothed labels, the segmented image is reconstructed by
using a tight wavelet frame reconstruction operation. Experimental results
reported for synthetic, medical, and color images show that the proposed
algorithm is effective and efficient, and outperforms other algorithms.","['cs.CV', '62H30', 'I.4.6']"
Liver Segmentation in Abdominal CT Images via Auto-Context Neural Network and Self-Supervised Contour Attention,"Accurate image segmentation of the liver is a challenging problem owing to
its large shape variability and unclear boundaries. Although the applications
of fully convolutional neural networks (CNNs) have shown groundbreaking
results, limited studies have focused on the performance of generalization. In
this study, we introduce a CNN for liver segmentation on abdominal computed
tomography (CT) images that shows high generalization performance and accuracy.
To improve the generalization performance, we initially propose an auto-context
algorithm in a single CNN. The proposed auto-context neural network exploits an
effective high-level residual estimation to obtain the shape prior. Identical
dual paths are effectively trained to represent mutual complementary features
for an accurate posterior analysis of a liver. Further, we extend our network
by employing a self-supervised contour scheme. We trained sparse contour
features by penalizing the ground-truth contour to focus more contour
attentions on the failures. The experimental results show that the proposed
network results in better accuracy when compared to the state-of-the-art
networks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT
images for training and validation. Two-fold cross-validation is presented for
a comparison with the state-of-the-art neural networks. Novel multiple N-fold
cross-validations are conducted to verify the performance of generalization.
The proposed network showed the best generalization performance among the
networks. Additionally, we present a series of ablation experiments that
comprehensively support the importance of the underlying concepts.","['cs.CV', '68U10']"
Clustering based on Point-Set Kernel,"Measuring similarity between two objects is the core operation in existing
cluster analyses in grouping similar objects into clusters. Cluster analyses
have been applied to a number of applications, including image segmentation,
social network analysis, and computational biology. This paper introduces a new
similarity measure called point-set kernel which computes the similarity
between an object and a sample of objects generated from an unknown
distribution. The proposed clustering procedure utilizes this new measure to
characterize both the typical point of every cluster and the cluster grown from
the typical point. We show that the new clustering procedure is both effective
and efficient such that it can deal with large scale datasets. In contrast,
existing clustering algorithms are either efficient or effective; and even
efficient ones have difficulty dealing with large scale datasets without
special hardware. We show that the proposed algorithm is more effective and
runs orders of magnitude faster than the state-of-the-art density-peak
clustering and scalable kernel k-means clustering when applying to datasets of
millions of data points, on commonly used computing machines.","['cs.LG', 'stat.ML']"
Simple Interactive Image Segmentation using Label Propagation through kNN graphs,"Many interactive image segmentation techniques are based on semi-supervised
learning. The user may label some pixels from each object and the SSL algorithm
will propagate the labels from the labeled to the unlabeled pixels, finding
object boundaries. This paper proposes a new SSL graph-based interactive image
segmentation approach, using undirected and unweighted kNN graphs, from which
the unlabeled nodes receive contributions from other nodes (either labeled or
unlabeled). It is simpler than many other techniques, but it still achieves
significant classification accuracy in the image segmentation task. Computer
simulations are performed using some real-world images, extracted from the
Microsoft GrabCut dataset. The segmentation results show the effectiveness of
the proposed approach.","['cs.LG', 'stat.ML']"
Constrained Dominant sets and Its applications in computer vision,"In this thesis, we present new schemes which leverage a constrained
clustering method to solve several computer vision tasks ranging from image
retrieval, image segmentation and co-segmentation, to person re-identification.
In the last decades clustering methods have played a vital role in computer
vision applications; herein, we focus on the extension, reformulation, and
integration of a well-known graph and game theoretic clustering method known as
Dominant Sets. Thus, we have demonstrated the validity of the proposed methods
with extensive experiments which are conducted on several benchmark datasets.",['cs.CV']
"Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation","We use Deep Convolutional Neural Networks (DCNNs) for image segmentation
problems. DCNNs can well extract the features from natural images. However, the
classification functions in the existing network architecture of CNNs are
simple and lack capabilities to handle important spatial information in a way
that have been done for many well-known traditional variational models. Prior
such as spatial regularity, volume prior and object shapes cannot be well
handled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD)
framework which can easily integrate many spatial priors of the classical
variational models into the DCNNs for image segmentation. The novelty of our
method is to interpret the softmax activation function as a dual variable in a
variational problem, and thus many spatial priors can be imposed in the dual
space. From this viewpoint, we can build a STD based framework which can enable
the outputs of DCNNs to have many special priors such as spatial regularity,
volume constraints and star-shape priori. The proposed method is a general
mathematical framework and it can be applied to any semantic segmentation
DCNNs. To show the efficiency and accuracy of our method, we applied it to the
popular DeepLabV3+ image segmentation network, and the experiments results show
that our method can work efficiently on data-driven image segmentation DCNNs.","['cs.CV', 'cs.LG']"
One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation,"Class imbalance has emerged as one of the major challenges for medical image
segmentation. The model cascade (MC) strategy significantly alleviates the
class imbalance issue via running a set of individual deep models for
coarse-to-fine segmentation. Despite its outstanding performance, however, this
method leads to undesired system complexity and also ignores the correlation
among the models. To handle these flaws, we propose a light-weight deep model,
i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better
than MC does, while requiring only one-pass computation. First, OM-Net
integrates the separate segmentation tasks into one deep model, which consists
of shared parameters to learn joint features, as well as task-specific
parameters to learn discriminative features. Second, to more effectively
optimize OM-Net, we take advantage of the correlation among tasks to design
both an online training data transfer strategy and a curriculum learning-based
training strategy. Third, we further propose sharing prediction results between
tasks and design a cross-task guided attention (CGA) module which can
adaptively recalibrate channel-wise feature responses based on the
category-specific statistics. Finally, a simple yet effective post-processing
method is introduced to refine the segmentation results. Extensive experiments
are conducted to demonstrate the effectiveness of the proposed techniques. Most
impressively, we achieve state-of-the-art performance on the BraTS 2015 testing
set and BraTS 2017 online validation set. Using these proposed approaches, we
also won joint third place in the BraTS 2018 challenge among 64 participating
teams. The code is publicly available at
https://github.com/chenhong-zhou/OM-Net.","['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']"
Segmentao de imagens utilizando competio e cooperao entre partculas,"This paper presents an extension proposal of the semi-supervised learning
method known as Particle Competition and Cooperation for carrying out tasks of
image segmentation. Preliminary results show that this is a promising approach.
  Este artigo apresenta uma proposta de extens\~ao do modelo de aprendizado
semi-supervisionado conhecido como Competi\c{c}\~ao e Coopera\c{c}\~ao entre
Part\'iculas para a realiza\c{c}\~ao de tarefas de segmenta\c{c}\~ao de
imagens. Resultados preliminares mostram que esta \'e uma abordagem promissora.","['cs.LG', 'cs.NE']"
"Unsupervised Community Detection with a Potts Model Hamiltonian, an Efficient Algorithmic Solution, and Applications in Digital Pathology","Unsupervised segmentation of large images using a Potts model Hamiltonian is
unique in that segmentation is governed by a resolution parameter which scales
the sensitivity to small clusters. Here, the input image is first modeled as a
graph, which is then segmented by minimizing a Hamiltonian cost function
defined on the graph and the respective segments. However, there exists no
closed form solution of this optimization, and using previous iterative
algorithmic solution techniques, the problem scales quadratically in the Input
Length. Therefore, while Potts model segmentation gives accurate segmentation,
it is grossly underutilized as an unsupervised learning technique. We propose a
fast statistical down-sampling of input image pixels based on the respective
color features, and a new iterative method to minimize the Potts model energy
considering pixel to segment relationship. This method is generalizable and can
be extended for image pixel texture features as well as spatial features. We
demonstrate that this new method is highly efficient, and outperforms existing
methods for Potts model based image segmentation. We demonstrate the
application of our method in medical microscopy image segmentation;
particularly, in segmenting renal glomerular micro-environment in renal
pathology. Our method is not limited to image segmentation, and can be extended
to any image/data segmentation/clustering task for arbitrary datasets with
discrete features.","['cs.CV', 'cond-mat.stat-mech']"
Efficient 2D neuron boundary segmentation with local topological constraints,"We present a method for segmenting neuron membranes in 2D electron microscopy
imagery. This segmentation task has been a bottleneck to reconstruction efforts
of the brain's synaptic circuits. One common problem is the misclassification
of blurry membrane fragments as cell interior, which leads to merging of two
adjacent neuron sections into one via the blurry membrane region. Human
annotators can easily avoid such errors by implicitly performing gap
completion, taking into account the continuity of membranes.
  Drawing inspiration from these human strategies, we formulate the
segmentation task as an edge labeling problem on a graph with local topological
constraints. We derive an integer linear program (ILP) that enforces membrane
continuity, i.e. the absence of gaps. The cost function of the ILP is the
pixel-wise deviation of the segmentation from a priori membrane probabilities
derived from the data.
  Based on membrane probability maps obtained using random forest classifiers
and convolutional neural networks, our method improves the neuron boundary
segmentation accuracy compared to a variety of standard segmentation
approaches. Our method successfully performs gap completion and leads to fewer
topological errors. The method could potentially also be incorporated into
other image segmentation pipelines with known topological constraints.",['cs.CV']
Adversarial normalization for multi domain image segmentation,"Image normalization is a critical step in medical imaging. This step is often
done on a per-dataset basis, preventing current segmentation algorithms from
the full potential of exploiting jointly normalized information across multiple
datasets. To solve this problem, we propose an adversarial normalization
approach for image segmentation which learns common normalizing functions
across multiple datasets while retaining image realism. The adversarial
training provides an optimal normalizer that improves both the segmentation
accuracy and the discrimination of unrealistic normalizing functions. Our
contribution therefore leverages common imaging information from multiple
domains. The optimality of our common normalizer is evaluated by combining
brain images from both infants and adults. Results on the challenging iSEG and
MRBrainS datasets reveal the potential of our adversarial normalization
approach for segmentation, with Dice improvements of up to 59.6% over the
baseline.","['cs.LG', 'cs.CV', 'stat.ML']"
Dual Convolutional LSTM Network for Referring Image Segmentation,"We consider referring image segmentation. It is a problem at the intersection
of computer vision and natural language understanding. Given an input image and
a referring expression in the form of a natural language sentence, the goal is
to segment the object of interest in the image referred by the linguistic
query. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to
tackle this problem. Our model consists of an encoder network and a decoder
network, where ConvLSTM is used in both encoder and decoder networks to capture
spatial and sequential information. The encoder network extracts visual and
linguistic features for each word in the expression sentence, and adopts an
attention mechanism to focus on words that are more informative in the
multimodal interaction. The decoder network integrates the features generated
by the encoder network at multiple levels as its input and produces the final
precise segmentation mask. Experimental results on four challenging datasets
demonstrate that the proposed network achieves superior segmentation
performance compared with other state-of-the-art methods.",['cs.CV']
Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators,"Generative adversarial networks (GANs) have shown great success in
applications such as image generation and inpainting. However, they typically
require large datasets, which are often not available, especially in the
context of prediction tasks such as image segmentation that require labels.
Therefore, methods such as the CycleGAN use more easily available unlabelled
data, but do not offer a way to leverage additional labelled data for improved
performance. To address this shortcoming, we show how to factorise the joint
data distribution into a set of lower-dimensional distributions along with
their dependencies. This allows splitting the discriminator in a GAN into
multiple ""sub-discriminators"" that can be independently trained from incomplete
observations. Their outputs can be combined to estimate the density ratio
between the joint real and the generator distribution, which enables training
generators as in the original GAN framework. We apply our method to image
generation, image segmentation and audio source separation, and obtain improved
performance over a standard GAN when additional incomplete training examples
are available. For the Cityscapes segmentation task in particular, our method
also improves accuracy by an absolute 14.9% over CycleGAN while using only 25
additional paired examples.","['cs.LG', 'stat.ML']"
Robust Submodular Minimization with Applications to Cooperative Modeling,"Robust Optimization is becoming increasingly important in machine learning
applications. This paper studies the problem of robust submodular minimization
subject to combinatorial constraints. Constrained Submodular Minimization
arises in several applications such as co-operative cuts in image segmentation,
co-operative matchings in image correspondence, etc. Many of these models are
defined over clusterings of data points (for example pixels in images), and it
is important for these models to be robust to perturbations and uncertainty in
the data. While several existing papers have studied robust submodular
maximization, ours is the first work to study the minimization version under a
broad range of combinatorial constraints including cardinality, knapsack,
matroid as well as graph-based constraints such as cuts, paths, matchings, and
trees. In each case, we provide scalable approximation algorithms and also
study hardness bounds. Finally, we empirically demonstrate the utility of our
algorithms on synthetic and real-world datasets.","['cs.LG', 'math.OC', 'stat.ML']"
Observer variation-aware medical image segmentation by combining deep learning and surrogate-assisted genetic algorithms,"There has recently been great progress in automatic segmentation of medical
images with deep learning algorithms. In most works observer variation is
acknowledged to be a problem as it makes training data heterogeneous but so far
no attempts have been made to explicitly capture this variation. Here, we
propose an approach capable of mimicking different styles of segmentation,
which potentially can improve quality and clinical acceptance of automatic
segmentation methods. In this work, instead of training one neural network on
all available data, we train several neural networks on subgroups of data
belonging to different segmentation variations separately. Because a priori it
may be unclear what styles of segmentation exist in the data and because
different styles do not necessarily map one-on-one to different observers, the
subgroups should be automatically determined. We achieve this by searching for
the best data partition with a genetic algorithm. Therefore, each network can
learn a specific style of segmentation from grouped training data. We provide
proof of principle results for open-sourced prostate segmentation MRI data with
simulated observer variations. Our approach provides an improvement of up to
23% (depending on simulated variations) in terms of Dice and surface Dice
coefficients compared to one network trained on all data.","['cs.CV', 'cs.LG', 'cs.NE', 'eess.IV']"
Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning,"Image segmentation with a volume constraint is an important prior for many
real applications. In this work, we present a novel volume preserving image
segmentation algorithm, which is based on the framework of entropic regularized
optimal transport theory. The classical Total Variation (TV) regularizer and
volume preserving are integrated into a regularized optimal transport model,
and the volume and classification constraints can be regarded as two measures
preserving constraints in the optimal transport problem. By studying the dual
problem, we develop a simple and efficient dual algorithm for our model.
Moreover, to be different from many variational based image segmentation
algorithms, the proposed algorithm can be directly unrolled to a new Volume
Preserving and TV regularized softmax (VPTV-softmax) layer for semantic
segmentation in the popular Deep Convolution Neural Network (DCNN). The
experiment results show that our proposed model is very competitive and can
improve the performance of many semantic segmentation nets such as the popular
U-net.",['cs.CV']
Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Synthesis and Perspective,"This fourth and last tome is focusing on describing the envisioned works for
a project that has been presented in the preceding tome. It is about a new
approach dedicated to the coding of still and moving pictures, trying to bridge
the MPEG-4 and MPEG-7 standard bodies. The aim of this project is to define the
principles of self-descriptive video coding. In order to establish them, the
document is composed in five chapters that describe the various envisioned
techniques for developing such a new approach in visual coding: - image
segmentation, - computation of visual descriptors, - computation of perceptual
groupings, - building of visual dictionaries, - picture and video coding. Based
on the techniques of multiresolution computing, it is proposed to develop an
image segmentation made from piecewise regular components, to compute
attributes on the frame and the rendering of so produced shapes, independently
to the geometric transforms that can occur in the image plane, and to gather
them into perceptual groupings so as to be able in performing recognition of
partially hidden patterns. Due to vector quantization of shapes frame and
rendering, it will appear that simple shapes may be compared to a visual
alphabet and that complex shapes then become words written using this alphabet
and be recorded into a dictionary. With the help of a nearest neighbour
scanning applied on the picture shapes, the self-descriptive coding will then
generate a sentence made from words written using the simple shape alphabet.","['cs.CV', 'E.1; I.4; I.5; I.6']"
Few Labeled Atlases are Necessary for Deep-Learning-Based Segmentation,"We tackle biomedical image segmentation in the scenario of only a few labeled
brain MR images. This is an important and challenging task in medical
applications, where manual annotations are time-consuming. Current multi-atlas
based segmentation methods use image registration to warp segments from labeled
images onto a new scan. In a different paradigm, supervised learning-based
segmentation strategies have gained popularity. These method consistently use
relatively large sets of labeled training data, and their behavior in the
regime of a few labeled biomedical images has not been thoroughly evaluated. In
this work, we provide two important results for segmentation in the scenario
where few labeled images are available. First, we propose a straightforward
implementation of efficient semi-supervised learning-based registration method,
which we showcase in a multi-atlas segmentation framework. Second, through an
extensive empirical study, we evaluate the performance of a supervised
segmentation approach, where the training images are augmented via random
deformations. Surprisingly, we find that in both paradigms, accurate
segmentation is generally possible even in the context of few labeled images.",['cs.CV']
Unpaired Multi-modal Segmentation via Knowledge Distillation,"Multi-modal learning is typically performed with network architectures
containing modality-specific layers and shared layers, utilizing co-registered
images of different modalities. We propose a novel learning scheme for unpaired
cross-modality image segmentation, with a highly compact architecture achieving
superior segmentation accuracy. In our method, we heavily reuse network
parameters, by sharing all convolutional kernels across CT and MRI, and only
employ modality-specific internal normalization layers which compute respective
statistics. To effectively train such a highly compact model, we introduce a
novel loss term inspired by knowledge distillation, by explicitly constraining
the KL-divergence of our derived prediction distributions between modalities.
We have extensively validated our approach on two multi-class segmentation
problems: i) cardiac structure segmentation, and ii) abdominal organ
segmentation. Different network settings, i.e., 2D dilated network and 3D
U-net, are utilized to investigate our method's general efficacy. Experimental
results on both tasks demonstrate that our novel multi-modal learning scheme
consistently outperforms single-modal training and previous multi-modal
approaches.","['cs.CV', 'eess.IV']"
Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification,"Image segmentation and classification are the two main fundamental steps in
pattern recognition. To perform medical image segmentation or classification
with deep learning models, it requires training on large image dataset with
annotation. The dermoscopy images (ISIC archive) considered for this work does
not have ground truth information for lesion segmentation. Performing manual
labelling on this dataset is time-consuming. To overcome this issue,
self-learning annotation scheme was proposed in the two-stage deep learning
algorithm. The two-stage deep learning algorithm consists of U-Net segmentation
model with the annotation scheme and CNN classifier model. The annotation
scheme uses a K-means clustering algorithm along with merging conditions to
achieve initial labelling information for training the U-Net model. The
classifier models namely ResNet-50 and LeNet-5 were trained and tested on the
image dataset without segmentation for comparison and with the U-Net
segmentation for implementing the proposed self-learning Artificial
Intelligence (AI) framework. The classification results of the proposed AI
framework achieved training accuracy of 93.8% and testing accuracy of 82.42%
when compared with the two classifier models directly trained on the input
images.","['cs.CV', 'cs.LG', 'eess.IV']"
Reducing the Model Variance of a Rectal Cancer Segmentation Network,"In preoperative imaging, the demarcation of rectal cancer with magnetic
resonance images provides an important basis for cancer staging and treatment
planning. Recently, deep learning has greatly improved the state-of-the-art
method in automatic segmentation. However, limitations in data availability in
the medical field can cause large variance and consequent overfitting to
medical image segmentation networks. In this study, we propose methods to
reduce the model variance of a rectal cancer segmentation network by adding a
rectum segmentation task and performing data augmentation; the geometric
correlation between the rectum and rectal cancer motivated the former approach.
Moreover, we propose a method to perform a bias-variance analysis within an
arbitrary region-of-interest (ROI) of a segmentation network, which we applied
to assess the efficacy of our approaches in reducing model variance. As a
result, adding a rectum segmentation task reduced the model variance of the
rectal cancer segmentation network within tumor regions by a factor of 0.90;
data augmentation further reduced the variance by a factor of 0.89. These
approaches also reduced the training duration by a factor of 0.96 and a further
factor of 0.78, respectively. Our approaches will improve the quality of rectal
cancer staging by increasing the accuracy of its automatic demarcation and by
providing rectum boundary information since rectal cancer staging requires the
demarcation of both rectum and rectal cancer. Besides such clinical benefits,
our method also enables segmentation networks to be assessed with bias-variance
analysis within an arbitrary ROI, such as a cancerous region.","['cs.CV', 'cs.LG']"
Efficient Video Semantic Segmentation with Labels Propagation and Refinement,"This paper tackles the problem of real-time semantic segmentation of high
definition videos using a hybrid GPU / CPU approach. We propose an Efficient
Video Segmentation(EVS) pipeline that combines:
  (i) On the CPU, a very fast optical flow method, that is used to exploit the
temporal aspect of the video and propagate semantic information from one frame
to the next. It runs in parallel with the GPU.
  (ii) On the GPU, two Convolutional Neural Networks: A main segmentation
network that is used to predict dense semantic labels from scratch, and a
Refiner that is designed to improve predictions from previous frames with the
help of a fast Inconsistencies Attention Module (IAM). The latter can identify
regions that cannot be propagated accurately.
  We suggest several operating points depending on the desired frame rate and
accuracy. Our pipeline achieves accuracy levels competitive to the existing
real-time methods for semantic image segmentation(mIoU above 60%), while
achieving much higher frame rates. On the popular Cityscapes dataset with high
resolution frames (2048 x 1024), the proposed operating points range from 80 to
1000 Hz on a single GPU and CPU.",['cs.CV']
Neural ODEs for Image Segmentation with Level Sets,"We propose a novel approach for image segmentation that combines Neural
Ordinary Differential Equations (NODEs) and the Level Set method. Our approach
parametrizes the evolution of an initial contour with a NODE that implicitly
learns from data a speed function describing the evolution. In addition, for
cases where an initial contour is not available and to alleviate the need for
careful choice or design of contour embedding functions, we propose a
NODE-based method that evolves an image embedding into a dense per-pixel
semantic label space. We evaluate our methods on kidney segmentation (KiTS19)
and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to
improving initial contours provided by deep learning models while using a
fraction of their number of parameters, our approach achieves F scores that are
higher than several state-of-the-art deep learning algorithms.","['cs.CV', 'cs.LG', 'eess.IV']"
Generalizing Deep Models for Overhead Image Segmentation Through Getis-Ord Gi* Pooling,"That most deep learning models are purely data driven is both a strength and
a weakness. Given sufficient training data, the optimal model for a particular
problem can be learned. However, this is usually not the case and so instead
the model is either learned from scratch from a limited amount of training data
or pre-trained on a different problem and then fine-tuned. Both of these
situations are potentially suboptimal and limit the generalizability of the
model. Inspired by this, we investigate methods to inform or guide deep
learning models for geospatial image analysis to increase their performance
when a limited amount of training data is available or when they are applied to
scenarios other than which they were trained on. In particular, we exploit the
fact that there are certain fundamental rules as to how things are distributed
on the surface of the Earth and these rules do not vary substantially between
locations. Based on this, we develop a novel feature pooling method for
convolutional neural networks using Getis-Ord Gi* analysis from geostatistics.
Experimental results show our proposed pooling function has significantly
better generalization performance compared to a standard data-driven approach
when applied to overhead image segmentation.",['cs.CV']
Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation,"Recent advances in AI technology have made the forgery of digital images and
videos easier, and it has become significantly more difficult to identify such
forgeries. These forgeries, if disseminated with malicious intent, can
negatively impact social and political stability, and pose significant ethical
and legal challenges as well. Deepfake is a variant of auto-encoders that use
deep learning techniques to identify and exchange images of a person's face in
a picture or film. Deepfake can result in an erosion of public trust in digital
images and videos, which has far-reaching effects on political and social
stability. This study therefore proposes a solution for facial forgery
detection to determine if a picture or film has ever been processed by
Deepfake. The proposed solution reaches detection efficiency by using the
recently proposed separable convolutional neural network (CNN) and image
segmentation. In addition, this study also examined how different image
segmentation methods affect detection results. Finally, the ensemble model is
used to improve detection capabilities. Experiment results demonstrated the
excellent performance of the proposed solution.",['cs.CV']
Eikonal Region-based Active Contours for Image Segmentation,"The minimal path model based on the Eikonal partial differential equation
(PDE) has served as a fundamental tool for the applications of image
segmentation and boundary detection in the passed three decades. However, the
existing minimal paths-based image segmentation approaches commonly rely on the
image boundary features, potentially limiting their performance in some
situations. In this paper, we introduce a new variational image segmentation
model based on the minimal path framework and the Eikonal PDE, where the
region-based functional that defines the homogeneity criteria can be taken into
account for estimating the associated geodesic paths. This is done by
establishing a geodesic curve interpretation to the region-based active contour
evolution problem. The image segmentation processing is carried out in an
iterative manner in our approach. A crucial ingredient in each iteration is to
construct an asymmetric Randers geodesic metric using a sufficiently small
vector field, such that a set of geodesic paths can be tracked from the
geodesic distance map which is the solution to an Eikonal PDE. The object
boundary can be delineated by the concatenation of the final geodesic paths. We
invoke the Finsler variant of the fast marching method to estimate the geodesic
distance map, yielding an efficient implementation of the proposed Eikonal
region-based active contour model. Experimental results on both of the
synthetic and real images exhibit that our model indeed achieves encouraging
segmentation performance.","['cs.CV', 'cs.CG']"
Multi-focus Image Fusion Based on Similarity Characteristics,"A novel multi-focus image fusion algorithm performed in spatial domain based
on similarity characteristics is proposed incorporating with region
segmentation. In this paper, a new similarity measure is developed based on the
structural similarity (SSIM) index, which is more suitable for multi-focus
image segmentation. Firstly, the SSNSIM map is calculated between two input
images. Then we segment the SSNSIM map using watershed method, and merge the
small homogeneous regions with fuzzy c-means clustering algorithm (FCM). For
three source images, a joint region segmentation method based on segmentation
of two images is used to obtain the final segmentation result. Finally, the
corresponding segmented regions of the source images are fused according to
their average gradient. The performance of the image fusion method is evaluated
by several criteria including spatial frequency, average gradient, entropy,
edge retention etc. The evaluation results indicate that the proposed method is
effective and has good visual perception.","['cs.CV', 'eess.IV']"
What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance,"There is active research targeting local image manipulations that can fool
deep neural networks (DNNs) into producing incorrect results. This paper
examines a type of global image manipulation that can produce similar adverse
effects. Specifically, we explore how strong color casts caused by incorrectly
applied computational color constancy - referred to as white balance (WB) in
photography - negatively impact the performance of DNNs targeting image
segmentation and classification. In addition, we discuss how existing image
augmentation methods used to improve the robustness of DNNs are not well suited
for modeling WB errors. To address this problem, a novel augmentation method is
proposed that can emulate accurate color constancy degradation. We also explore
pre-processing training and testing images with a recent WB correction
algorithm to reduce the effects of incorrectly white-balanced images. We
examine both augmentation and pre-processing strategies on different datasets
and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K
datasets.",['cs.CV']
LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation,"Semantic image segmentation plays a pivotal role in many vision applications
including autonomous driving and medical image analysis. Most of the former
approaches move towards enhancing the performance in terms of accuracy with a
little awareness of computational efficiency. In this paper, we introduce
LiteSeg, a lightweight architecture for semantic image segmentation. In this
work, we explore a new deeper version of Atrous Spatial Pyramid Pooling module
(ASPP) and apply short and long residual connections, and depthwise separable
convolution, resulting in a faster and efficient model. LiteSeg architecture is
introduced and tested with multiple backbone networks as Darknet19, MobileNet,
and ShuffleNet to provide multiple trade-offs between accuracy and
computational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone
network, achieves an accuracy of 67.81% mean intersection over union at 161
frames per second with $640 \times 360$ resolution on the Cityscapes dataset.",['cs.CV']
Greenery Segmentation In Urban Images By Deep Learning,"Vegetation is a relevant feature in the urban scenery and its awareness can
be measured in an image by the Green View Index (GVI). Previous approaches to
estimate the GVI were based upon heuristics image processing approaches and
recently by deep learning networks (DLN). By leveraging some recent DLN
architectures tuned to the image segmentation problem and exploiting a
weighting strategy in the loss function (LF) we improved previously reported
results in similar datasets.","['cs.CV', 'cs.LG', 'I.4.6, I.5.4', 'I.4.6; I.5.4']"
Computing Valid p-values for Image Segmentation by Selective Inference,"Image segmentation is one of the most fundamental tasks of computer vision.
In many practical applications, it is essential to properly evaluate the
reliability of individual segmentation results. In this study, we propose a
novel framework to provide the statistical significance of segmentation results
in the form of p-values. Specifically, we consider a statistical hypothesis
test for determining the difference between the object and the background
regions. This problem is challenging because the difference can be deceptively
large (called segmentation bias) due to the adaptation of the segmentation
algorithm to the data. To overcome this difficulty, we introduce a statistical
approach called selective inference, and develop a framework to compute valid
p-values in which the segmentation bias is properly accounted for. Although the
proposed framework is potentially applicable to various segmentation
algorithms, we focus in this paper on graph cut-based and threshold-based
segmentation algorithms, and develop two specific methods to compute valid
p-values for the segmentation results obtained by these algorithms. We prove
the theoretical validity of these two methods and demonstrate their
practicality by applying them to segmentation problems for medical images.","['cs.CV', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']"
End-to-end Training of CNN-CRF via Differentiable Dual-Decomposition,"Modern computer vision (CV) is often based on convolutional neural networks
(CNNs) that excel at hierarchical feature extraction. The previous generation
of CV approaches was often based on conditional random fields (CRFs) that excel
at modeling flexible higher order interactions. As their benefits are
complementary they are often combined. However, these approaches generally use
mean-field approximations and thus, arguably, did not directly optimize the
real problem. Here we revisit dual-decomposition-based approaches to CRF
optimization, an alternative to the mean-field approximation. These algorithms
can efficiently and exactly solve sub-problems and directly optimize a convex
upper bound of the real problem, providing optimality certificates on the way.
Our approach uses a novel fixed-point iteration algorithm which enjoys
dual-monotonicity, dual-differentiability and high parallelism. The whole
system, CRF and CNN can thus be efficiently trained using back-propagation. We
demonstrate the effectiveness of our system on semantic image segmentation,
showing consistent improvement over baseline models.","['cs.CV', 'cs.LG']"
OASIS: One-pass aligned Atlas Set for Image Segmentation,"Medical image segmentation is a fundamental task in medical image analysis.
Despite that deep convolutional neural networks have gained stellar performance
in this challenging task, they typically rely on large labeled datasets, which
have limited their extension to customized applications. By revisiting the
superiority of atlas based segmentation methods, we present a new framework of
One-pass aligned Atlas Set for Images Segmentation (OASIS). To address the
problem of time-consuming iterative image registration used for atlas warping,
the proposed method takes advantage of the power of deep learning to achieve
one-pass image registration. In addition, by applying label constraint, OASIS
also makes the registration process to be focused on the regions to be
segmented for improving the performance of segmentation. Furthermore, instead
of using image based similarity for label fusion, which can be distracted by
the large background areas, we propose a novel strategy to compute the label
similarity based weights for label fusion. Our experimental results on the
challenging task of prostate MR image segmentation demonstrate that OASIS is
able to significantly increase the segmentation performance compared to other
state-of-the-art methods.",['cs.CV']
Online Normalization for Training Neural Networks,"Online Normalization is a new technique for normalizing the hidden
activations of a neural network. Like Batch Normalization, it normalizes the
sample dimension. While Online Normalization does not use batches, it is as
accurate as Batch Normalization. We resolve a theoretical limitation of Batch
Normalization by introducing an unbiased technique for computing the gradient
of normalized activations. Online Normalization works with automatic
differentiation by adding statistical normalization as a primitive. This
technique can be used in cases not covered by some other normalizers, such as
recurrent networks, fully connected networks, and networks with activation
memory requirements prohibitive for batching. We show its applications to image
classification, image segmentation, and language modeling. We present formal
proofs and experimental results on ImageNet, CIFAR, and PTB datasets.","['cs.LG', 'stat.ML']"
PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module,"LIDAR point clouds and RGB-images are both extremely essential for 3D object
detection. So many state-of-the-art 3D detection algorithms dedicate in fusing
these two types of data effectively. However, their fusion methods based on
Birds Eye View (BEV) or voxel format are not accurate. In this paper, we
propose a novel fusion approach named Point-based Attentive Cont-conv
Fusion(PACF) module, which fuses multi-sensor features directly on 3D points.
Except for continuous convolution, we additionally add a Point-Pooling and an
Attentive Aggregation to make the fused features more expressive. Moreover,
based on the PACF module, we propose a 3D multi-sensor multi-task network
called Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image
segmentation and 3D object detection tasks. PI-RCNN employs a segmentation
sub-network to extract full-resolution semantic feature maps from images and
then fuses the multi-sensor features via powerful PACF module. Beneficial from
the effectiveness of the PACF module and the expressive semantic features from
the segmentation module, PI-RCNN can improve much in 3D object detection. We
demonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D
Detection benchmark, and our method can achieve state-of-the-art on the metric
of 3D AP.",['cs.CV']
End to End Trainable Active Contours via Differentiable Rendering,"We present an image segmentation method that iteratively evolves a polygon.
At each iteration, the vertices of the polygon are displaced based on the local
value of a 2D shift map that is inferred from the input image via an
encoder-decoder architecture. The main training loss that is used is the
difference between the polygon shape and the ground truth segmentation mask.
The network employs a neural renderer to create the polygon from its vertices,
making the process fully differentiable. We demonstrate that our method
outperforms the state of the art segmentation networks and deep active contour
solutions in a variety of benchmarks, including medical imaging and aerial
images. Our code is available at https://github.com/shirgur/ACDRNet.",['cs.CV']
Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages,"Cutting and pasting image segments feels intuitive: the choice of source
templates gives artists flexibility in recombining existing source material.
Formally, this process takes an image set as input and outputs a collage of the
set elements. Such selection from sets of source templates does not fit easily
in classical convolutional neural models requiring inputs of fixed size.
Inspired by advances in attention and set-input machine learning, we present a
novel architecture that can generate in one forward pass image collages of
source templates using set-structured representations. This paper has the
following contributions: (i) a novel framework for image generation called
Memory Attentive Generation of Image Collages (MAGIC) which gives artists new
ways to create digital collages; (ii) from the machine-learning perspective, we
show a novel Generative Adversarial Networks (GAN) architecture that uses
Set-Transformer layers and set-pooling to blend sets of random image samples -
a hybrid non-parametric approach.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
Pixel Level Data Augmentation for Semantic Image Segmentation using Generative Adversarial Networks,"Semantic segmentation is one of the basic topics in computer vision, it aims
to assign semantic labels to every pixel of an image. Unbalanced semantic label
distribution could have a negative influence on segmentation accuracy. In this
paper, we investigate using data augmentation approach to balance the semantic
label distribution in order to improve segmentation performance. We propose
using generative adversarial networks (GANs) to generate realistic images for
improving the performance of semantic segmentation networks. Experimental
results show that the proposed method can not only improve segmentation
performance on those classes with low accuracy, but also obtain 1.3% to 2.1%
increase in average segmentation accuracy. It shows that this augmentation
method can boost accuracy and be easily applicable to any other segmentation
models.",['cs.CV']
Hierarchical Attention Networks for Medical Image Segmentation,"The medical image is characterized by the inter-class indistinction, high
variability, and noise, where the recognition of pixels is challenging. Unlike
previous self-attention based methods that capture context information from one
level, we reformulate the self-attention mechanism from the view of the
high-order graph and propose a novel method, namely Hierarchical Attention
Network (HANet), to address the problem of medical image segmentation.
Concretely, an HA module embedded in the HANet captures context information
from neighbors of multiple levels, where these neighbors are extracted from the
high-order graph. In the high-order graph, there will be an edge between two
nodes only if the correlation between them is high enough, which naturally
reduces the noisy attention information caused by the inter-class
indistinction. The proposed HA module is robust to the variance of input and
can be flexibly inserted into the existing convolution neural networks. We
conduct experiments on three medical image segmentation tasks including optic
disc/cup segmentation, blood vessel segmentation, and lung segmentation.
Extensive results show our method is more effective and robust than the
existing state-of-the-art methods.",['cs.CV']
Iteratively-Refined Interactive 3D Medical Image Segmentation with Multi-Agent Reinforcement Learning,"Existing automatic 3D image segmentation methods usually fail to meet the
clinic use. Many studies have explored an interactive strategy to improve the
image segmentation performance by iteratively incorporating user hints.
However, the dynamic process for successive interactions is largely ignored. We
here propose to model the dynamic process of iterative interactive image
segmentation as a Markov decision process (MDP) and solve it with reinforcement
learning (RL). Unfortunately, it is intractable to use single-agent RL for
voxel-wise prediction due to the large exploration space. To reduce the
exploration space to a tractable size, we treat each voxel as an agent with a
shared voxel-level behavior strategy so that it can be solved with multi-agent
reinforcement learning. An additional advantage of this multi-agent model is to
capture the dependency among voxels for segmentation task. Meanwhile, to enrich
the information of previous segmentations, we reserve the prediction
uncertainty in the state space of MDP and derive an adjustment action space
leading to a more precise and finer segmentation. In addition, to improve the
efficiency of exploration, we design a relative cross-entropy gain-based reward
to update the policy in a constrained direction. Experimental results on
various medical datasets have shown that our method significantly outperforms
existing state-of-the-art methods, with the advantage of fewer interactions and
a faster convergence.",['cs.CV']
Thickened 2D Networks for Efficient 3D Medical Image Segmentation,"There has been a debate in 3D medical image segmentation on whether to use 2D
or 3D networks, where both pipelines have advantages and disadvantages. 2D
methods enjoy a low inference time and greater transfer-ability while 3D
methods are superior in performance for hard targets requiring contextual
information. This paper investigates efficient 3D segmentation from another
perspective, which uses 2D networks to mimic 3D segmentation. To compensate the
lack of contextual information in 2D manner, we propose to thicken the 2D
network inputs by feeding multiple slices as multiple channels into 2D networks
and thus 3D contextual information is incorporated. We also put forward to use
early-stage multiplexing and slice sensitive attention to solve the confusion
problem of information loss which occurs when 2D networks face thickened
inputs. With this design, we achieve a higher performance while maintaining a
lower inference latency on a few abdominal organs from CT scans, in particular
when the organ has a peculiar 3D shape and thus strongly requires contextual
information, demonstrating our method's effectiveness and ability in capturing
3D information. We also point out that ""thickened"" 2D inputs pave a new method
of 3D segmentation, and look forward to more efforts in this direction.
Experiments on segmenting a few abdominal targets in particular blood vessels
which require strong 3D contexts demonstrate the advantages of our approach.",['cs.CV']
Identify the cells' nuclei based on the deep learning neural network,"Identify the cells' nuclei is the important point for most medical analyses.
To assist doctors finding the accurate cell' nuclei location automatically is
highly demanded in the clinical practice. Recently, fully convolutional neural
network (FCNs) serve as the back-bone in many image segmentation, like liver
and tumer segmentation in medical field, human body block in technical filed.
The cells' nuclei identification task is also kind of image segmentation. To
achieve this, we prefer to use deep learning algorithms. we construct three
general frameworks, one is Mask Region-based Convolutional Neural Network (Mask
RCNN), which has the high performance in many image segmentations, one is
U-net, which has the high generalization performance on small dataset and the
other is DenseUNet, which is mixture network architecture with Dense Net and
U-net. we compare the performance of these three frameworks. And we evaluated
our method on the dataset of data science bowl 2018 challenge. For single model
without any ensemble, they all have good performance.","['cs.CV', 'cs.LG', 'eess.IV']"
Automated Weed Detection in Aerial Imagery with Context,"In this paper, we demonstrate the ability to discriminate between cultivated
maize plant and grass or grass-like weed image segments using the context
surrounding the image segments. While convolutional neural networks have
brought state of the art accuracies within object detection, errors arise when
objects in different classes share similar features. This scenario often occurs
when objects in images are viewed at too small of a scale to discern distinct
differences in features, causing images to be incorrectly classified or
localized. To solve this problem, we will explore using context when
classifying image segments. This technique involves feeding a convolutional
neural network a central square image along with a border of its direct
surroundings at train and test times. This means that although images are
labelled at a smaller scale to preserve accurate localization, the network
classifies the images and learns features that include the wider context. We
demonstrate the benefits of this context technique in the object detection task
through a case study of grass (foxtail) and grass-like (yellow nutsedge) weed
detection in maize fields. In this standard situation, adding context alone
nearly halved the error of the neural network from 7.1% to 4.3%. After only one
epoch with context, the network also achieved a higher accuracy than the
network without context did after 50 epochs. The benefits of using the context
technique are likely to particularly evident in agricultural contexts in which
parts (such as leaves) of several plants may appear similar when not taking
into account the context in which those parts appear.","['cs.CV', 'eess.IV']"
An Approach for Adaptive Automatic Threat Recognition Within 3D Computed Tomography Images for Baggage Security Screening,"The screening of baggage using X-ray scanners is now routine in aviation
security with automatic threat detection approaches, based on 3D X-ray computed
tomography (CT) images, known as Automatic Threat Recognition (ATR) within the
aviation security industry. These current strategies use pre-defined threat
material signatures in contrast to adaptability towards new and emerging threat
signatures. To address this issue, the concept of adaptive automatic threat
recognition (AATR) was proposed in previous work. In this paper, we present a
solution to AATR based on such X-ray CT baggage scan imagery. This aims to
address the issues of rapidly evolving threat signatures within the screening
requirements. Ideally, the detection algorithms deployed within the security
scanners should be readily adaptable to different situations with varying
requirements of threat characteristics (e.g., threat material, physical
properties of objects). We tackle this issue using a novel adaptive machine
learning methodology with our solution consisting of a multi-scale 3D CT image
segmentation algorithm, a multi-class support vector machine (SVM) classifier
for object material recognition and a strategy to enable the adaptability of
our approach. Experiments are conducted on both open and sequestered 3D CT
baggage image datasets specifically collected for the AATR study. Our proposed
approach performs well on both recognition and adaptation. Overall our approach
can achieve the probability of detection around 90% with a probability of false
alarm below 20%. Our AATR shows the capabilities of adapting to varying types
of materials, even the unknown materials which are not available in the
training data, adapting to varying required probability of detection and
adapting to varying scales of the threat object.","['cs.CV', 'cs.SY']"
Automatic Image Co-Segmentation: A Survey,"Image co-segmentation is important for its advantage of alleviating the
ill-pose nature of image segmentation through exploring the correlation between
related images. Many automatic image co-segmentation algorithms have been
developed in the last decade, which are investigated comprehensively in this
paper. We firstly analyze visual/semantic cues for guiding image
co-segmentation, including object cues and correlation cues. Then we describe
the traditional methods in three categories of object elements based, object
regions/contours based, common object model based. In the next part, deep
learning based methods are reviewed. Furthermore, widely used test datasets and
evaluation criteria are introduced and the reported performances of the
surveyed algorithms are compared with each other. Finally, we discuss the
current challenges and possible future directions and conclude the paper.
Hopefully, this comprehensive investigation will be helpful for the development
of image co-segmentation technique.",['cs.CV']
Signal Clustering with Class-independent Segmentation,"Radar signals have been dramatically increasing in complexity, limiting the
source separation ability of traditional approaches. In this paper we propose a
Deep Learning-based clustering method, which encodes concurrent signals into
images, and, for the first time, tackles clustering with image segmentation.
Novel loss functions are introduced to optimize a Neural Network to separate
the input pulses into pure and non-fragmented clusters. Outperforming a variety
of baselines, the proposed approach is capable of clustering inputs directly
with a Neural Network, in an end-to-end fashion.","['cs.CV', 'cs.LG', 'eess.SP']"
Cost-efficient segmentation of electron microscopy images using active learning,"Over the last decade, electron microscopy has improved up to a point that
generating high quality gigavoxel sized datasets only requires a few hours.
Automated image analysis, particularly image segmentation, however, has not
evolved at the same pace. Even though state-of-the-art methods such as U-Net
and DeepLab have improved segmentation performance substantially, the required
amount of labels remains too expensive. Active learning is the subfield in
machine learning that aims to mitigate this burden by selecting the samples
that require labeling in a smart way. Many techniques have been proposed,
particularly for image classification, to increase the steepness of learning
curves. In this work, we extend these techniques to deep CNN based image
segmentation. Our experiments on three different electron microscopy datasets
show that active learning can improve segmentation quality by 10 to 15% in
terms of Jaccard score compared to standard randomized sampling.",['cs.CV']
Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning,"Convolutional neural networks (CNNs) have been widely and successfully used
for medical image segmentation. However, CNNs are typically considered to
require large numbers of dedicated expert-segmented training volumes, which may
be limiting in practice. This work investigates whether clinically obtained
segmentations which are readily available in picture archiving and
communication systems (PACS) could provide a possible source of data to train a
CNN for segmentation of organs-at-risk (OARs) in radiotherapy treatment
planning. In such data, delineations of structures deemed irrelevant to the
target clinical use may be lacking. To overcome this issue, we use multi-label
instead of multi-class segmentation. We empirically assess how many clinical
delineations would be sufficient to train a CNN for the segmentation of OARs
and find that increasing the training set size beyond a limited number of
images leads to sharply diminishing returns. Moreover, we find that by using
multi-label segmentation, missing structures in the reference standard do not
have a negative effect on overall segmentation accuracy. These results indicate
that segmentations obtained in a clinical workflow can be used to train an
accurate OAR segmentation model.","['cs.LG', 'cs.CV', 'stat.ML']"
FaultNet: Faulty Rail-Valves Detection using Deep Learning and Computer Vision,"Regular inspection of rail valves and engines is an important task to ensure
the safety and efficiency of railway networks around the globe. Over the past
decade, computer vision and pattern recognition based techniques have gained
traction for such inspection and defect detection tasks. An automated
end-to-end trained system can potentially provide a low-cost, high throughput,
and cheap alternative to manual visual inspection of these components. However,
such systems require a huge amount of defective images for networks to
understand complex defects. In this paper, a multi-phase deep learning based
technique is proposed to perform accurate fault detection of rail-valves. Our
approach uses a two-step method to perform high precision image segmentation of
rail-valves resulting in pixel-wise accurate segmentation. Thereafter, a
computer vision technique is used to identify faulty valves. We demonstrate
that the proposed approach results in improved detection performance when
compared to current state-of-theart techniques used in fault detection.",['cs.CV']
QANet -- Quality Assurance Network for Image Segmentation,"We introduce a novel Deep Learning framework, which quantitatively estimates
image segmentation quality without the need for human inspection or labeling.
We refer to this method as a Quality Assurance Network -- QANet. Specifically,
given an image and a `proposed' corresponding segmentation, obtained by any
method including manual annotation, the QANet solves a regression problem in
order to estimate a predefined quality measure with respect to the unknown
ground truth. The QANet is by no means yet another segmentation method.
Instead, it performs a multi-level, multi-feature comparison of an
image-segmentation pair based on a unique network architecture, called the
RibCage.
  To demonstrate the strength of the QANet, we addressed the evaluation of
instance segmentation using two different datasets from different domains,
namely, high throughput live cell microscopy images from the Cell Segmentation
Benchmark and natural images of plants from the Leaf Segmentation Challenge.
While synthesized segmentations were used to train the QANet, it was tested on
segmentations obtained by publicly available methods that participated in the
different challenges. We show that the QANet accurately estimates the scores of
the evaluated segmentations with respect to the hidden ground truth, as
published by the challenges' organizers.
  The code is available at: TBD.","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']"
"One Network to Segment Them All: A General, Lightweight System for Accurate 3D Medical Image Segmentation","Many recent medical segmentation systems rely on powerful deep learning
models to solve highly specific tasks. To maximize performance, it is standard
practice to evaluate numerous pipelines with varying model topologies,
optimization parameters, pre- & postprocessing steps, and even model cascades.
It is often not clear how the resulting pipeline transfers to different tasks.
We propose a simple and thoroughly evaluated deep learning framework for
segmentation of arbitrary medical image volumes. The system requires no
task-specific information, no human interaction and is based on a fixed model
topology and a fixed hyperparameter set, eliminating the process of model
selection and its inherent tendency to cause method-level over-fitting. The
system is available in open source and does not require deep learning expertise
to use. Without task-specific modifications, the system performed better than
or similar to highly specialized deep learning methods across 3 separate
segmentation tasks. In addition, it ranked 5-th and 6-th in the first and
second round of the 2018 Medical Segmentation Decathlon comprising another 10
tasks. The system relies on multi-planar data augmentation which facilitates
the application of a single 2D architecture based on the familiar U-Net.
Multi-planar training combines the parameter efficiency of a 2D fully
convolutional neural network with a systematic train- and test-time
augmentation scheme, which allows the 2D model to learn a representation of the
3D image volume that fosters generalization.","['cs.LG', 'eess.IV']"
Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice,"The Dice score and Jaccard index are commonly used metrics for the evaluation
of segmentation tasks in medical imaging. Convolutional neural networks trained
for image segmentation tasks are usually optimized for (weighted)
cross-entropy. This introduces an adverse discrepancy between the learning
optimization objective (the loss) and the end target metric. Recent works in
computer vision have proposed soft surrogates to alleviate this discrepancy and
directly optimize the desired metric, either through relaxations (soft-Dice,
soft-Jaccard) or submodular optimization (Lov\'asz-softmax). The aim of this
study is two-fold. First, we investigate the theoretical differences in a risk
minimization framework and question the existence of a weighted cross-entropy
loss with weights theoretically optimized to surrogate Dice or Jaccard. Second,
we empirically investigate the behavior of the aforementioned loss functions
w.r.t. evaluation with Dice score and Jaccard index on five medical
segmentation tasks. Through the application of relative approximation bounds,
we show that all surrogates are equivalent up to a multiplicative factor, and
that no optimal weighting of cross-entropy exists to approximate Dice or
Jaccard measures. We validate these findings empirically and show that, while
it is important to opt for one of the target metric surrogates rather than a
cross-entropy-based loss, the choice of the surrogate does not make a
statistical difference on a wide range of medical segmentation tasks.","['cs.CV', 'cs.LG', 'eess.IV']"
Semi-Supervised Medical Image Segmentation via Learning Consistency under Transformations,"The scarcity of labeled data often limits the application of supervised deep
learning techniques for medical image segmentation. This has motivated the
development of semi-supervised techniques that learn from a mixture of labeled
and unlabeled images. In this paper, we propose a novel semi-supervised method
that, in addition to supervised learning on labeled training images, learns to
predict segmentations consistent under a given class of transformations on both
labeled and unlabeled images. More specifically, in this work we explore
learning equivariance to elastic deformations. We implement this through: 1) a
Siamese architecture with two identical branches, each of which receives a
differently transformed image, and 2) a composite loss function with a
supervised segmentation loss term and an unsupervised term that encourages
segmentation consistency between the predictions of the two branches. We
evaluate the method on a public dataset of chest radiographs with segmentations
of anatomical structures using 5-fold cross-validation. The proposed method
reaches significantly higher segmentation accuracy compared to supervised
learning. This is due to learning transformation consistency on both labeled
and unlabeled images, with the latter contributing the most. We achieve the
performance comparable to state-of-the-art chest X-ray segmentation methods
while using substantially fewer labeled images.","['cs.CV', 'cs.LG']"
Deep Co-Training for Semi-Supervised Image Segmentation,"In this paper, we aim to improve the performance of semantic image
segmentation in a semi-supervised setting in which training is effectuated with
a reduced set of annotated images and additional non-annotated images. We
present a method based on an ensemble of deep segmentation models. Each model
is trained on a subset of the annotated data, and uses the non-annotated images
to exchange information with the other models, similar to co-training. Even if
each model learns on the same non-annotated images, diversity is preserved with
the use of adversarial samples. Our results show that this ability to
simultaneously train models, which exchange knowledge while preserving
diversity, leads to state-of-the-art results on two challenging medical image
datasets.",['cs.CV']
Gated CRF Loss for Weakly Supervised Semantic Image Segmentation,"State-of-the-art approaches for semantic segmentation rely on deep
convolutional neural networks trained on fully annotated datasets, that have
been shown to be notoriously expensive to collect, both in terms of time and
money. To remedy this situation, weakly supervised methods leverage other forms
of supervision that require substantially less annotation effort, but they
typically present an inability to predict precise object boundaries due to
approximate nature of the supervisory signals in those regions. While great
progress has been made in improving the performance, many of these weakly
supervised methods are highly tailored to their own specific settings. This
raises challenges in reusing algorithms and making steady progress. In this
paper, we intentionally avoid such practices when tackling weakly supervised
semantic segmentation. In particular, we train standard neural networks with
partial cross-entropy loss function for the labeled pixels and our proposed
Gated CRF loss for the unlabeled pixels. The Gated CRF loss is designed to
deliver several important assets: 1) it enables flexibility in the kernel
construction to mask out influence from undesired pixel positions; 2) it
offloads learning contextual relations to CNN and concentrates on semantic
boundaries; 3) it does not rely on high-dimensional filtering and thus has a
simple implementation. Throughout the paper we present the advantages of the
loss function, analyze several aspects of weakly supervised training, and show
that our `purist' approach achieves state-of-the-art performance for both
click-based and scribble-based annotations.",['cs.CV']
Domain Generalization via Model-Agnostic Learning of Semantic Features,"Generalization capability to unseen domains is crucial for machine learning
models when deploying to real-world conditions. We investigate the challenging
problem of domain generalization, i.e., training a model on multi-domain source
data such that it can directly generalize to target domains with unknown
statistics. We adopt a model-agnostic learning paradigm with gradient-based
meta-train and meta-test procedures to expose the optimization to domain shift.
Further, we introduce two complementary losses which explicitly regularize the
semantic structure of the feature space. Globally, we align a derived soft
confusion matrix to preserve general knowledge about inter-class relationships.
Locally, we promote domain-independent class-specific cohesion and separation
of sample features with a metric-learning component. The effectiveness of our
method is demonstrated with new state-of-the-art results on two common object
recognition benchmarks. Our method also shows consistent improvement on a
medical image segmentation task.",['cs.CV']
PT-ResNet: Perspective Transformation-Based Residual Network for Semantic Road Image Segmentation,"Semantic road region segmentation is a high-level task, which paves the way
towards road scene understanding. This paper presents a residual network
trained for semantic road segmentation. Firstly, we represent the projections
of road disparities in the v-disparity map as a linear model, which can be
estimated by optimizing the v-disparity map using dynamic programming. This
linear model is then utilized to reduce the redundant information in the left
and right road images. The right image is also transformed into the left
perspective view, which greatly enhances the road surface similarity between
the two images. Finally, the processed stereo images and their disparity maps
are concatenated to create a set of 3D images, which are then utilized to train
our neural network. The experimental results illustrate that our network
achieves a maximum F1-measure of approximately 91.19% when analyzing the images
from the KITTI road dataset.","['cs.CV', 'cs.LG', 'cs.RO', 'eess.IV']"
Multiple Light Source Dataset for Colour Research,"We present a collection of 24 multiple object scenes each recorded under 18
multiple light source illumination scenarios. The illuminants are varying in
dominant spectral colours, intensity and distance from the scene. We mainly
address the realistic scenarios for evaluation of computational colour
constancy algorithms, but also have aimed to make the data as general as
possible for computational colour science and computer vision. Along with the
images of the scenes, we provide spectral characteristics of the camera, light
sources and the objects and include pixel-by-pixel ground truth annotation of
uniformly coloured object surfaces thus making this useful for benchmarking
colour-based image segmentation algorithms. The dataset is freely available at
https://github.com/visillect/mls-dataset.",['cs.CV']
U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging,"Neural networks are becoming more and more popular for the analysis of
physiological time-series. The most successful deep learning systems in this
domain combine convolutional and recurrent layers to extract useful features to
model temporal relations. Unfortunately, these recurrent models are difficult
to tune and optimize. In our experience, they often require task-specific
modifications, which makes them challenging to use for non-experts. We propose
U-Time, a fully feed-forward deep learning approach to physiological time
series segmentation developed for the analysis of sleep data. U-Time is a
temporal fully convolutional network based on the U-Net architecture that was
originally proposed for image segmentation. U-Time maps sequential inputs of
arbitrary length to sequences of class labels on a freely chosen temporal
scale. This is done by implicitly classifying every individual time-point of
the input signal and aggregating these classifications over fixed intervals to
form the final predictions. We evaluated U-Time for sleep stage classification
on a large collection of sleep electroencephalography (EEG) datasets. In all
cases, we found that U-Time reaches or outperforms current state-of-the-art
deep learning models while being much more robust in the training process and
without requiring architecture or hyperparameter adaptation across tasks.","['cs.LG', 'eess.SP', 'stat.ML']"
Anchor Diffusion for Unsupervised Video Object Segmentation,"Unsupervised video object segmentation has often been tackled by methods
based on recurrent neural networks and optical flow. Despite their complexity,
these kinds of approaches tend to favour short-term temporal dependencies and
are thus prone to accumulating inaccuracies, which cause drift over time.
Moreover, simple (static) image segmentation models, alone, can perform
competitively against these methods, which further suggests that the way
temporal dependencies are modelled should be reconsidered. Motivated by these
observations, in this paper we explore simple yet effective strategies to model
long-term temporal dependencies. Inspired by the non-local operators of [70],
we introduce a technique to establish dense correspondences between pixel
embeddings of a reference ""anchor"" frame and the current one. This allows the
learning of pairwise dependencies at arbitrarily long distances without
conditioning on intermediate frames. Without online supervision, our approach
can suppress the background and precisely segment the foreground object even in
challenging scenarios, while maintaining consistent performance over time. With
a mean IoU of $81.7\%$, our method ranks first on the DAVIS-2016 leaderboard of
unsupervised methods, while still being competitive against state-of-the-art
online semi-supervised approaches. We further evaluate our method on the FBMS
dataset and the ViSal video saliency dataset, showing results competitive with
the state of the art.",['cs.CV']
Deeply Self-Supervised Contour Embedded Neural Network Applied to Liver Segmentation,"Objective: Herein, a neural network-based liver segmentation algorithm is
proposed, and its performance was evaluated using abdominal computed tomography
(CT) images. Methods: A fully convolutional network was developed to overcome
the volumetric image segmentation problem. To guide a neural network to
accurately delineate a target liver object, the network was deeply supervised
by applying the adaptive self-supervision scheme to derive the essential
contour, which acted as a complement with the global shape. The discriminative
contour, shape, and deep features were internally merged for the segmentation
results. Results and Conclusion: 160 abdominal CT images were used for training
and validation. The quantitative evaluation of the proposed network was
performed through an eight-fold cross-validation. The result showed that the
method, which uses the contour feature, segmented the liver more accurately
than the state-of-the-art with a 2.13% improvement in the dice score.
Significance: In this study, a new framework was introduced to guide a neural
network and learn complementary contour features. The proposed neural network
demonstrates that the guided contour features can significantly improve the
performance of the segmentation task.","['cs.CV', '68U10']"
Encoder-Decoder based CNN and Fully Connected CRFs for Remote Sensed Image Segmentation,"With the advancement of remote-sensed imaging large volumes of very high
resolution land cover images can now be obtained. Automation of object
recognition in these 2D images, however, is still a key issue. High intra-class
variance and low inter-class variance in Very High Resolution (VHR) images
hamper the accuracy of prediction in object recognition tasks. Most successful
techniques in various computer vision tasks recently are based on deep
supervised learning. In this work, a deep Convolutional Neural Network (CNN)
based on symmetric encoder-decoder architecture with skip connections is
employed for the 2D semantic segmentation of most common land cover object
classes - impervious surface, buildings, low vegetation, trees and cars. Atrous
convolutions are employed to have large receptive field in the proposed CNN
model. Further, the CNN outputs are post-processed using Fully Connected
Conditional Random Field (FCRF) model to refine the CNN pixel label
predictions. The proposed CNN-FCRF model achieves an overall accuracy of 90.5%
on the ISPRS Vaihingen Dataset.","['cs.CV', 'eess.IV']"
Deep Multiphase Level Set for Scene Parsing,"Recently, Fully Convolutional Network (FCN) seems to be the go-to
architecture for image segmentation, including semantic scene parsing. However,
it is difficult for a generic FCN to discriminate pixels around the object
boundaries, thus FCN based methods may output parsing results with inaccurate
boundaries. Meanwhile, level set based active contours are superior to the
boundary estimation due to the sub-pixel accuracy that they achieve. However,
they are quite sensitive to initial settings. To address these limitations, in
this paper we propose a novel Deep Multiphase Level Set (DMLS) method for
semantic scene parsing, which efficiently incorporates multiphase level sets
into deep neural networks. The proposed method consists of three modules, i.e.,
recurrent FCNs, adaptive multiphase level set, and deeply supervised learning.
More specifically, recurrent FCNs learn multi-level representations of input
images with different contexts. Adaptive multiphase level set drives the
discriminative contour for each semantic class, which makes use of the
advantages of both global and local information. In each time-step of the
recurrent FCNs, deeply supervised learning is incorporated for model training.
Extensive experiments on three public benchmarks have shown that our proposed
method achieves new state-of-the-art performances.",['cs.CV']
'Squeeze & Excite' Guided Few-Shot Segmentation of Volumetric Images,"Deep neural networks enable highly accurate image segmentation, but require
large amounts of manually annotated data for supervised training. Few-shot
learning aims to address this shortcoming by learning a new class from a few
annotated support examples. We introduce, a novel few-shot framework, for the
segmentation of volumetric medical images with only a few annotated slices.
Compared to other related works in computer vision, the major challenges are
the absence of pre-trained networks and the volumetric nature of medical scans.
We address these challenges by proposing a new architecture for few-shot
segmentation that incorporates 'squeeze & excite' blocks. Our two-armed
architecture consists of a conditioner arm, which processes the annotated
support input and generates a task-specific representation. This representation
is passed on to the segmenter arm that uses this information to segment the new
query image. To facilitate efficient interaction between the conditioner and
the segmenter arm, we propose to use 'channel squeeze & spatial excitation'
blocks - a light-weight computational module - that enables heavy interaction
between both the arms with negligible increase in model complexity. This
contribution allows us to perform image segmentation without relying on a
pre-trained model, which generally is unavailable for medical scans.
Furthermore, we propose an efficient strategy for volumetric segmentation by
optimally pairing a few slices of the support volume to all the slices of the
query volume. We perform experiments for organ segmentation on whole-body
contrast-enhanced CT scans from the Visceral Dataset. Our proposed model
outperforms multiple baselines and existing approaches with respect to the
segmentation accuracy by a significant margin. The source code is available at
https://github.com/abhi4ssj/few-shot-segmentation.",['cs.CV']
End-to-End Defect Detection in Automated Fiber Placement Based on Artificially Generated Data,"Automated fiber placement (AFP) is an advanced manufacturing technology that
increases the rate of production of composite materials. At the same time, the
need for adaptable and fast inline control methods of such parts raises.
Existing inspection systems make use of handcrafted filter chains and feature
detectors, tuned for a specific measurement methods by domain experts. These
methods hardly scale to new defects or different measurement devices. In this
paper, we propose to formulate AFP defect detection as an image segmentation
problem that can be solved in an end-to-end fashion using artificially
generated training data. We employ a probabilistic graphical model to generate
training images and annotations. We then train a deep neural network based on
recent architectures designed for image segmentation. This leads to an
appealing method that scales well with new defect types and measurement devices
and requires little real world data for training.","['cs.CV', 'cs.LG']"
Post-mortem Iris Recognition with Deep-Learning-based Image Segmentation,"This paper proposes the first known to us iris recognition methodology
designed specifically for post-mortem samples. We propose to use deep
learning-based iris segmentation models to extract highly irregular iris
texture areas in post-mortem iris images. We show how to use segmentation masks
predicted by neural networks in conventional, Gabor-based iris recognition
method, which employs circular approximations of the pupillary and limbic iris
boundaries. As a whole, this method allows for a significant improvement in
post-mortem iris recognition accuracy over the methods designed only for
ante-mortem irises, including the academic OSIRIS and commercial IriCore
implementations. The proposed method reaches the EER less than 1% for samples
collected up to 10 hours after death, when compared to 16.89% and 5.37% of EER
observed for OSIRIS and IriCore, respectively. For samples collected up to 369
hours post-mortem, the proposed method achieves the EER 21.45%, while 33.59%
and 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, the
method is tested on a database of iris images collected from ophthalmology
clinic patients, for which it also offers an advantage over the two other
algorithms. This work is the first step towards post-mortem-specific iris
recognition, which increases the chances of identification of deceased subjects
in forensic investigations. The new database of post-mortem iris images
acquired from 42 subjects, as well as the deep learning-based segmentation
models are made available along with the paper, to ensure all the results
presented in this manuscript are reproducible.",['cs.CV']
End-to-End Deep Convolutional Active Contours for Image Segmentation,"The Active Contour Model (ACM) is a standard image analysis technique whose
numerous variants have attracted an enormous amount of research attention
across multiple fields. Incorrectly, however, the ACM's
differential-equation-based formulation and prototypical dependence on user
initialization have been regarded as being largely incompatible with the
recently popular deep learning approaches to image segmentation. This paper
introduces the first tight unification of these two paradigms. In particular,
we devise Deep Convolutional Active Contours (DCAC), a truly end-to-end
trainable image segmentation framework comprising a Convolutional Neural
Network (CNN) and an ACM with learnable parameters. The ACM's Eulerian energy
functional includes per-pixel parameter maps predicted by the backbone CNN,
which also initializes the ACM. Importantly, both the CNN and ACM components
are fully implemented in TensorFlow, and the entire DCAC architecture is
end-to-end automatically differentiable and backpropagation trainable without
user intervention. As a challenging test case, we tackle the problem of
building instance segmentation in aerial images and evaluate DCAC on two
publicly available datasets, Vaihingen and Bing Huts. Our reseults demonstrate
that, for building segmentation, the DCAC establishes a new state-of-the-art
performance by a wide margin.",['cs.CV']
NeurReg: Neural Registration and Its Application to Image Segmentation,"Registration is a fundamental task in medical image analysis which can be
applied to several tasks including image segmentation, intra-operative
tracking, multi-modal image alignment, and motion analysis. Popular
registration tools such as ANTs and NiftyReg optimize an objective function for
each pair of images from scratch which is time-consuming for large images with
complicated deformation. Facilitated by the rapid progress of deep learning,
learning-based approaches such as VoxelMorph have been emerging for image
registration. These approaches can achieve competitive performance in a
fraction of a second on advanced GPUs. In this work, we construct a neural
registration framework, called NeurReg, with a hybrid loss of displacement
fields and data similarity, which substantially improves the current
state-of-the-art of registrations. Within the framework, we simulate various
transformations by a registration simulator which generates fixed image and
displacement field ground truth for training. Furthermore, we design three
segmentation frameworks based on the proposed registration framework: 1)
atlas-based segmentation, 2) joint learning of both segmentation and
registration tasks, and 3) multi-task learning with atlas-based segmentation as
an intermediate feature. Extensive experimental results validate the
effectiveness of the proposed NeurReg framework based on various metrics: the
endpoint error (EPE) of the predicted displacement field, mean square error
(MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dice
coefficient, uncertainty estimation, and the interpretability of the
segmentation. The proposed NeurReg improves registration accuracy with fast
inference speed, which can greatly accelerate related medical image analysis
tasks.","['cs.CV', 'cs.LG', 'cs.NE']"
CNN-based Semantic Segmentation using Level Set Loss,"Thesedays, Convolutional Neural Networks are widely used in semantic
segmentation. However, since CNN-based segmentation networks produce
low-resolution outputs with rich semantic information, it is inevitable that
spatial details (e.g., small bjects and fine boundary information) of
segmentation results will be lost. To address this problem, motivated by a
variational approach to image segmentation (i.e., level set theory), we propose
a novel loss function called the level set loss which is designed to refine
spatial details of segmentation results. To deal with multiple classes in an
image, we first decompose the ground truth into binary images. Note that each
binary image consists of background and regions belonging to a class. Then we
convert level set functions into class probability maps and calculate the
energy for each class. The network is trained to minimize the weighted sum of
the level set loss and the cross-entropy loss. The proposed level set loss
improves the spatial details of segmentation results in a time and memory
efficient way. Furthermore, our experimental results show that the proposed
loss function achieves better performance than previous approaches.",['cs.CV']
TFLMS: Large Model Support in TensorFlow by Graph Rewriting,"While accelerators such as GPUs have limited memory, deep neural networks are
becoming larger and will not fit with the memory limitation of accelerators for
training. We propose an approach to tackle this problem by rewriting the
computational graph of a neural network, in which swap-out and swap-in
operations are inserted to temporarily store intermediate results on CPU
memory. In particular, we first revise the concept of a computational graph by
defining a concrete semantics for variables in a graph. We then formally show
how to derive swap-out and swap-in operations from an existing graph and
present rules to optimize the graph. To realize our approach, we developed a
module in TensorFlow, named TFLMS. TFLMS is published as a pull request in the
TensorFlow repository for contributing to the TensorFlow community. With TFLMS,
we were able to train ResNet-50 and 3DUnet with 4.7x and 2x larger batch size,
respectively. In particular, we were able to train 3DUNet using images of size
of $192^3$ for image segmentation, which, without TFLMS, had been done only by
dividing the images to smaller images, which affects the accuracy.","['cs.LG', 'cs.AI', 'stat.ML']"
Overfitting of neural nets under class imbalance: Analysis and improvements for segmentation,"Overfitting in deep learning has been the focus of a number of recent works,
yet its exact impact on the behavior of neural networks is not well understood.
This study analyzes overfitting by examining how the distribution of logits
alters in relation to how much the model overfits. Specifically, we find that
when training with few data samples, the distribution of logit activations when
processing unseen test samples of an under-represented class tends to shift
towards and even across the decision boundary, while the over-represented class
seems unaffected. In image segmentation, foreground samples are often heavily
under-represented. We observe that sensitivity of the model drops as a result
of overfitting, while precision remains mostly stable. Based on our analysis,
we derive asymmetric modifications of existing loss functions and regularizers
including a large margin loss, focal loss, adversarial training and mixup,
which specifically aim at reducing the shift observed when embedding unseen
samples of the under-represented class. We study the case of binary
segmentation of brain tumor core and show that our proposed simple
modifications lead to significantly improved segmentation performance over the
symmetric variants.","['cs.LG', 'cs.CV', 'stat.ML']"
SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth,"A key limitation of deep convolutional neural networks (DCNN) based image
segmentation methods is the lack of generalizability. Manually traced training
images are typically required when segmenting organs in a new imaging modality
or from distinct disease cohort. The manual efforts can be alleviated if the
manually traced images in one imaging modality (e.g., MRI) are able to train a
segmentation network for another imaging modality (e.g., CT). In this paper, we
propose an end-to-end synthetic segmentation network (SynSeg-Net) to train a
segmentation network for a target imaging modality without having manual
labels. SynSeg-Net is trained by using (1) unpaired intensity images from
source and target modalities, and (2) manual labels only from source modality.
SynSeg-Net is enabled by the recent advances of cycle generative adversarial
networks (CycleGAN) and DCNN. We evaluate the performance of the SynSeg-Net on
two experiments: (1) MRI to CT splenomegaly synthetic segmentation for
abdominal images, and (2) CT to MRI total intracranial volume synthetic
segmentation (TICV) for brain images. The proposed end-to-end approach achieved
superior performance to two stage methods. Moreover, the SynSeg-Net achieved
comparable performance to the traditional segmentation network using target
modality labels in certain scenarios. The source code of SynSeg-Net is publicly
available (https://github.com/MASILab/SynSeg-Net).",['cs.CV']
Biomedical Image Segmentation by Retina-like Sequential Attention Mechanism Using Only A Few Training Images,"In this paper we propose a novel deep learning-based algorithm for biomedical
image segmentation which uses a sequential attention mechanism able to shift
the focus of attention across the image in a selective way, allowing subareas
which are more difficult to classify to be processed at increased resolution.
The spatial distribution of class information in each subarea is learned using
a retina-like representation where resolution decreases with distance from the
center of attention. The final segmentation is achieved by averaging class
predictions over overlapping subareas, utilizing the power of ensemble learning
to increase segmentation accuracy. Experimental results for semantic
segmentation task for which only a few training images are available show that
a CNN using the proposed method outperforms both a patch-based classification
CNN and a fully convolutional-based method.","['cs.CV', 'cs.NE']"
From Active Contours to Minimal Geodesic Paths: New Solutions to Active Contours Problems by Eikonal Equations,"In this chapter, we give an overview of part of our previous work based on
the minimal path framework and the Eikonal partial differential equation (PDE).
We show that by designing adequate Riemannian and Randers geodesic metrics the
minimal paths can be utilized to search for solutions to almost all of the
active contour problems and to the Euler-Mumford elastica problem, which allows
to blend the advantages from minimal geodesic paths and those original
approaches, i.e. the active contours and elastica curves. The proposed minimal
path-based models can be applied to deal with a broad variety of image analysis
tasks such as boundary detection, image segmentation and tubular structure
extraction. The numerical implementations for the computation of minimal paths
are known to be quite efficient thanks to the Eikonal solvers such as the
Finsler variant of the fast marching method.",['cs.CV']
Dual Adaptive Pyramid Network for Cross-Stain Histopathology Image Segmentation,"Supervised semantic segmentation normally assumes the test data being in a
similar data domain as the training data. However, in practice, the domain
mismatch between the training and unseen data could lead to a significant
performance drop. Obtaining accurate pixel-wise label for images in different
domains is tedious and labor intensive, especially for histopathology images.
In this paper, we propose a dual adaptive pyramid network (DAPNet) for
histopathological gland segmentation adapting from one stain domain to another.
We tackle the domain adaptation problem on two levels: 1) the image-level
considers the differences of image color and style; 2) the feature-level
addresses the spatial inconsistency between two domains. The two components are
implemented as domain classifiers with adversarial training. We evaluate our
new approach using two gland segmentation datasets with H&E and DAB-H stains
respectively. The extensive experiments and ablation study demonstrate the
effectiveness of our approach on the domain adaptive segmentation task. We show
that the proposed approach performs favorably against other state-of-the-art
methods.",['cs.CV']
Spatio-Temporal Convolutional LSTMs for Tumor Growth Prediction by Learning 4D Longitudinal Patient Data,"Prognostic tumor growth modeling via volumetric medical imaging observations
can potentially lead to better outcomes of tumor treatment and surgical
planning. Recent advances of convolutional networks have demonstrated higher
accuracy than traditional mathematical models in predicting future tumor
volumes. This indicates that deep learning-based techniques may have great
potentials on addressing such problem. However, current 2D patch-based modeling
approaches cannot make full use of the spatio-temporal imaging context of the
tumor's longitudinal 4D (3D + time) data. Moreover, they are incapable to
predict clinically-relevant tumor properties, other than volumes. In this
paper, we exploit to formulate the tumor growth process through convolutional
Long Short-Term Memory (ConvLSTM) that extract tumor's static imaging
appearances and capture its temporal dynamic changes within a single network.
We extend ConvLSTM into the spatio-temporal domain (ST-ConvLSTM) by jointly
learning the inter-slice 3D contexts and the longitudinal or temporal dynamics
from multiple patient studies. Our approach can incorporate other non-imaging
patient information in an end-to-end trainable manner. Experiments are
conducted on the largest 4D longitudinal tumor dataset of 33 patients to date.
Results validate that the ST-ConvLSTM produces a Dice score of 83.2%+-5.1% and
a RVD of 11.2%+-10.8%, both significantly outperforming (p<0.05) other compared
methods of linear model, ConvLSTM, and generative adversarial network (GAN)
under the metric of predicting future tumor volumes. Additionally, our new
method enables the prediction of both cell density and CT intensity numbers.
Last, we demonstrate the generalizability of ST-ConvLSTM by employing it in 4D
medical image segmentation task, which achieves an averaged Dice score of
86.3+-1.2% for left-ventricle segmentation in 4D ultrasound with 3 seconds per
patient.",['cs.CV']
Neural Style Transfer Improves 3D Cardiovascular MR Image Segmentation on Inconsistent Data,"Three-dimensional medical image segmentation is one of the most important
problems in medical image analysis and plays a key role in downstream diagnosis
and treatment. Recent years, deep neural networks have made groundbreaking
success in medical image segmentation problem. However, due to the high
variance in instrumental parameters, experimental protocols, and subject
appearances, the generalization of deep learning models is often hindered by
the inconsistency in medical images generated by different machines and
hospitals. In this work, we present StyleSegor, an efficient and easy-to-use
strategy to alleviate this inconsistency issue. Specifically, neural style
transfer algorithm is applied to unlabeled data in order to minimize the
differences in image properties including brightness, contrast, texture, etc.
between the labeled and unlabeled data. We also apply probabilistic adjustment
on the network output and integrate multiple predictions through ensemble
learning. On a publicly available whole heart segmentation benchmarking dataset
from MICCAI HVSMR 2016 challenge, we have demonstrated an elevated dice
accuracy surpassing current state-of-the-art method and notably, an improvement
of the total score by 29.91\%. StyleSegor is thus corroborated to be an
accurate tool for 3D whole heart segmentation especially on highly inconsistent
data, and is available at https://github.com/horsepurve/StyleSegor.",['cs.CV']
Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation,"Recent advances in generative models and adversarial training have led to a
flourishing image-to-image (I2I) translation literature. The current I2I
translation approaches require training images from the two domains that are
either all paired (supervised) or all unpaired (unsupervised). In practice,
obtaining paired training data in sufficient quantities is often very costly
and cumbersome. Therefore solutions that employ unpaired data, while less
accurate, are largely preferred. In this paper, we aim to bridge the gap
between supervised and unsupervised I2I translation, with application to
semantic image segmentation. We build upon pix2pix and CycleGAN,
state-of-the-art seminal I2I translation techniques. We propose a method to
select (very few) paired training samples and achieve significant improvements
in both supervised and unsupervised I2I translation settings over random
selection. Further, we boost the performance by incorporating both (selected)
paired and unpaired samples in the training process. Our experiments show that
an extremely weak supervised I2I translation solution using only one paired
training sample can achieve a quantitative performance much better than the
unsupervised CycleGAN model, and comparable to that of the supervised pix2pix
model trained on thousands of pairs.",['cs.CV']
Risk-Aware Planning by Confidence Estimation using Deep Learning-Based Perception,"This work proposes the use of Bayesian approximations of uncertainty from
deep learning in a robot planner, showing that this produces more cautious
actions in safety-critical scenarios. The case study investigated is motivated
by a setup where an aerial robot acts as a ""scout"" for a ground robot. This is
useful when the below area is unknown or dangerous, with applications in space
exploration, military, or search-and-rescue. Images taken from the aerial view
are used to provide a less obstructed map to guide the navigation of the robot
on the ground. Experiments are conducted using a deep learning semantic image
segmentation, followed by a path planner based on the resulting cost map, to
provide an empirical analysis of the proposed method. A comparison with similar
approaches is presented to portray the usefulness of certain techniques, or
variations within a technique, in similar experimental settings. The method is
analyzed to assess the impact of variations in the uncertainty extraction, as
well as the absence of an uncertainty metric, on the overall system with the
use of a defined metric which measures surprise to the planner. The analysis is
performed on multiple datasets, showing a similar trend of lower surprise when
uncertainty information is incorporated in the planning, given threshold values
of the hyperparameters in the uncertainty extraction have been met. We find
that taking uncertainty into account leads to paths that could be 18% less
risky on an average.","['cs.LG', 'cs.AI', 'cs.CV', 'cs.SY', 'eess.SY', 'stat.ML']"
Phase Collaborative Network for Two-Phase Medical Image Segmentation,"In real-world practice, medical images acquired in different phases possess
complementary information, {\em e.g.}, radiologists often refer to both
arterial and venous scans in order to make the diagnosis. However, in medical
image analysis, fusing prediction from two phases is often difficult, because
(i) there is a domain gap between two phases, and (ii) the semantic labels are
not pixel-wise corresponded even for images scanned from the same patient. This
paper studies organ segmentation in two-phase CT scans. We propose Phase
Collaborative Network (PCN), an end-to-end framework that contains both
generative and discriminative modules. PCN can be mathematically explained to
formulate phase-to-phase and data-to-label relations jointly. Experiments are
performed on a two-phase CT dataset, on which PCN outperforms the baselines
working with one-phase data by a large margin, and we empirically verify that
the gain comes from inter-phase collaboration. Besides, PCN transfers well to
two public single-phase datasets, demonstrating its potential applications.",['cs.CV']
Image Segmentation using Multi-Threshold technique by Histogram Sampling,"The segmentation of digital images is one of the essential steps in image
processing or a computer vision system. It helps in separating the pixels into
different regions according to their intensity level. A large number of
segmentation techniques have been proposed, and a few of them use complex
computational operations. Among all, the most straightforward procedure that
can be easily implemented is thresholding. In this paper, we present a unique
heuristic approach for image segmentation that automatically determines
multilevel thresholds by sampling the histogram of a digital image. Our
approach emphasis on selecting a valley as optimal threshold values. We
demonstrated that our approach outperforms the popular Otsu's method in terms
of CPU computational time. We demonstrated that our approach outperforms the
popular Otsu's method in terms of CPU computational time. We observed a maximum
speed-up of 35.58x and a minimum speed-up of 10.21x on popular image processing
benchmarks. To demonstrate the correctness of our approach in determining
threshold values, we compute PSNR, SSIM, and FSIM values to compare with the
values obtained by Otsu's method. This evaluation shows that our approach is
comparable and better in many cases as compared to well known Otsu's method.",['cs.CV']
End-to-End Boundary Aware Networks for Medical Image Segmentation,"Fully convolutional neural networks (CNNs) have proven to be effective at
representing and classifying textural information, thus transforming image
intensity into output class masks that achieve semantic image segmentation. In
medical image analysis, however, expert manual segmentation often relies on the
boundaries of anatomical structures of interest. We propose boundary aware CNNs
for medical image segmentation. Our networks are designed to account for organ
boundary information, both by providing a special network edge branch and
edge-aware loss terms, and they are trainable end-to-end. We validate their
effectiveness on the task of brain tumor segmentation using the BraTS 2018
dataset. Our experiments reveal that our approach yields more accurate
segmentation results, which makes it promising for more extensive application
to medical image segmentation.","['cs.CV', 'cs.LG', 'eess.IV']"
Mumford-Shah Loss Functional for Image Segmentation with Deep Learning,"Recent state-of-the-art image segmentation algorithms are mostly based on
deep neural networks, thanks to their high performance and fast computation
time. However, these methods are usually trained in a supervised manner, which
requires large number of high quality ground-truth segmentation masks. On the
other hand, classical image segmentation approaches such as level-set methods
are formulated in a self-supervised manner by minimizing energy functions such
as Mumford-Shah functional, so they are still useful to help generation of
segmentation masks without labels. Unfortunately, these algorithms are usually
computationally expensive and often have limitation in semantic segmentation.
In this paper, we propose a novel loss function based on Mumford-Shah
functional that can be used in deep-learning based image segmentation without
or with small labeled data. This loss function is based on the observation that
the softmax layer of deep neural networks has striking similarity to the
characteristic function in the Mumford-Shah functional. We show that the new
loss function enables semi-supervised and unsupervised segmentation. In
addition, our loss function can be also used as a regularized function to
enhance supervised semantic segmentation algorithms. Experimental results on
multiple datasets demonstrate the effectiveness of the proposed method.","['cs.CV', 'cs.LG', 'stat.ML']"
CC-Net: Image Complexity Guided Network Compression for Biomedical Image Segmentation,"Convolutional neural networks (CNNs) for biomedical image analysis are often
of very large size, resulting in high memory requirement and high latency of
operations. Searching for an acceptable compressed representation of the base
CNN for a specific imaging application typically involves a series of
time-consuming training/validation experiments to achieve a good compromise
between network size and accuracy. To address this challenge, we propose
CC-Net, a new image complexity-guided CNN compression scheme for biomedical
image segmentation. Given a CNN model, CC-Net predicts the final accuracy of
networks of different sizes based on the average image complexity computed from
the training data. It then selects a multiplicative factor for producing a
desired network with acceptable network accuracy and size. Experiments show
that CC-Net is effective for generating compressed segmentation networks,
retaining up to 95% of the base network segmentation accuracy and utilizing
only 0.1% of trainable parameters of the full-sized networks in the best case.",['cs.CV']
Automatic Image Pixel Clustering based on Mussels Wandering Optimiz,"Image segmentation as a clustering problem is to identify pixel groups on an
image without any preliminary labels available. It remains a challenge in
machine vision because of the variations in size and shape of image segments.
Furthermore, determining the segment number in an image is NP-hard without
prior knowledge of the image content. This paper presents an automatic color
image pixel clustering scheme based on mussels wandering optimization. By
applying an activation variable to determine the number of clusters along with
the cluster centers optimization, an image is segmented with minimal prior
knowledge and human intervention. By revising the within- and between-class sum
of squares ratio for random natural image contents, we provide a novel fitness
function for image pixel clustering tasks. Comprehensive empirical studies of
the proposed scheme against other state-of-the-art competitors on synthetic
data and the ASD dataset have demonstrated the promising performance of the
proposed scheme.",['cs.CV']
Extreme Augmentation : Can deep learning based medical image segmentation be trained using a single manually delineated scan?,"Yes, it can. Data augmentation is perhaps the oldest preprocessing step in
computer vision literature. Almost every computer vision model trained on
imaging data uses some form of augmentation. In this paper, we use the
inter-vertebral disk segmentation task alongside a deep residual U-Net as the
learning model, to explore the effectiveness of augmentation. In the extreme,
we observed that a model trained on patches extracted from just one scan, with
each patch augmented 50 times; achieved a Dice score of 0.73 in a validation
set of 40 cases. Qualitative evaluation indicated a clinically usable
segmentation algorithm, which appropriately segments regions of interest,
alongside limited false positive specks. When the initial patches are extracted
from nine scans the average Dice coefficient jumps to 0.86 and most of the
false positives disappear. While this still falls short of state-of-the-art
deep learning based segmentation of discs reported in literature, qualitative
examination reveals that it does yield segmentation, which can be amended by
expert clinicians with minimal effort to generate additional data for training
improved deep models. Extreme augmentation of training data, should thus be
construed as a strategy for training deep learning based algorithms, when very
little manually annotated data is available to work with. Models trained with
extreme augmentation can then be used to accelerate the generation of manually
labelled data. Hence, we show that extreme augmentation can be a valuable tool
in addressing scaling up small imaging data sets to address medical image
segmentation tasks.",['cs.CV']
Revisiting CycleGAN for semi-supervised segmentation,"In this work, we study the problem of training deep networks for semantic
image segmentation using only a fraction of annotated images, which may
significantly reduce human annotation efforts. Particularly, we propose a
strategy that exploits the unpaired image style transfer capabilities of
CycleGAN in semi-supervised segmentation. Unlike recent works using adversarial
learning for semi-supervised segmentation, we enforce cycle consistency to
learn a bidirectional mapping between unpaired images and segmentation masks.
This adds an unsupervised regularization effect that boosts the segmentation
performance when annotated data is limited. Experiments on three different
public segmentation benchmarks (PASCAL VOC 2012, Cityscapes and ACDC)
demonstrate the effectiveness of the proposed method. The proposed model
achieves 2-4% of improvement with respect to the baseline and outperforms
recent approaches for this task, particularly in low labeled data regime.",['cs.CV']
Exploiting Temporality for Semi-Supervised Video Segmentation,"In recent years, there has been remarkable progress in supervised image
segmentation. Video segmentation is less explored, despite the temporal
dimension being highly informative. Semantic labels, e.g. that cannot be
accurately detected in the current frame, may be inferred by incorporating
information from previous frames. However, video segmentation is challenging
due to the amount of data that needs to be processed and, more importantly, the
cost involved in obtaining ground truth annotations for each frame. In this
paper, we tackle the issue of label scarcity by using consecutive frames of a
video, where only one frame is annotated. We propose a deep, end-to-end
trainable model which leverages temporal information in order to make use of
easy to acquire unlabeled data. Our network architecture relies on a novel
interconnection of two components: a fully convolutional network to model
spatial information and temporal units that are employed at intermediate levels
of the convolutional network in order to propagate information through time.
The main contribution of this work is the guidance of the temporal signal
through the network. We show that only placing a temporal module between the
encoder and decoder is suboptimal (baseline). Our extensive experiments on the
CityScapes dataset indicate that the resulting model can leverage unlabeled
temporal frames and significantly outperform both the frame-by-frame image
segmentation and the baseline approach.","['cs.CV', 'cs.LG', 'eess.IV']"
LadderNet: Multi-path networks based on U-Net for medical image segmentation,"U-Net has been providing state-of-the-art performance in many medical image
segmentation problems. Many modifications have been proposed for U-Net, such as
attention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Net
with residual blocks or blocks with dense connections. However, all these
modifications have an encoder-decoder structure with skip connections, and the
number of paths for information flow is limited. We propose LadderNet in this
paper, which can be viewed as a chain of multiple U-Nets. Instead of only one
pair of encoder branch and decoder branch in U-Net, a LadderNet has multiple
pairs of encoder-decoder branches, and has skip connections between every pair
of adjacent decoder and decoder branches in each level. Inspired by the success
of ResNet and R2-UNet, we use modified residual blocks where two convolutional
layers in one block share the same weights. A LadderNet has more paths for
information flow because of skip connections and residual blocks, and can be
viewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence to
an ensemble of FCNs improves segmentation accuracy, while the shared weights
within each residual block reduce parameter number. Semantic segmentation is
essential for retinal disease detection. We tested LadderNet on two benchmark
datasets for blood vessel segmentation in retinal images, and achieved superior
performance over methods in the literature. The implementation is provided
\url{https://github.com/juntang-zhuang/LadderNet}","['cs.CV', 'eess.IV']"
Customizable Architecture Search for Semantic Segmentation,"In this paper, we propose a Customizable Architecture Search (CAS) approach
to automatically generate a network architecture for semantic image
segmentation. The generated network consists of a sequence of stacked
computation cells. A computation cell is represented as a directed acyclic
graph, in which each node is a hidden representation (i.e., feature map) and
each edge is associated with an operation (e.g., convolution and pooling),
which transforms data to a new layer. During the training, the CAS algorithm
explores the search space for an optimized computation cell to build a network.
The cells of the same type share one architecture but with different weights.
In real applications, however, an optimization may need to be conducted under
some constraints such as GPU time and model size. To this end, a cost
corresponding to the constraint will be assigned to each operation. When an
operation is selected during the search, its associated cost will be added to
the objective. As a result, our CAS is able to search an optimized architecture
with customized constraints. The approach has been thoroughly evaluated on
Cityscapes and CamVid datasets, and demonstrates superior performance over
several state-of-the-art techniques. More remarkably, our CAS achieves 72.3%
mIoU on the Cityscapes dataset with speed of 108 FPS on an Nvidia TitanXp GPU.",['cs.CV']
Mixed-Supervised Dual-Network for Medical Image Segmentation,"Deep learning based medical image segmentation models usually require large
datasets with high-quality dense segmentations to train, which are very
time-consuming and expensive to prepare. One way to tackle this challenge is by
using the mixed-supervised learning framework, in which only a part of data is
densely annotated with segmentation label and the rest is weakly labeled with
bounding boxes. The model is trained jointly in a multi-task learning setting.
In this paper, we propose Mixed-Supervised Dual-Network (MSDN), a novel
architecture which consists of two separate networks for the detection and
segmentation tasks respectively, and a series of connection modules between the
layers of the two networks. These connection modules are used to transfer
useful information from the auxiliary detection task to help the segmentation
task. We propose to use a recent technique called ""Squeeze and Excitation"" in
the connection module to boost the transfer. We conduct experiments on two
medical image segmentation datasets. The proposed MSDN model outperforms
multiple baselines.",['cs.CV']
Don't ignore Dropout in Fully Convolutional Networks,"Data for Image segmentation models can be costly to obtain due to the
precision required by human annotators. We run a series of experiments showing
the effect of different kinds of Dropout training on the DeepLabv3+ Image
segmentation model when trained using a small dataset. We find that when
appropriate forms of Dropout are applied in the right place in the model
architecture that non-insignificant improvement in Mean Intersection over Union
(mIoU) score can be observed. In our best case, we find that applying Dropout
scheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49
to 0.59. This result shows that even where a model architecture makes extensive
use of Batch Normalization, Dropout can still be an effective way of improving
performance in low data situations.",['cs.CV']
ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths,"Nowadays U-net-like FCNs predominate various biomedical image segmentation
applications and attain promising performance, largely due to their elegant
architectures, e.g., symmetric contracting and expansive paths as well as
lateral skip-connections. It remains a research direction to devise novel
architectures to further benefit the segmentation. In this paper, we develop an
ACE-net that aims to enhance the feature representation and utilization by
augmenting the contracting and expansive paths. In particular, we augment the
paths by the recently proposed advanced techniques including ASPP, dense
connection and deep supervision mechanisms, and novel connections such as
directly connecting the raw image to the expansive side. With these
augmentations, ACE-net can utilize features from multiple sources, scales and
reception fields to segment while still maintains a relative simple
architecture. Experiments on two typical biomedical segmentation tasks validate
its effectiveness, where highly competitive results are obtained in both tasks
while ACE-net still runs fast at inference.",['cs.CV']
Learning to Segment Skin Lesions from Noisy Annotations,"Deep convolutional neural networks have driven substantial advancements in
the automatic understanding of images. Requiring a large collection of images
and their associated annotations is one of the main bottlenecks limiting the
adoption of deep networks. In the task of medical image segmentation, requiring
pixel-level semantic annotations performed by human experts exacerbate this
difficulty. This paper proposes a new framework to train a fully convolutional
segmentation network from a large set of cheap unreliable annotations and a
small set of expert-level clean annotations. We propose a spatially adaptive
reweighting approach to treat clean and noisy pixel-level annotations
commensurately in the loss function. We deploy a meta-learning approach to
assign higher importance to pixels whose loss gradient direction is closer to
those of clean data. Our experiments on training the network using segmentation
ground truth corrupted with different levels of annotation noise show how
spatial reweighting improves the robustness of deep networks to noisy
annotations.",['cs.CV']
Weakly Supervised Segmentation by A Deep Geodesic Prior,"The performance of the state-of-the-art image segmentation methods heavily
relies on the high-quality annotations, which are not easily affordable,
particularly for medical data. To alleviate this limitation, in this study, we
propose a weakly supervised image segmentation method based on a deep geodesic
prior. We hypothesize that integration of this prior information can reduce the
adverse effects of weak labels in segmentation accuracy. Our proposed algorithm
is based on a prior information, extracted from an auto-encoder, trained to map
objects geodesic maps to their corresponding binary maps. The obtained
information is then used as an extra term in the loss function of the
segmentor. In order to show efficacy of the proposed strategy, we have
experimented segmentation of cardiac substructures with clean and two levels of
noisy labels (L1, L2). Our experiments showed that the proposed algorithm
boosted the performance of baseline deep learning-based segmentation for both
clean and noisy labels by 4.4%, 4.6%(L1), and 6.3%(L2) in dice score,
respectively. We also showed that the proposed method was more robust in the
presence of high-level noise due to the existence of shape priors.","['stat.ML', 'cs.CV', 'cs.LG']"
Computing the Spatial Probability of Inclusion inside Partial Contours for Computer Vision Applications,"In Computer Vision, edge detection is one of the favored approaches for
feature and object detection in images since it provides information about
their objects boundaries. Other region-based approaches use probabilistic
analysis such as clustering and Markov random fields, but those methods cannot
be used to analyze edges and their interaction. In fact, only image
segmentation can produce regions based on edges, but it requires thresholding
by simply separating the regions into binary in-out information. Hence, there
is currently a gap between edge-based and region-based algorithms, since edges
cannot be used to study the properties of a region and vice versa. The
objective of this paper is to present a novel spatial probability analysis that
allows determining the probability of inclusion inside a set of partial
contours (strokes). To answer this objective, we developed a new approach that
uses electromagnetic convolutions and repulsion optimization to compute the
required probabilities. Hence, it becomes possible to generate a continuous
space of probability based only on the edge information, thus bridging the gap
between the edge-based methods and the region-based methods. The developed
method is consistent with the fundamental properties of inclusion probabilities
and its results are validated by comparing an image with the probability-based
estimation given by our algorithm. The method can also be generalized to take
into consideration the intensity of the edges or to be used for 3D shapes. This
is the first documented method that allows computing a space of probability
based on interacting edges, which opens the path to broader applications such
as image segmentation and contour completion.","['cs.CV', 'cs.NA', 'math.NA']"
Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization,"Long-term visual localization is the problem of estimating the camera pose of
a given query image in a scene whose appearance changes over time. It is an
important problem in practice, for example, encountered in autonomous driving.
In order to gain robustness to such changes, long-term localization approaches
often use segmantic segmentations as an invariant scene representation, as the
semantic meaning of each scene part should not be affected by seasonal and
other changes. However, these representations are typically not very
discriminative due to the limited number of available classes. In this paper,
we propose a new neural network, the Fine-Grained Segmentation Network (FGSN),
that can be used to provide image segmentations with a larger number of labels
and can be trained in a self-supervised fashion. In addition, we show how FGSNs
can be trained to output consistent labels across seasonal changes. We
demonstrate through extensive experiments that integrating the fine-grained
segmentations produced by our FGSNs into existing localization algorithms leads
to substantial improvements in localization performance.","['cs.CV', '68T45']"
Boundary-weighted Domain Adaptive Neural Network for Prostate MR Image Segmentation,"Accurate segmentation of the prostate from magnetic resonance (MR) images
provides useful information for prostate cancer diagnosis and treatment.
However, automated prostate segmentation from 3D MR images still faces several
challenges. For instance, a lack of clear edge between the prostate and other
anatomical structures makes it challenging to accurately extract the
boundaries. The complex background texture and large variation in size, shape
and intensity distribution of the prostate itself make segmentation even
further complicated. With deep learning, especially convolutional neural
networks (CNNs), emerging as commonly used methods for medical image
segmentation, the difficulty in obtaining large number of annotated medical
images for training CNNs has become much more pronounced that ever before.
Since large-scale dataset is one of the critical components for the success of
deep learning, lack of sufficient training data makes it difficult to fully
train complex CNNs. To tackle the above challenges, in this paper, we propose a
boundary-weighted domain adaptive neural network (BOWDA-Net). To make the
network more sensitive to the boundaries during segmentation, a
boundary-weighted segmentation loss (BWL) is proposed. Furthermore, an advanced
boundary-weighted transfer leaning approach is introduced to address the
problem of small medical imaging datasets. We evaluate our proposed model on
the publicly available MICCAI 2012 Prostate MR Image Segmentation (PROMISE12)
challenge dataset. Our experimental results demonstrate that the proposed model
is more sensitive to boundary information and outperformed other
state-of-the-art methods.",['cs.CV']
SFSegNet: Parse Freehand Sketches using Deep Fully Convolutional Networks,"Parsing sketches via semantic segmentation is attractive but challenging,
because (i) free-hand drawings are abstract with large variances in depicting
objects due to different drawing styles and skills; (ii) distorting lines drawn
on the touchpad make sketches more difficult to be recognized; (iii) the
high-performance image segmentation via deep learning technologies needs
enormous annotated sketch datasets during the training stage. In this paper, we
propose a Sketch-target deep FCN Segmentation Network(SFSegNet) for automatic
free-hand sketch segmentation, labeling each sketch in a single object with
multiple parts. SFSegNet has an end-to-end network process between the input
sketches and the segmentation results, composed of 2 parts: (i) a modified deep
Fully Convolutional Network(FCN) using a reweighting strategy to ignore
background pixels and classify which part each pixel belongs to; (ii) affine
transform encoders that attempt to canonicalize the shaking strokes. We train
our network with the dataset that consists of 10,000 annotated sketches, to
find an extensively applicable model to segment stokes semantically in one
ground truth. Extensive experiments are carried out and segmentation results
show that our method outperforms other state-of-the-art networks.","['cs.CV', 'cs.LG']"
DDSL: Deep Differentiable Simplex Layer for Learning Geometric Signals,"We present a Deep Differentiable Simplex Layer (DDSL) for neural networks for
geometric deep learning. The DDSL is a differentiable layer compatible with
deep neural networks for bridging simplex mesh-based geometry representations
(point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images
(e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) to
perform differentiable, efficient, anti-aliased rasterization of simplex-based
signals. We present a complete theoretical framework for the process as well as
an efficient backpropagation algorithm. Compared to previous differentiable
renderers and rasterizers, the DDSL generalizes to arbitrary simplex degrees
and dimensions. In particular, we explore its applications to 2D shapes and
illustrate two applications of this method: (1) mesh editing and optimization
guided by neural network outputs, and (2) using DDSL for a differentiable
rasterization loss to facilitate end-to-end training of polygon generators. We
are able to validate the effectiveness of gradient-based shape optimization
with the example of airfoil optimization, and using the differentiable
rasterization loss to facilitate end-to-end training, we surpass state of the
art for polygonal image segmentation given ground-truth bounding boxes.","['cs.CV', 'cs.CG']"
Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation,"For the task of medical image segmentation, fully convolutional network (FCN)
based architectures have been extensively used with various modifications. A
rising trend in these architectures is to employ joint-learning of the target
region with an auxiliary task, a method commonly known as multi-task learning.
These approaches help impose smoothness and shape priors, which vanilla FCN
approaches do not necessarily incorporate. In this paper, we propose a novel
plug-and-play module, which we term as Conv-MCD, which exploits structural
information in two ways - i) using the contour map and ii) using the distance
map, both of which can be obtained from ground truth segmentation maps with no
additional annotation costs. The key benefit of our module is the ease of its
addition to any state-of-the-art architecture, resulting in a significant
improvement in performance with a minimal increase in parameters. To
substantiate the above claim, we conduct extensive experiments using 4
state-of-the-art architectures across various evaluation metrics, and report a
significant increase in performance in relation to the base networks. In
addition to the aforementioned experiments, we also perform ablative studies
and visualization of feature maps to further elucidate our approach.",['cs.CV']
Psi-Net: Shape and boundary aware joint multi-task deep network for medical image segmentation,"Image segmentation is a primary task in many medical applications. Recently,
many deep networks derived from U-Net have been extensively used in various
medical image segmentation tasks. However, in most of the cases, networks
similar to U-net produce coarse and non-smooth segmentations with lots of
discontinuities. To improve and refine the performance of U-Net like networks,
we propose the use of parallel decoders which along with performing the mask
predictions also perform contour prediction and distance map estimation. The
contour and distance map aid in ensuring smoothness in the segmentation
predictions. To facilitate joint training of three tasks, we propose a novel
architecture called Psi-Net with a single encoder and three parallel decoders
(thus having a shape of $\Psi$), one decoder to learns the segmentation mask
prediction and other two decoders to learn the auxiliary tasks of contour
detection and distance map estimation. The learning of these auxiliary tasks
helps in capturing the shape and the boundary information. We also propose a
new joint loss function for the proposed architecture. The loss function
consists of a weighted combination of Negative Log likelihood and Mean Square
Error loss. We have used two publicly available datasets: 1) Origa dataset for
the task of optic cup and disc segmentation and 2) Endovis segment dataset for
the task of polyp segmentation to evaluate our model. We have conducted
extensive experiments using our network to show our model gives better results
in terms of segmentation, boundary and shape metrics.",['cs.CV']
High Accurate Unhealthy Leaf Detection,"India is an agriculture-dependent country. As we all know that farming is the
backbone of our country it is our responsibility to preserve the crops.
However, we cannot stop the destruction of crops by natural calamities at least
we have to try to protect our crops from diseases. To, detect a plant disease
we need a fast automatic way. So, this paper presents a model to identify the
particular disease of plant leaves at early stages so that we can prevent or
take a remedy to stop spreading of the disease. This proposed model is made
into five sessions. Image preprocessing includes the enhancement of the low
light image done using inception modules in CNN. Low-resolution image
enhancement is done using an Adversarial Neural Network. This also includes
Conversion of RGB Image to YCrCb color space. Next, this paper presents a
methodology for image segmentation which is an important aspect for identifying
the disease symptoms. This segmentation is done using the genetic algorithm.
Due to this process the segmentation of the leaf Image this helps in detection
of the leaf mage automatically and classifying. Texture extraction is done
using the statistical model called GLCM and finally, the classification of the
diseases is done using the SVM using Different Kernels with the high accuracy.","['cs.CV', 'eess.IV']"
The Chan-Vese Model with Elastica and Landmark Constraints for Image Segmentation,"In order to completely separate objects with large sections of occluded
boundaries in an image, we devise a new variational level set model for image
segmentation combining the Chan-Vese model with elastica and landmark
constraints. For computational efficiency, we design its Augmented Lagrangian
Method (ALM) or Alternating Direction Method of Multiplier (ADMM) method by
introducing some auxiliary variables, Lagrange multipliers, and penalty
parameters. In each loop of alternating iterative optimization, the
sub-problems of minimization can be easily solved via the Gauss-Seidel
iterative method and generalized soft thresholding formulas with projection,
respectively. Numerical experiments show that the proposed model can not only
recover larger broken boundaries but can also improve segmentation efficiency,
as well as decrease the dependence of segmentation on parameter tuning and
initialization.",['cs.CV']
SkrGAN: Sketching-rendering Unconditional Generative Adversarial Networks for Medical Image Synthesis,"Generative Adversarial Networks (GANs) have the capability of synthesizing
images, which have been successfully applied to medical image synthesis tasks.
However, most of existing methods merely consider the global contextual
information and ignore the fine foreground structures, e.g., vessel, skeleton,
which may contain diagnostic indicators for medical image analysis. Inspired by
human painting procedure, which is composed of stroking and color rendering
steps, we propose a Sketching-rendering Unconditional Generative Adversarial
Network (SkrGAN) to introduce a sketch prior constraint to guide the medical
image generation. In our SkrGAN, a sketch guidance module is utilized to
generate a high quality structural sketch from random noise, then a color
render mapping is used to embed the sketch-based representations and resemble
the background appearances. Experimental results show that the proposed SkrGAN
achieves the state-of-the-art results in synthesizing images for various image
modalities, including retinal color fundus, X-Ray, Computed Tomography (CT) and
Magnetic Resonance Imaging (MRI). In addition, we also show that the
performances of medical image segmentation method have been improved by using
our synthesized images as data augmentation.","['cs.CV', 'eess.IV']"
AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks,"Dense prediction models are widely used for image segmentation. One important
challenge is to sufficiently train these models to yield good generalizations
for hard-to-learn pixels. A typical group of such hard-to-learn pixels are
boundaries between instances. Many studies have proposed to give specific
attention to learning the boundary pixels. They include designing multi-task
networks with an additional task of boundary prediction and increasing the
weights of boundary pixels' predictions in the loss function. Such strategies
require defining what to attend beforehand and incorporating this defined
attention to the learning model. However, there may exist other groups of
hard-to-learn pixels and manually defining and incorporating the appropriate
attention for each group may not be feasible. In order to provide a more
attainable and scalable solution, this paper proposes AttentionBoost, which is
a new multi-attention learning model based on adaptive boosting. AttentionBoost
designs a multi-stage network and introduces a new loss adjustment mechanism
for a dense prediction model to adaptively learn what to attend at each stage
directly on image data without necessitating any prior definition about what to
attend. This mechanism modulates the attention of each stage to correct the
mistakes of previous stages, by adjusting the loss weight of each pixel
prediction separately with respect to how accurate the previous stages are on
this pixel. This mechanism enables AttentionBoost to learn different attentions
for different pixels at the same stage, according to difficulty of learning
these pixels, as well as multiple attentions for the same pixel at different
stages, according to confidence of these stages on their predictions for this
pixel. Using gland segmentation as a showcase application, our experiments
demonstrate that AttentionBoost improves the results of its counterparts.","['cs.CV', 'cs.LG']"
Leveraging Domain Knowledge to Improve Microscopy Image Segmentation with Lifted Multicuts,"The throughput of electron microscopes has increased significantly in recent
years, enabling detailed analysis of cell morphology and ultrastructure.
Analysis of neural circuits at single-synapse resolution remains the flagship
target of this technique, but applications to cell and developmental biology
are also starting to emerge at scale. The amount of data acquired in such
studies makes manual instance segmentation, a fundamental step in many analysis
pipelines, impossible. While automatic segmentation approaches have improved
significantly thanks to the adoption of convolutional neural networks, their
accuracy still lags behind human annotations and requires additional manual
proof-reading. A major hindrance to further improvements is the limited field
of view of the segmentation networks preventing them from exploiting the
expected cell morphology or other prior biological knowledge which humans use
to inform their segmentation decisions. In this contribution, we show how such
domain-specific information can be leveraged by expressing it as long-range
interactions in a graph partitioning problem known as the lifted multicut
problem. Using this formulation, we demonstrate significant improvement in
segmentation accuracy for three challenging EM segmentation problems from
neuroscience and cell biology.",['cs.CV']
Massively Parallel Benders Decomposition for Correlation Clustering,"We tackle the problem of graph partitioning for image segmentation using
correlation clustering (CC), which we treat as an integer linear program (ILP).
We reformulate optimization in the ILP so as to admit efficient optimization
via Benders decomposition, a classic technique from operations research. Our
Benders decomposition formulation has many subproblems, each associated with a
node in the CC instance's graph, which are solved in parallel. Each Benders
subproblem enforces the cycle inequalities corresponding to the negative weight
edges attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows, to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and allows for massive
parallelization.","['cs.CV', 'cs.DS']"
A novel framework of the fuzzy c-means distances problem based weighted distance,"Clustering is one of the major roles in data mining that is widely
application in pattern recognition and image segmentation. Fuzzy C-means (FCM)
is the most used clustering algorithm that proven efficient, fast and easy to
implement, however, FCM uses the Euclidean distance that often leads to
clustering errors, especially when handling multidimensional and noisy data. In
the last few years, many distances metric have been proposed by researchers to
improve the performance of the FCM algorithms, and the majority of researchers
propose weighted distance. In this paper, we proposed Canberra Weighted
Distance to improved performance of the FCM algorithm. The experimental result
using the UCI data set show the proposed method is superior to the original
method and other clustering methods.","['cs.LG', 'stat.ML']"
Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation,"Separating and labeling each instance of a nucleus (instance-aware
segmentation) is the key challenge in segmenting single cell nuclei on
fluorescence microscopy images. Deep Neural Networks can learn the implicit
transformation of a nuclear image into a probability map indicating the class
membership of each pixel (nucleus or background), but the use of
post-processing steps to turn the probability map into a labeled object mask is
error-prone. This especially accounts for nuclear images of tissue sections and
nuclear images across varying tissue preparations. In this work, we aim to
evaluate the performance of state-of-the-art deep learning architectures to
segment nuclei in fluorescence images of various tissue origins and sample
preparation types without post-processing. We compare architectures that
operate on pixel to pixel translation and an architecture that operates on
object detection and subsequent locally applied segmentation. In addition, we
propose a novel strategy to create artificial images to extend the training
set. We evaluate the influence of ground truth annotation quality, image scale
and segmentation complexity on segmentation performance. Results show that
three out of four deep learning architectures (U-Net, U-Net with ResNet34
backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the
sample preparation types and tissue origins with satisfactory segmentation
performance. Mask R-CNN, an architecture designed to address instance aware
segmentation tasks, outperforms other architectures. Equal nuclear mean size,
consistent nuclear annotations and the use of artificially generated images
result in overall acceptable precision and recall across different tissues and
sample preparation types.","['cs.CV', 'cs.LG', 'q-bio.TO']"
Multi-Task Attention-Based Semi-Supervised Learning for Medical Image Segmentation,"We propose a novel semi-supervised image segmentation method that
simultaneously optimizes a supervised segmentation and an unsupervised
reconstruction objectives. The reconstruction objective uses an attention
mechanism that separates the reconstruction of image areas corresponding to
different classes. The proposed approach was evaluated on two applications:
brain tumor and white matter hyperintensities segmentation. Our method, trained
on unlabeled and a small number of labeled images, outperformed supervised CNNs
trained with the same number of images and CNNs pre-trained on unlabeled data.
In ablation experiments, we observed that the proposed attention mechanism
substantially improves segmentation performance. We explore two multi-task
training strategies: joint training and alternating training. Alternating
training requires fewer hyperparameters and achieves a better, more stable
performance than joint training. Finally, we analyze the features learned by
different methods and find that the attention mechanism helps to learn more
discriminative features in the deeper layers of encoders.",['cs.CV']
Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation,"Deep learning methods have achieved promising performance in many areas, but
they are still struggling with noisy-labeled images during the training
process. Considering that the annotation quality indispensably relies on great
expertise, the problem is even more crucial in the medical image domain. How to
eliminate the disturbance from noisy labels for segmentation tasks without
further annotations is still a significant challenge. In this paper, we
introduce our label quality evaluation strategy for deep neural networks
automatically assessing the quality of each label, which is not explicitly
provided, and training on clean-annotated ones. We propose a solution for
network automatically evaluating the relative quality of the labels in the
training set and using good ones to tune the network parameters. We also design
an overfitting control module to let the network maximally learn from the
precise annotations during the training process. Experiments on the public
biomedical image segmentation dataset have proved the method outperforms
baseline methods and retains both high accuracy and good generalization at
different noise levels.","['cs.CV', 'cs.LG', 'eess.IV']"
Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation,"Most progress in semantic segmentation reports on daytime images taken under
favorable illumination conditions. We instead address the problem of semantic
segmentation of nighttime images and improve the state-of-the-art, by adapting
daytime models to nighttime without using nighttime annotations. Moreover, we
design a new evaluation framework to address the substantial uncertainty of
semantics in nighttime images. Our central contributions are: 1) a curriculum
framework to gradually adapt semantic segmentation models from day to night via
labeled synthetic images and unlabeled real images, both for progressively
darker times of day, which exploits cross-time-of-day correspondences for the
real images to guide the inference of their labels; 2) a novel
uncertainty-aware annotation and evaluation framework and metric for semantic
segmentation, designed for adverse conditions and including image regions
beyond human recognition capability in the evaluation in a principled fashion;
3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920
unlabeled twilight images with correspondences to their daytime counterparts
plus a set of 151 nighttime images with fine pixel-level annotations created
with our protocol, which serves as a first benchmark to perform our novel
evaluation. Experiments show that our guided curriculum adaptation
significantly outperforms state-of-the-art methods on real nighttime sets both
for standard metrics and our uncertainty-aware metric. Furthermore, our
uncertainty-aware evaluation reveals that selective invalidation of predictions
can lead to better results on data with ambiguous content such as our nighttime
benchmark and profit safety-oriented applications which involve invalid inputs.",['cs.CV']
Boundary and Entropy-driven Adversarial Learning for Fundus Image Segmentation,"Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images
from different datasets is critical for glaucoma disease screening. The
cross-domain discrepancy (domain shift) hinders the generalization of deep
neural networks to work on different domain datasets.In this work, we present
an unsupervised domain adaptation framework,called Boundary and Entropy-driven
Adversarial Learning (BEAL), to improve the OD and OC segmentation performance,
especially on the ambiguous boundary regions. In particular, our proposed BEAL
frame-work utilizes the adversarial learning to encourage the boundary
prediction and mask probability entropy map (uncertainty map) of the target
domain to be similar to the source ones, generating more accurate boundaries
and suppressing the high uncertainty predictions of OD and OC segmentation. We
evaluate the proposed BEAL framework on two public retinal fundus image
datasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate
that our method outperforms the state-of-the-art unsupervised domain adaptation
methods. Codes will be available at https://github.com/EmmaW8/BEAL.",['cs.CV']
DeepAtlas: Joint Semi-Supervised Learning of Image Registration and Segmentation,"Deep convolutional neural networks (CNNs) are state-of-the-art for semantic
image segmentation, but typically require many labeled training samples.
Obtaining 3D segmentations of medical images for supervised training is
difficult and labor intensive. Motivated by classical approaches for joint
segmentation and registration we therefore propose a deep learning framework
that jointly learns networks for image registration and image segmentation. In
contrast to previous work on deep unsupervised image registration, which showed
the benefit of weak supervision via image segmentations, our approach can use
existing segmentations when available and computes them via the segmentation
network otherwise, thereby providing the same registration benefit. Conversely,
segmentation network training benefits from the registration, which essentially
provides a realistic form of data augmentation. Experiments on knee and brain
3D magnetic resonance (MR) images show that our approach achieves large
simultaneous improvements of segmentation and registration accuracy (over
independently trained networks) and allows training high-quality models with
very limited training data. Specifically, in a one-shot-scenario (with only one
manually labeled image) our approach increases Dice scores (%) over an
unsupervised registration network by 2.7 and 1.8 on the knee and brain images
respectively.",['cs.CV']
ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation,"Segmentation is a fundamental task in medical image analysis. However, most
existing methods focus on primary region extraction and ignore edge
information, which is useful for obtaining accurate segmentation. In this
paper, we propose a generic medical segmentation method, called Edge-aTtention
guidance Network (ET-Net), which embeds edge-attention representations to guide
the segmentation network. Specifically, an edge guidance module is utilized to
learn the edge-attention representations in the early encoding layers, which
are then transferred to the multi-scale decoding layers, fused using a weighted
aggregation module. The experimental results on four segmentation tasks (i.e.,
optic disc/cup and vessel segmentation in retinal images, and lung segmentation
in chest X-Ray and CT images) demonstrate that preserving edge-attention
representations contributes to the final segmentation accuracy, and our
proposed method outperforms current state-of-the-art segmentation methods. The
source code of our method is available at https://github.com/ZzzJzzZ/ETNet.",['cs.CV']
One-stage Shape Instantiation from a Single 2D Image to 3D Point Cloud,"Shape instantiation which predicts the 3D shape of a dynamic target from one
or more 2D images is important for real-time intra-operative navigation.
Previously, a general shape instantiation framework was proposed with manual
image segmentation to generate a 2D Statistical Shape Model (SSM) and with
Kernel Partial Least Square Regression (KPLSR) to learn the relationship
between the 2D and 3D SSM for 3D shape prediction. In this paper, the two-stage
shape instantiation is improved to be one-stage. PointOutNet with 19
convolutional layers and three fully-connected layers is used as the network
structure and Chamfer distance is used as the loss function to predict the 3D
target point cloud from a single 2D image. With the proposed one-stage shape
instantiation algorithm, a spontaneous image-to-point cloud training and
inference can be achieved. A dataset from 27 Right Ventricle (RV) subjects,
indicating 609 experiments, were used to validate the proposed one-stage shape
instantiation algorithm. An average point cloud-to-point cloud (PC-to-PC) error
of 1.72mm has been achieved, which is comparable to the PLSR-based (1.42mm) and
KPLSR-based (1.31mm) two-stage shape instantiation algorithm.","['cs.CV', 'cs.LG']"
Incremental Class Discovery for Semantic Segmentation with RGBD Sensing,"This work addresses the task of open world semantic segmentation using RGBD
sensing to discover new semantic classes over time. Although there are many
types of objects in the real-word, current semantic segmentation methods make a
closed world assumption and are trained only to segment a limited number of
object classes. Towards a more open world approach, we propose a novel method
that incrementally learns new classes for image segmentation. The proposed
system first segments each RGBD frame using both color and geometric
information, and then aggregates that information to build a single segmented
dense 3D map of the environment. The segmented 3D map representation is a key
component of our approach as it is used to discover new object classes by
identifying coherent regions in the 3D map that have no semantic label. The use
of coherent region in the 3D map as a primitive element, rather than
traditional elements such as surfels or voxels, also significantly reduces the
computational complexity and memory use of our method. It thus leads to
semi-real-time performance at {10.7}Hz when incrementally updating the dense 3D
map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate
that the proposed method is able to correctly cluster objects of both known and
unseen classes. We also show the quantitative comparison with the
state-of-the-art supervised methods, the processing time of each step, and the
influences of each component.","['cs.CV', 'cs.RO']"
Brain Segmentation from k-space with End-to-end Recurrent Attention Network,"The task of medical image segmentation commonly involves an image
reconstruction step to convert acquired raw data to images before any analysis.
However, noises, artifacts and loss of information due to the reconstruction
process are almost inevitable, which compromises the final performance of
segmentation. We present a novel learning framework that performs magnetic
resonance brain image segmentation directly from k-space data. The end-to-end
framework consists of a unique task-driven attention module that recurrently
utilizes intermediate segmentation estimation to facilitate image-domain
feature extraction from the raw data, thus closely bridging the reconstruction
and the segmentation tasks. In addition, to address the challenge of manual
labeling, we introduce a novel workflow to generate labeled training data for
segmentation by exploiting imaging modality simulators and digital phantoms.
Extensive experimental results show that the proposed method outperforms
several state-of-the-art methods.",['cs.CV']
An Efficient 3D CNN for Action/Object Segmentation in Video,"Convolutional Neural Network (CNN) based image segmentation has made great
progress in recent years. However, video object segmentation remains a
challenging task due to its high computational complexity. Most of the previous
methods employ a two-stream CNN framework to handle spatial and motion features
separately. In this paper, we propose an end-to-end encoder-decoder style 3D
CNN to aggregate spatial and temporal information simultaneously for video
object segmentation. To efficiently process video, we propose 3D separable
convolution for the pyramid pooling module and decoder, which dramatically
reduces the number of operations while maintaining the performance. Moreover,
we also extend our framework to video action segmentation by adding an extra
classifier to predict the action label for actors in videos. Extensive
experiments on several video datasets demonstrate the superior performance of
the proposed approach for action and object segmentation compared to the
state-of-the-art.","['cs.CV', 'eess.IV']"
Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left Atrium Segmentation,"Training deep convolutional neural networks usually requires a large amount
of labeled data. However, it is expensive and time-consuming to annotate data
for medical image segmentation tasks. In this paper, we present a novel
uncertainty-aware semi-supervised framework for left atrium segmentation from
3D MR images. Our framework can effectively leverage the unlabeled data by
encouraging consistent predictions of the same input under different
perturbations. Concretely, the framework consists of a student model and a
teacher model, and the student model learns from the teacher model by
minimizing a segmentation loss and a consistency loss with respect to the
targets of the teacher model. We design a novel uncertainty-aware scheme to
enable the student model to gradually learn from the meaningful and reliable
targets by exploiting the uncertainty information. Experiments show that our
method achieves high performance gains by incorporating the unlabeled data. Our
method outperforms the state-of-the-art semi-supervised methods, demonstrating
the potential of our framework for the challenging semi-supervised problems.",['cs.CV']
Separable Convolutional LSTMs for Faster Video Segmentation,"Semantic Segmentation is an important module for autonomous robots such as
self-driving cars. The advantage of video segmentation approaches compared to
single image segmentation is that temporal image information is considered, and
their performance increases due to this. Hence, single image segmentation
approaches are extended by recurrent units such as convolutional LSTM
(convLSTM) cells, which are placed at suitable positions in the basic network
architecture. However, a major critique of video segmentation approaches based
on recurrent neural networks is their large parameter count and their
computational complexity, and so, their inference time of one video frame takes
up to 66 percent longer than their basic version. Inspired by the success of
the spatial and depthwise separable convolutional neural networks, we
generalize these techniques for convLSTMs in this work, so that the number of
parameters and the required FLOPs are reduced significantly. Experiments on
different datasets show that the segmentation approaches using the proposed,
modified convLSTM cells achieve similar or slightly worse accuracy, but are up
to 15 percent faster on a GPU than the ones using the standard convLSTM cells.
Furthermore, a new evaluation metric is introduced, which measures the amount
of flickering pixels in the segmented video sequence.","['cs.CV', 'eess.IV']"
Introduction to Camera Pose Estimation with Deep Learning,"Over the last two decades, deep learning has transformed the field of
computer vision. Deep convolutional networks were successfully applied to learn
different vision tasks such as image classification, image segmentation, object
detection and many more. By transferring the knowledge learned by deep models
on large generic datasets, researchers were further able to create fine-tuned
models for other more specific tasks. Recently this idea was applied for
regressing the absolute camera pose from an RGB image. Although the resulting
accuracy was sub-optimal, compared to classic feature-based solutions, this
effort led to a surge of learning-based pose estimation methods. Here, we
review deep learning approaches for camera pose estimation. We describe key
methods in the field and identify trends aiming at improving the original deep
pose regression solution. We further provide an extensive cross-comparison of
existing learning-based pose estimators, together with practical notes on their
execution for reproducibility purposes. Finally, we discuss emerging solutions
and potential future research directions.",['cs.CV']
Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis,"In this paper, an stereo-based traversability analysis approach for all
terrains in off-road mobile robotics, e.g. Unmanned Ground Vehicles (UGVs) is
proposed. This approach reformulates the problem of terrain traversability
analysis into two main problems: (1) 3D terrain reconstruction and (2) terrain
all surfaces detection and analysis. The proposed approach is using stereo
camera for perception and 3D reconstruction of the terrain. In order to detect
all the existing surfaces in the 3D reconstructed terrain as superpixel
surfaces (i.e. segments), an image segmentation technique is applied using
geometry-based features (pixel-based surface normals). Having detected all the
surfaces, Superpixel Surface Traversability Analysis approach (SSTA) is applied
on all of the detected surfaces (superpixel segments) in order to classify them
based on their traversability index. The proposed SSTA approach is based on:
(1) Superpixel surface normal and plane estimation, (2) Traversability analysis
using superpixel surface planes. Having analyzed all the superpixel surfaces
based on their traversability, these surfaces are finally classified into five
main categories as following: traversable, semi-traversable, non-traversable,
unknown and undecided.","['cs.CV', 'cs.RO', 'eess.IV']"
Hyperspectral Image Classification with Deep Metric Learning and Conditional Random Field,"To improve the classification performance in the context of hyperspectral
image processing, many works have been developed based on two common
strategies, namely the spatial-spectral information integration and the
utilization of neural networks. However, both strategies typically require more
training data than the classical algorithms, aggregating the shortage of
labeled samples. In this letter, we propose a novel framework that organically
combines the spectrum-based deep metric learning model and the conditional
random field algorithm. The deep metric learning model is supervised by the
center loss to produce spectrum-based features that gather more tightly in
Euclidean space within classes. The conditional random field with Gaussian edge
potentials, which is firstly proposed for image segmentation tasks, is
introduced to give the pixel-wise classification over the hyperspectral image
by utilizing both the geographical distances between pixels and the Euclidean
distances between the features produced by the deep metric learning model. The
proposed framework is trained by spectral pixels at the deep metric learning
stage and utilizes the half handcrafted spatial features at the conditional
random field stage. This settlement alleviates the shortage of training data to
some extent. Experiments on two real hyperspectral images demonstrate the
advantages of the proposed method in terms of both classification accuracy and
computation cost.","['cs.CV', 'cs.LG', 'stat.ML']"
A Divide-and-Conquer Approach towards Understanding Deep Networks,"Deep neural networks have achieved tremendous success in various fields
including medical image segmentation. However, they have long been criticized
for being a black-box, in that interpretation, understanding and correcting
architectures is difficult as there is no general theory for deep neural
network design. Previously, precision learning was proposed to fuse deep
architectures and traditional approaches. Deep networks constructed in this way
benefit from the original known operator, have fewer parameters, and improved
interpretability. However, they do not yield state-of-the-art performance in
all applications. In this paper, we propose to analyze deep networks using
known operators, by adopting a divide-and-conquer strategy to replace network
components, whilst retaining its performance. The task of retinal vessel
segmentation is investigated for this purpose. We start with a high-performance
U-Net and show by step-by-step conversion that we are able to divide the
network into modules of known operators. The results indicate that a
combination of a trainable guided filter and a trainable version of the Frangi
filter yields a performance at the level of U-Net (AUC 0.974 vs. 0.972) with a
tremendous reduction in parameters (111,536 vs. 9,575). In addition, the
trained layers can be mapped back into their original algorithmic
interpretation and analyzed using standard tools of signal processing.","['cs.LG', 'cs.CV', 'eess.IV']"
Automatic 3D bi-ventricular segmentation of cardiac images by a shape-refined multi-task deep learning approach,"Deep learning approaches have achieved state-of-the-art performance in
cardiac magnetic resonance (CMR) image segmentation. However, most approaches
have focused on learning image intensity features for segmentation, whereas the
incorporation of anatomical shape priors has received less attention. In this
paper, we combine a multi-task deep learning approach with atlas propagation to
develop a shape-constrained bi-ventricular segmentation pipeline for short-axis
CMR volumetric images. The pipeline first employs a fully convolutional network
(FCN) that learns segmentation and landmark localisation tasks simultaneously.
The architecture of the proposed FCN uses a 2.5D representation, thus combining
the computational advantage of 2D FCNs networks and the capability of
addressing 3D spatial consistency without compromising segmentation accuracy.
Moreover, the refinement step is designed to explicitly enforce a shape
constraint and improve segmentation quality. This step is effective for
overcoming image artefacts (e.g. due to different breath-hold positions and
large slice thickness), which preclude the creation of anatomically meaningful
3D cardiac shapes. The proposed pipeline is fully automated, due to network's
ability to infer landmarks, which are then used downstream in the pipeline to
initialise atlas propagation. We validate the pipeline on 1831 healthy subjects
and 649 subjects with pulmonary hypertension. Extensive numerical experiments
on the two datasets demonstrate that our proposed method is robust and capable
of producing accurate, high-resolution and anatomically smooth bi-ventricular
3D models, despite the artefacts in input CMR volumes.","['cs.CV', 'cs.AI']"
Understanding Deep Learning Techniques for Image Segmentation,"The machine learning community has been overwhelmed by a plethora of deep
learning based approaches. Many challenging computer vision tasks such as
detection, localization, recognition and segmentation of objects in
unconstrained environment are being efficiently addressed by various types of
deep neural networks like convolutional neural networks, recurrent networks,
adversarial networks, autoencoders and so on. While there have been plenty of
analytical studies regarding the object detection or recognition domain, many
new deep learning techniques have surfaced with respect to image segmentation
techniques. This paper approaches these various deep learning techniques of
image segmentation from an analytical perspective. The main goal of this work
is to provide an intuitive understanding of the major techniques that has made
significant contribution to the image segmentation domain. Starting from some
of the traditional image segmentation approaches, the paper progresses
describing the effect deep learning had on the image segmentation domain.
Thereafter, most of the major segmentation algorithms have been logically
categorized with paragraphs dedicated to their unique contribution. With an
ample amount of intuitive explanations, the reader is expected to have an
improved ability to visualize the internal dynamics of these processes.","['cs.CV', 'cs.LG', 'cs.NE']"
Gated-SCNN: Gated Shape CNNs for Semantic Segmentation,"Current state-of-the-art methods for image segmentation form a dense image
representation where the color, shape and texture information are all processed
together inside a deep CNN. This however may not be ideal as they contain very
different type of information relevant for recognition. Here, we propose a new
two-stream CNN architecture for semantic segmentation that explicitly wires
shape information as a separate processing branch, i.e. shape stream, that
processes information in parallel to the classical stream. Key to this
architecture is a new type of gates that connect the intermediate layers of the
two streams. Specifically, we use the higher-level activations in the classical
stream to gate the lower-level activations in the shape stream, effectively
removing noise and helping the shape stream to only focus on processing the
relevant boundary-related information. This enables us to use a very shallow
architecture for the shape stream that operates on the image-level resolution.
Our experiments show that this leads to a highly effective architecture that
produces sharper predictions around object boundaries and significantly boosts
performance on thinner and smaller objects. Our method achieves
state-of-the-art performance on the Cityscapes benchmark, in terms of both mask
(mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong
baselines.","['cs.CV', 'cs.LG']"
Deep Active Learning for Axon-Myelin Segmentation on Histology Data,"Semantic segmentation is a crucial task in biomedical image processing, which
recent breakthroughs in deep learning have allowed to improve. However, deep
learning methods in general are not yet widely used in practice since they
require large amount of data for training complex models. This is particularly
challenging for biomedical images, because data and ground truths are a scarce
resource. Annotation efforts for biomedical images come with a real cost, since
experts have to manually label images at pixel-level on samples usually
containing many instances of the target anatomy (e.g. in histology samples:
neurons, astrocytes, mitochondria, etc.). In this paper we provide a framework
for Deep Active Learning applied to a real-world scenario. Our framework relies
on the U-Net architecture and overall uncertainty measure to suggest which
sample to annotate. It takes advantage of the uncertainty measure obtained by
taking Monte Carlo samples while using Dropout regularization scheme.
Experiments were done on spinal cord and brain microscopic histology samples to
perform a myelin segmentation task. Two realistic small datasets of 14 and 24
images were used, from different acquisition settings (Serial Block-Face
Electron Microscopy and Transmitting Electron Microscopy) and showed that our
method reached a maximum Dice value after adding 3 uncertainty-selected samples
to the initial training set, versus 15 randomly-selected samples, thereby
significantly reducing the annotation effort. We focused on a plausible
scenario and showed evidence that this straightforward implementation achieves
a high segmentation performance with very few labelled samples. We believe our
framework may benefit any biomedical researcher willing to obtain fast and
accurate image segmentation on their own dataset. The code is freely available
at https://github.com/neuropoly/deep-active-learning.","['cs.CV', 'cs.LG']"
Improved Inference via Deep Input Transfer,"Although numerous improvements have been made in the field of image
segmentation using convolutional neural networks, the majority of these
improvements rely on training with larger datasets, model architecture
modifications, novel loss functions, and better optimizers. In this paper, we
propose a new segmentation performance boosting paradigm that relies on
optimally modifying the network's input instead of the network itself. In
particular, we leverage the gradients of a trained segmentation network with
respect to the input to transfer it to a space where the segmentation accuracy
improves. We test the proposed method on three publicly available medical image
segmentation datasets: the ISIC 2017 Skin Lesion Segmentation dataset, the
Shenzhen Chest X-Ray dataset, and the CVC-ColonDB dataset, for which our method
achieves improvements of 5.8%, 0.5%, and 4.8% in the average Dice scores,
respectively.",['cs.CV']
Chan-Vese Reformulation for Selective Image Segmentation,"Selective segmentation involves incorporating user input to partition an
image into foreground and background, by discriminating between objects of a
similar type. Typically, such methods involve introducing additional
constraints to generic segmentation approaches. However, we show that this is
often inconsistent with respect to common assumptions about the image. The
proposed method introduces a new fitting term that is more useful in practice
than the Chan-Vese framework. In particular, the idea is to define a term that
allows for the background to consist of multiple regions of inhomogeneity. We
provide comparitive experimental results to alternative approaches to
demonstrate the advantages of the proposed method, broadening the possible
application of these methods.","['cs.CV', 'cs.NA', 'math.NA']"
Self-Supervised Learning for Cardiac MR Image Segmentation by Anatomical Position Prediction,"In the recent years, convolutional neural networks have transformed the field
of medical image analysis due to their capacity to learn discriminative image
features for a variety of classification and regression tasks. However,
successfully learning these features requires a large amount of manually
annotated data, which is expensive to acquire and limited by the available
resources of expert image analysts. Therefore, unsupervised, weakly-supervised
and self-supervised feature learning techniques receive a lot of attention,
which aim to utilise the vast amount of available data, while at the same time
avoid or substantially reduce the effort of manual annotation. In this paper,
we propose a novel way for training a cardiac MR image segmentation network, in
which features are learnt in a self-supervised manner by predicting anatomical
positions. The anatomical positions serve as a supervisory signal and do not
require extra manual annotation. We demonstrate that this seemingly simple task
provides a strong signal for feature learning and with self-supervised
learning, we achieve a high segmentation accuracy that is better than or
comparable to a U-net trained from scratch, especially at a small data setting.
When only five annotated subjects are available, the proposed method improves
the mean Dice metric from 0.811 to 0.852 for short-axis image segmentation,
compared to the baseline U-net.",['cs.CV']
A General Framework for Complex Network-Based Image Segmentation,"With the recent advances in complex networks theory, graph-based techniques
for image segmentation has attracted great attention recently. In order to
segment the image into meaningful connected components, this paper proposes an
image segmentation general framework using complex networks based community
detection algorithms. If we consider regions as communities, using community
detection algorithms directly can lead to an over-segmented image. To address
this problem, we start by splitting the image into small regions using an
initial segmentation. The obtained regions are used for building the complex
network. To produce meaningful connected components and detect homogeneous
communities, some combinations of color and texture based features are employed
in order to quantify the regions similarities. To sum up, the network of
regions is constructed adaptively to avoid many small regions in the image, and
then, community detection algorithms are applied on the resulting adaptive
similarity matrix to obtain the final segmented image. Experiments are
conducted on Berkeley Segmentation Dataset and four of the most influential
community detection algorithms are tested. Experimental results have shown that
the proposed general framework increases the segmentation performances compared
to some existing methods.","['cs.CV', 'cs.LG', 'stat.ML']"
Using Deep Learning to Count Albatrosses from Space,"In this paper we test the use of a deep learning approach to automatically
count Wandering Albatrosses in Very High Resolution (VHR) satellite imagery. We
use a dataset of manually labelled imagery provided by the British Antarctic
Survey to train and develop our methods. We employ a U-Net architecture,
designed for image segmentation, to simultaneously classify and localise
potential albatrosses. We aid training with the use of the Focal Loss
criterion, to deal with extreme class imbalance in the dataset. Initial results
achieve peak precision and recall values of approximately 80%. Finally we
assess the model's performance in relation to inter-observer variation, by
comparing errors against an image labelled by multiple observers. We conclude
model accuracy falls within the range of human counters. We hope that the
methods will streamline the analysis of VHR satellite images, enabling more
frequent monitoring of a species which is of high conservation concern.",['cs.CV']
Semi-Bagging Based Deep Neural Architecture to Extract Text from High Entropy Images,"Extracting texts of various size and shape from images containing multiple
objects is an important problem in many contexts, especially, in connection to
e-commerce, augmented reality assistance system in natural scene, etc. The
existing works (based on only CNN) often perform sub-optimally when the image
contains regions of high entropy having multiple objects. This paper presents
an end-to-end text detection strategy combining a segmentation algorithm and an
ensemble of multiple text detectors of different types to detect text in every
individual image segments independently. The proposed strategy involves a
super-pixel based image segmenter which splits an image into multiple regions.
A convolutional deep neural architecture is developed which works on each of
the segments and detects texts of multiple shapes, sizes, and structures. It
outperforms the competing methods in terms of coverage in detecting texts in
images especially the ones where the text of various types and sizes are
compacted in a small region along with various other objects. Furthermore, the
proposed text detection method along with a text recognizer outperforms the
existing state-of-the-art approaches in extracting text from high entropy
images. We validate the results on a dataset consisting of product images on an
e-commerce website.","['cs.CV', 'cs.LG', 'eess.IV']"
Consistent estimation of the max-flow problem: Towards unsupervised image segmentation,"Advances in the image-based diagnostics of complex biological and
manufacturing processes have brought unsupervised image segmentation to the
forefront of enabling automated, on the fly decision making. However, most
existing unsupervised segmentation approaches are either computationally
complex or require manual parameter selection (e.g., flow capacities in
max-flow/min-cut segmentation). In this work, we present a fully unsupervised
segmentation approach using a continuous max-flow formulation over the image
domain while optimally estimating the flow parameters from the image
characteristics. More specifically, we show that the maximum a posteriori
estimate of the image labels can be formulated as a continuous max-flow problem
given the flow capacities are known. The flow capacities are then iteratively
obtained by employing a novel Markov random field prior over the image domain.
We present theoretical results to establish the posterior consistency of the
flow capacities. We compare the performance of our approach on two real-world
case studies including brain tumor image segmentation and defect identification
in additively manufactured components using electron microscopic images.
Comparative results with several state-of-the-art supervised as well as
unsupervised methods suggest that the present method performs statistically
similar to the supervised methods, but results in more than 90% improvement in
the Dice score when compared to the state-of-the-art unsupervised methods.","['cs.CV', 'eess.IV']"
A Regularized Convolutional Neural Network for Semantic Image Segmentation,"Convolutional neural networks (CNNs) show outstanding performance in many
image processing problems, such as image recognition, object detection and
image segmentation. Semantic segmentation is a very challenging task that
requires recognizing, understanding what's in the image in pixel level. Though
the state of the art has been greatly improved by CNNs, there is no explicit
connections between prediction of neighbouring pixels. That is, spatial
regularity of the segmented objects is still a problem for CNNs. In this paper,
we propose a method to add spatial regularization to the segmented objects. In
our method, the spatial regularization such as total variation (TV) can be
easily integrated into CNN network. It can help CNN find a better local optimum
and make the segmentation results more robust to noise. We apply our proposed
method to Unet and Segnet, which are well established CNNs for image
segmentation, and test them on WBC, CamVid and SUN-RGBD datasets, respectively.
The results show that the regularized networks not only could provide better
segmentation results with regularization effect than the original ones but also
have certain robustness to noise.",['cs.CV']
Topology Maintained Structure Encoding,"Deep learning has been used as a powerful tool for various tasks in computer
vision, such as image segmentation, object recognition and data generation. A
key part of end-to-end training is designing the appropriate encoder to extract
specific features from the input data. However, few encoders maintain the
topological properties of data, such as connection structures and global
contours. In this paper, we introduce a Voronoi Diagram encoder based on convex
set distance (CSVD) and apply it in edge encoding. The boundaries of Voronoi
cells is related to detected edges of structures and contours. The CSVD model
improves contour extraction in CNN and structure generation in GAN. We also
show the experimental results and demonstrate that the proposed model has great
potentiality in different visual problems where topology information should be
involved.",['cs.CV']
Learning of Image Dehazing Models for Segmentation Tasks,"To evaluate their performance, existing dehazing approaches generally rely on
distance measures between the generated image and its corresponding ground
truth. Despite its ability to produce visually good images, using pixel-based
or even perceptual metrics do not guarantee, in general, that the produced
image is fit for being used as input for low-level computer vision tasks such
as segmentation. To overcome this weakness, we are proposing a novel end-to-end
approach for image dehazing, fit for being used as input to an image
segmentation procedure, while maintaining the visual quality of the generated
images. Inspired by the success of Generative Adversarial Networks (GAN), we
propose to optimize the generator by introducing a discriminator network and a
loss function that evaluates segmentation quality of dehazed images. In
addition, we make use of a supplementary loss function that verifies that the
visual and the perceptual quality of the generated image are preserved in hazy
conditions. Results obtained using the proposed technique are appealing, with a
favorable comparison to state-of-the-art approaches when considering the
performance of segmentation algorithms on the hazy images.","['cs.CV', 'eess.IV']"
Task Decomposition and Synchronization for Semantic Biomedical Image Segmentation,"Semantic segmentation is essentially important to biomedical image analysis.
Many recent works mainly focus on integrating the Fully Convolutional Network
(FCN) architecture with sophisticated convolution implementation and deep
supervision. In this paper, we propose to decompose the single segmentation
task into three subsequent sub-tasks, including (1) pixel-wise image
segmentation, (2) prediction of the class labels of the objects within the
image, and (3) classification of the scene the image belonging to. While these
three sub-tasks are trained to optimize their individual loss functions of
different perceptual levels, we propose to let them interact by the task-task
context ensemble. Moreover, we propose a novel sync-regularization to penalize
the deviation between the outputs of the pixel-wise segmentation and the class
prediction tasks. These effective regularizations help FCN utilize context
information comprehensively and attain accurate semantic segmentation, even
though the number of the images for training may be limited in many biomedical
applications. We have successfully applied our framework to three diverse 2D/3D
medical image datasets, including Robotic Scene Segmentation Challenge 18
(ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal Fundus
Glaucoma Challenge (REFUGE18). We have achieved top-tier performance in all
three challenges.","['cs.CV', 'eess.IV']"
A Partially Reversible U-Net for Memory-Efficient Volumetric Image Segmentation,"One of the key drawbacks of 3D convolutional neural networks for segmentation
is their memory footprint, which necessitates compromises in the network
architecture in order to fit into a given memory budget. Motivated by the
RevNet for image classification, we propose a partially reversible U-Net
architecture that reduces memory consumption substantially. The reversible
architecture allows us to exactly recover each layer's outputs from the
subsequent layer's ones, eliminating the need to store activations for
backpropagation. This alleviates the biggest memory bottleneck and enables very
deep (theoretically infinitely deep) 3D architectures. On the BraTS challenge
dataset, we demonstrate substantial memory savings. We further show that the
freed memory can be used for processing the whole field-of-view (FOV) instead
of patches. Increasing network depth led to higher segmentation accuracy while
growing the memory footprint only by a very small fraction, thanks to the
partially reversible architecture.","['cs.CV', 'eess.IV']"
Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation,"This paper presents a novel unsupervised domain adaptation framework, called
Synergistic Image and Feature Adaptation (SIFA), to effectively tackle the
problem of domain shift. Domain adaptation has become an important and hot
topic in recent studies on deep learning, aiming to recover performance
degradation when applying the neural networks to new testing domains. Our
proposed SIFA is an elegant learning diagram which presents synergistic fusion
of adaptations from both image and feature perspectives. In particular, we
simultaneously transform the appearance of images across domains and enhance
domain-invariance of the extracted features towards the segmentation task. The
feature encoder layers are shared by both perspectives to grasp their mutual
benefits during the end-to-end learning procedure. Without using any annotation
from the target domain, the learning of our unified model is guided by
adversarial losses, with multiple discriminators employed from various aspects.
We have extensively validated our method with a challenging application of
cross-modality medical image segmentation of cardiac structures. Experimental
results demonstrate that our SIFA model recovers the degraded performance from
17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant
margin.",['cs.CV']
From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images,"Deep learning based models, generally, require a large number of samples for
appropriate training, a requirement that is difficult to satisfy in the medical
field. This issue can usually be avoided with a proper initialization of the
weights. On the task of medical image segmentation in general, two techniques
are oftentimes employed to tackle the training of a deep network $f_T$. The
first one consists in reusing some weights of a network $f_S$ pre-trained on a
large scale database ($e.g.$ ImageNet). This procedure, also known as
$transfer$ $learning$, happens to reduce the flexibility when it comes to new
network design since $f_T$ is constrained to match some parts of $f_S$. The
second commonly used technique consists in working on image patches to benefit
from the large number of available patches. This paper brings together these
two techniques and propose to train $arbitrarily$ $designed$ $networks$ that
segment an image in one forward pass, with a focus on relatively small
databases. An experimental work have been carried out on the tasks of retinal
blood vessel segmentation and the optic disc one, using four publicly available
databases. Furthermore, three types of network are considered, going from a
very light weighted network to a densely connected one. The final results show
the efficiency of the proposed framework along with state of the art results on
all the databases.","['cs.CV', 'cs.LG']"
A sparse annotation strategy based on attention-guided active learning for 3D medical image segmentation,"3D image segmentation is one of the most important and ubiquitous problems in
medical image processing. It provides detailed quantitative analysis for
accurate disease diagnosis, abnormal detection, and classification. Currently
deep learning algorithms are widely used in medical image segmentation, most
algorithms trained models with full annotated datasets. However, obtaining
medical image datasets is very difficult and expensive, and full annotation of
3D medical image is a monotonous and time-consuming work. Partially labelling
informative slices in 3D images will be a great relief of manual annotation.
Sample selection strategies based on active learning have been proposed in the
field of 2D image, but few strategies focus on 3D images. In this paper, we
propose a sparse annotation strategy based on attention-guided active learning
for 3D medical image segmentation. Attention mechanism is used to improve
segmentation accuracy and estimate the segmentation accuracy of each slice. The
comparative experiments with three different strategies using datasets from the
developing human connectome project (dHCP) show that, our strategy only needs
15% to 20% annotated slices in brain extraction task and 30% to 35% annotated
slices in tissue segmentation task to achieve comparative results as full
annotation.","['cs.CV', 'eess.IV']"
Scalable Neural Architecture Search for 3D Medical Image Segmentation,"In this paper, a neural architecture search (NAS) framework is proposed for
3D medical image segmentation, to automatically optimize a neural architecture
from a large design space. Our NAS framework searches the structure of each
layer including neural connectivities and operation types in both of the
encoder and decoder. Since optimizing over a large discrete architecture space
is difficult due to high-resolution 3D medical images, a novel stochastic
sampling algorithm based on a continuous relaxation is also proposed for
scalable gradient based optimization. On the 3D medical image segmentation
tasks with a benchmark dataset, an automatically designed architecture by the
proposed NAS framework outperforms the human-designed 3D U-Net, and moreover
this optimized architecture is well suited to be transferred for different
tasks.","['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']"
Topology-Preserving Deep Image Segmentation,"Segmentation algorithms are prone to make topological errors on fine-scale
structures, e.g., broken connections. We propose a novel method that learns to
segment with correct topology. In particular, we design a continuous-valued
loss function that enforces a segmentation to have the same topology as the
ground truth, i.e., having the same Betti number. The proposed
topology-preserving loss function is differentiable and we incorporate it into
end-to-end training of a deep neural network. Our method achieves much better
performance on the Betti number error, which directly accounts for the
topological correctness. It also performs superiorly on other topology-relevant
metrics, e.g., the Adjusted Rand Index and the Variation of Information. We
illustrate the effectiveness of the proposed method on a broad spectrum of
natural and biomedical datasets.","['cs.CV', 'cs.CG']"
When Unseen Domain Generalization is Unnecessary? Rethinking Data Augmentation,"Recent advances in deep learning for medical image segmentation demonstrate
expert-level accuracy. However, in clinically realistic environments, such
methods have marginal performance due to differences in image domains,
including different imaging protocols, device vendors and patient populations.
Here we consider the problem of domain generalization, when a model is trained
once, and its performance generalizes to unseen domains. Intuitively, within a
specific medical imaging modality the domain differences are smaller relative
to natural images domain variability. We rethink data augmentation for medical
3D images and propose a deep stacked transformations (DST) approach for domain
generalization. Specifically, a series of n stacked transformations are applied
to each image in each mini-batch during network training to account for the
contribution of domain-specific shifts in medical images. We comprehensively
evaluate our method on three tasks: segmentation of whole prostate from 3D MRI,
left atrial from 3D MRI, and left ventricle from 3D ultrasound. We demonstrate
that when trained on a small source dataset, (i) on average, DST models on
unseen datasets degrade only by 11% (Dice score change), compared to the
conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation
method (degrading 25%); (ii) when evaluation on the same domain, DST is also
better albeit only marginally. (iii) When training on large-sized data, DST on
unseen domains reaches performance of state-of-the-art fully supervised models.
These findings establish a strong benchmark for the study of domain
generalization in medical imaging, and can be generalized to the design of
robust deep segmentation models for clinical deployment.","['cs.CV', 'eess.IV']"
PAN: Projective Adversarial Network for Medical Image Segmentation,"Adversarial learning has been proven to be effective for capturing long-range
and high-level label consistencies in semantic segmentation. Unique to medical
imaging, capturing 3D semantics in an effective yet computationally efficient
way remains an open problem. In this study, we address this computational
burden by proposing a novel projective adversarial network, called PAN, which
incorporates high-level 3D information through 2D projections. Furthermore, we
introduce an attention module into our framework that helps for a selective
integration of global information directly from our segmentor to our
adversarial network. For the clinical application we chose pancreas
segmentation from CT scans. Our proposed framework achieved state-of-the-art
performance without adding to the complexity of the segmentor.","['cs.CV', 'cs.LG', 'eess.IV']"
Semantic-guided Encoder Feature Learning for Blurry Boundary Delineation,"Encoder-decoder architectures are widely adopted for medical image
segmentation tasks. With the lateral skip connection, the models can obtain and
fuse both semantic and resolution information in deep layers to achieve more
accurate segmentation performance. However, in many applications (e.g., blurry
boundary images), these models often cannot precisely locate complex boundaries
and segment tiny isolated parts. To solve this challenging problem, we firstly
analyze why simple skip connections are not enough to help accurately locate
indistinct boundaries and argue that it is due to the fuzzy information in the
skip connection provided in the encoder layers. Then we propose a
semantic-guided encoder feature learning strategy to learn both high resolution
and rich semantic encoder features so that we can more accurately locate the
blurry boundaries, which can also enhance the network by selectively learning
discriminative features. Besides, we further propose a soft contour constraint
mechanism to model the blurry boundary detection. Experimental results on real
clinical datasets show that our proposed method can achieve state-of-the-art
segmentation accuracy, especially for the blurry regions. Further analysis also
indicates that our proposed network components indeed contribute to the
improvement of performance. Experiments on additional datasets validate the
generalization ability of our proposed method.",['cs.CV']
MAVNet: an Effective Semantic Segmentation Micro-Network for MAV-based Tasks,"Real-time semantic image segmentation on platforms subject to size, weight
and power (SWaP) constraints is a key area of interest for air surveillance and
inspection. In this work, we propose MAVNet: a small, light-weight, deep neural
network for real-time semantic segmentation on micro Aerial Vehicles (MAVs).
MAVNet, inspired by ERFNet, features 400 times fewer parameters and achieves
comparable performance with some reference models in empirical experiments. Our
model achieves a trade-off between speed and accuracy, achieving up to 48 FPS
on an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing high
resolution imagery. Additionally, we provide two novel datasets that represent
challenges in semantic segmentation for real-time MAV tracking and
infrastructure inspection tasks and verify MAVNet on these datasets. Our
algorithm and datasets are made publicly available.",['cs.CV']
Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation,"In this work, we explore the issue of the inter-annotator agreement for
training and evaluating automated segmentation of skin lesions. We explore what
different degrees of agreement represent, and how they affect different use
cases for segmentation. We also evaluate how conditioning the ground truths
using different (but very simple) algorithms may help to enhance agreement and
may be appropriate for some use cases. The segmentation of skin lesions is a
cornerstone task for automated skin lesion analysis, useful both as an
end-result to locate/detect the lesions and as an ancillary task for lesion
classification. Lesion segmentation, however, is a very challenging task, due
not only to the challenge of image segmentation itself but also to the
difficulty in obtaining properly annotated data. Detecting accurately the
borders of lesions is challenging even for trained humans, since, for many
lesions, those borders are fuzzy and ill-defined. Using lesions and annotations
from the ISIC Archive, we estimate inter-annotator agreement for skin-lesion
segmentation and propose several simple procedures that may help to improve
inter-annotator agreement if used to condition the ground truths.",['cs.CV']
Learning Shape Representation on Sparse Point Clouds for Volumetric Image Segmentation,"Volumetric image segmentation with convolutional neural networks (CNNs)
encounters several challenges, which are specific to medical images. Among
these challenges are large volumes of interest, high class imbalances, and
difficulties in learning shape representations. To tackle these challenges, we
propose to improve over traditional CNN-based volumetric image segmentation
through point-wise classification of point clouds. The sparsity of point clouds
allows processing of entire image volumes, balancing highly imbalanced
segmentation problems, and explicitly learning an anatomical shape. We build
upon PointCNN, a neural network proposed to process point clouds, and propose
here to jointly encode shape and volumetric information within the point cloud
in a compact and computationally effective manner. We demonstrate how this
approach can then be used to refine CNN-based segmentation, which yields
significantly improved results in our experiments on the difficult task of
peripheral nerve segmentation from magnetic resonance neurography images. By
synthetic experiments, we further show the capability of our approach in
learning an explicit anatomical shape representation.","['cs.CV', 'eess.IV']"
VoteNet: A Deep Learning Label Fusion Method for Multi-Atlas Segmentation,"Deep learning (DL) approaches are state-of-the-art for many medical image
segmentation tasks. They offer a number of advantages: they can be trained for
specific tasks, computations are fast at test time, and segmentation quality is
typically high. In contrast, previously popular multi-atlas segmentation (MAS)
methods are relatively slow (as they rely on costly registrations) and even
though sophisticated label fusion strategies have been proposed, DL approaches
generally outperform MAS. In this work, we propose a DL-based label fusion
strategy (VoteNet) which locally selects a set of reliable atlases whose labels
are then fused via plurality voting. Experiments on 3D brain MRI data show that
by selecting a good initial atlas set MAS with VoteNet significantly
outperforms a number of other label fusion strategies as well as a direct DL
segmentation approach. We also provide an experimental analysis of the upper
performance bound achievable by our method. While unlikely achievable in
practice, this bound suggests room for further performance improvements.
Lastly, to address the runtime disadvantage of standard MAS, all our results
make use of a fast DL registration approach.",['cs.CV']
OCTID: Optical Coherence Tomography Image Database,"Optical coherence tomography (OCT) is a non-invasive imaging modality which
is widely used in clinical ophthalmology. OCT images are capable of visualizing
deep retinal layers which is crucial for early diagnosis of retinal diseases.
In this paper, we describe a comprehensive open-access database containing more
than 500 highresolution images categorized into different pathological
conditions. The image classes include Normal (NO), Macular Hole (MH),
Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), and
Diabetic Retinopathy (DR). The images were obtained from a raster scan protocol
with a 2mm scan length and 512x1024 pixel resolution. We have also included 25
normal OCT images with their corresponding ground truth delineations which can
be used for an accurate evaluation of OCT image segmentation. In addition, we
have provided a user-friendly GUI which can be used by clinicians for manual
(and semi-automated) segmentation.","['cs.CV', 'cs.LG']"
Budget-aware Semi-Supervised Semantic and Instance Segmentation,"Methods that move towards less supervised scenarios are key for image
segmentation, as dense labels demand significant human intervention. Generally,
the annotation burden is mitigated by labeling datasets with weaker forms of
supervision, e.g. image-level labels or bounding boxes. Another option are
semi-supervised settings, that commonly leverage a few strong annotations and a
huge number of unlabeled/weakly-labeled data. In this paper, we revisit
semi-supervised segmentation schemes and narrow down significantly the
annotation budget (in terms of total labeling time of the training set)
compared to previous approaches. With a very simple pipeline, we demonstrate
that at low annotation budgets, semi-supervised methods outperform by a wide
margin weakly-supervised ones for both semantic and instance segmentation. Our
approach also outperforms previous semi-supervised works at a much reduced
labeling cost. We present results for the Pascal VOC benchmark and unify weakly
and semi-supervised approaches by considering the total annotation budget, thus
allowing a fairer comparison between methods.",['cs.CV']
End-to-End Learned Random Walker for Seeded Image Segmentation,"We present an end-to-end learned algorithm for seeded segmentation. Our
method is based on the Random Walker algorithm, where we predict the edge
weights of the underlying graph using a convolutional neural network. This can
be interpreted as learning context-dependent diffusivities for a linear
diffusion process. Besides calculating the exact gradient for optimizing these
diffusivities, we also propose simplifications that sparsely sample the
gradient and still yield competitive results. The proposed method achieves the
currently best results on a seeded version of the CREMI neuron segmentation
challenge.","['cs.CV', 'cs.LG']"
Boundary Loss for Remote Sensing Imagery Semantic Segmentation,"In response to the growing importance of geospatial data, its analysis
including semantic segmentation becomes an increasingly popular task in
computer vision today. Convolutional neural networks are powerful visual models
that yield hierarchies of features and practitioners widely use them to process
remote sensing data. When performing remote sensing image segmentation,
multiple instances of one class with precisely defined boundaries are often the
case, and it is crucial to extract those boundaries accurately. The accuracy of
segments boundaries delineation influences the quality of the whole segmented
areas explicitly. However, widely-used segmentation loss functions such as BCE,
IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently.
In this paper, we propose a novel loss function, namely a differentiable
surrogate of a metric accounting accuracy of boundary detection. We can use the
loss function with any neural network for binary segmentation. We performed
validation of our loss function with various modifications of UNet on a
synthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL).
Trained with the proposed loss function, models outperform baseline methods in
terms of IoU score.","['cs.CV', 'cs.LG']"
A 2D dilated residual U-Net for multi-organ segmentation in thoracic CT,"Automatic segmentation of organs-at-risk (OAR) in computed tomography (CT) is
an essential part of planning effective treatment strategies to combat lung and
esophageal cancer. Accurate segmentation of organs surrounding tumours helps
account for the variation in position and morphology inherent across patients,
thereby facilitating adaptive and computer-assisted radiotherapy. Although
manual delineation of OARs is still highly prevalent, it is prone to errors due
to complex variations in the shape and position of organs across patients, and
low soft tissue contrast between neighbouring organs in CT images. Recently,
deep convolutional neural networks (CNNs) have gained tremendous traction and
achieved state-of-the-art results in medical image segmentation. In this paper,
we propose a deep learning framework to segment OARs in thoracic CT images,
specifically for the: heart, esophagus, trachea and aorta. Our approach employs
dilated convolutions and aggregated residual connections in the bottleneck of a
U-Net styled network, which incorporates global context and dense information.
Our method achieved an overall Dice score of 91.57% on 20 unseen test samples
from the ISBI 2019 SegTHOR challenge.","['cs.CV', 'eess.IV']"
Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells,"Automated design of neural network architectures tailored for a specific task
is an extremely promising, albeit inherently difficult, avenue to explore.
While most results in this domain have been achieved on image classification
and language modelling problems, here we concentrate on dense per-pixel tasks,
in particular, semantic image segmentation using fully convolutional networks.
In contrast to the aforementioned areas, the design choices of a fully
convolutional network require several changes, ranging from the sort of
operations that need to be used---e.g., dilated convolutions---to a solving of
a more difficult optimisation problem. In this work, we are particularly
interested in searching for high-performance compact segmentation
architectures, able to run in real-time using limited resources. To achieve
that, we intentionally over-parameterise the architecture during the training
time via a set of auxiliary cells that provide an intermediate supervisory
signal and can be omitted during the evaluation phase. The design of the
auxiliary cell is emitted by a controller, a neural network with the fixed
structure trained using reinforcement learning. More crucially, we demonstrate
how to efficiently search for these architectures within limited time and
computational budgets. In particular, we rely on a progressive strategy that
terminates non-promising architectures from being further trained, and on
Polyak averaging coupled with knowledge distillation to speed-up the
convergence. Quantitatively, in 8 GPU-days our approach discovers a set of
architectures performing on-par with state-of-the-art among compact models on
the semantic segmentation, pose estimation and depth prediction tasks. Code
will be made available here: https://github.com/drsleep/nas-segm-pytorch",['cs.CV']
Harvesting Information from Captions for Weakly Supervised Semantic Segmentation,"Since acquiring pixel-wise annotations for training convolutional neural
networks for semantic image segmentation is time-consuming, weakly supervised
approaches that only require class tags have been proposed. In this work, we
propose another form of supervision, namely image captions as they can be found
on the Internet. These captions have two advantages. They do not require
additional curation as it is the case for the clean class tags used by current
weakly supervised approaches and they provide textual context for the classes
present in an image. To leverage such textual context, we deploy a multi-modal
network that learns a joint embedding of the visual representation of the image
and the textual representation of the caption. The network estimates text
activation maps (TAMs) for class names as well as compound concepts, i.e.
combinations of nouns and their attributes. The TAMs of compound concepts
describing classes of interest substantially improve the quality of the
estimated class activation maps which are then used to train a network for
semantic segmentation. We evaluate our method on the COCO dataset where it
achieves state of the art results for weakly supervised image segmentation.",['cs.CV']
Diversity in Machine Learning,"Machine learning methods have achieved good performance and been widely
applied in various real-world applications. They can learn the model adaptively
and be better fit for special requirements of different tasks. Generally, a
good machine learning system is composed of plentiful training data, a good
model training process, and an accurate inference. Many factors can affect the
performance of the machine learning process, among which the diversity of the
machine learning process is an important one. The diversity can help each
procedure to guarantee a total good machine learning: diversity of the training
data ensures that the training data can provide more discriminative information
for the model, diversity of the learned model (diversity in parameters of each
model or diversity among different base models) makes each parameter/model
capture unique or complement information and the diversity in inference can
provide multiple choices each of which corresponds to a specific plausible
local optimal result. Even though the diversity plays an important role in
machine learning process, there is no systematical analysis of the
diversification in machine learning system. In this paper, we systematically
summarize the methods to make data diversification, model diversification, and
inference diversification in the machine learning process, respectively. In
addition, the typical applications where the diversity technology improved the
machine learning performance have been surveyed, including the remote sensing
imaging tasks, machine translation, camera relocalization, image segmentation,
object detection, topic modeling, and others. Finally, we discuss some
challenges of the diversity technology in machine learning and point out some
directions in future work.",['cs.CV']
Automated Segmentation of Cervical Nuclei in Pap Smear Images using Deformable Multi-path Ensemble Model,"Pap smear testing has been widely used for detecting cervical cancers based
on the morphology properties of cell nuclei in microscopic image. An accurate
nuclei segmentation could thus improve the success rate of cervical cancer
screening. In this work, a method of automated cervical nuclei segmentation
using Deformable Multipath Ensemble Model (D-MEM) is proposed. The approach
adopts a U-shaped convolutional network as a backbone network, in which dense
blocks are used to transfer feature information more effectively. To increase
the flexibility of the model, we then use deformable convolution to deal with
different nuclei irregular shapes and sizes. To reduce the predictive bias, we
further construct multiple networks with different settings, which form an
ensemble model. The proposed segmentation framework has achieved
state-of-the-art accuracy on Herlev dataset with Zijdenbos similarity index
(ZSI) of 0.933, and has the potential to be extended for solving other medical
image segmentation tasks.",['cs.CV']
Open Source Presentation Attack Detection Baseline for Iris Recognition,"This paper proposes the first, known to us, open source presentation attack
detection (PAD) solution to distinguish between authentic iris images (possibly
wearing clear contact lenses) and irises with textured contact lenses. This
software can serve as a baseline in various PAD evaluations, and also as an
open-source platform with an up-to-date reference method for iris PAD. The
software is written in C++ and Python and uses only open source resources, such
as OpenCV. This method does not incorporate iris image segmentation, which may
be problematic for unknown fake samples. Instead, it makes a best guess to
localize the rough position of the iris. The PAD-related features are extracted
with the Binary Statistical Image Features (BSIF), which are classified by an
ensemble of classifiers incorporating support vector machine, random forest and
multilayer perceptron. The models attached to the current software have been
trained with the NDCLD'15 database and evaluated on the independent datasets
included in the LivDet-Iris 2017 competition. The software implements the
functionality of retraining the classifiers with any database of authentic and
attack images. The accuracy of the current version offered with this paper
exceeds 99% when tested on subject-disjoint subsets of NDCLD'15, and oscillates
around 85% when tested on the LivDet-Iris 2017 benchmarks, which is on par with
the results obtained by the LivDet-Iris 2017 winner.",['cs.CV']
Nostalgin: Extracting 3D City Models from Historical Image Data,"What did it feel like to walk through a city from the past? In this work, we
describe Nostalgin (Nostalgia Engine), a method that can faithfully reconstruct
cities from historical images. Unlike existing work in city reconstruction, we
focus on the task of reconstructing 3D cities from historical images. Working
with historical image data is substantially more difficult, as there are
significantly fewer buildings available and the details of the camera
parameters which captured the images are unknown. Nostalgin can generate a city
model even if there is only a single image per facade, regardless of viewpoint
or occlusions. To achieve this, our novel architecture combines image
segmentation, rectification, and inpainting. We motivate our design decisions
with experimental analysis of individual components of our pipeline, and show
that we can improve on baselines in both speed and visual realism. We
demonstrate the efficacy of our pipeline by recreating two 1940s Manhattan city
blocks. We aim to deploy Nostalgin as an open source platform where users can
generate immersive historical experiences from their own photos.","['cs.CV', 'cs.CG', 'cs.LG']"
Semantic Segmentation of Video Sequences with Convolutional LSTMs,"Most of the semantic segmentation approaches have been developed for single
image segmentation, and hence, video sequences are currently segmented by
processing each frame of the video sequence separately. The disadvantage of
this is that temporal image information is not considered, which improves the
performance of the segmentation approach. One possibility to include temporal
information is to use recurrent neural networks. However, there are only a few
approaches using recurrent networks for video segmentation so far. These
approaches extend the encoder-decoder network architecture of well-known
segmentation approaches and place convolutional LSTM layers between encoder and
decoder. However, in this paper it is shown that this position is not optimal,
and that other positions in the network exhibit better performance. Nowadays,
state-of-the-art segmentation approaches rarely use the classical
encoder-decoder structure, but use multi-branch architectures. These
architectures are more complex, and hence, it is more difficult to place the
recurrent units at a proper position. In this work, the multi-branch
architectures are extended by convolutional LSTM layers at different positions
and evaluated on two different datasets in order to find the best one. It
turned out that the proposed approach outperforms the pure CNN-based approach
for up to 1.6 percent.","['cs.CV', 'cs.AI']"
Non-Local Context Encoder: Robust Biomedical Image Segmentation against Adversarial Attacks,"Recent progress in biomedical image segmentation based on deep convolutional
neural networks (CNNs) has drawn much attention. However, its vulnerability
towards adversarial samples cannot be overlooked. This paper is the first one
that discovers that all the CNN-based state-of-the-art biomedical image
segmentation models are sensitive to adversarial perturbations. This limits the
deployment of these methods in safety-critical biomedical fields. In this
paper, we discover that global spatial dependencies and global contextual
information in a biomedical image can be exploited to defend against
adversarial attacks. To this end, non-local context encoder (NLCE) is proposed
to model short- and long range spatial dependencies and encode global contexts
for strengthening feature activations by channel-wise attention. The NLCE
modules enhance the robustness and accuracy of the non-local context encoding
network (NLCEN), which learns robust enhanced pyramid feature representations
with NLCE modules, and then integrates the information across different levels.
Experiments on both lung and skin lesion segmentation datasets have
demonstrated that NLCEN outperforms any other state-of-the-art biomedical image
segmentation methods against adversarial attacks. In addition, NLCE modules can
be applied to improve the robustness of other CNN-based biomedical image
segmentation methods.","['cs.CV', 'eess.IV']"
Small Target Detection for Search and Rescue Operations using Distributed Deep Learning and Synthetic Data Generation,"It is important to find the target as soon as possible for search and rescue
operations. Surveillance camera systems and unmanned aerial vehicles (UAVs) are
used to support search and rescue. Automatic object detection is important
because a person cannot monitor multiple surveillance screens simultaneously
for 24 hours. Also, the object is often too small to be recognized by the human
eye on the surveillance screen. This study used UAVs around the Port of Houston
and fixed surveillance cameras to build an automatic target detection system
that supports the US Coast Guard (USCG) to help find targets (e.g., person
overboard). We combined image segmentation, enhancement, and convolution neural
networks to reduce detection time to detect small targets. We compared the
performance between the auto-detection system and the human eye. Our system
detected the target within 8 seconds, but the human eye detected the target
within 25 seconds. Our systems also used synthetic data generation and data
augmentation techniques to improve target detection accuracy. This solution may
help the search and rescue operations of the first responders in a timely
manner.",['cs.CV']
Indoor dense depth map at drone hovering,"Autonomous Micro Aerial Vehicles (MAVs) gained tremendous attention in recent
years. Autonomous flight in indoor requires a dense depth map for navigable
space detection which is the fundamental component for autonomous navigation.
In this paper, we address the problem of reconstructing dense depth while a
drone is hovering (small camera motion) in indoor scenes using already
estimated cameras and sparse point cloud obtained from a vSLAM. We start by
segmenting the scene based on sudden depth variation using sparse 3D points and
introduce a patch-based local plane fitting via energy minimization which
combines photometric consistency and co-planarity with neighbouring patches.
The method also combines a plane sweep technique for image segments having
almost no sparse point for initialization. Experiments show, the proposed
method produces better depth for indoor in artificial lighting condition,
low-textured environment compared to earlier literature in small motion.",['cs.CV']
Skin Cancer Segmentation and Classification with NABLA-N and Inception Recurrent Residual Convolutional Networks,"In the last few years, Deep Learning (DL) has been showing superior
performance in different modalities of biomedical image analysis. Several DL
architectures have been proposed for classification, segmentation, and
detection tasks in medical imaging and computational pathology. In this paper,
we propose a new DL architecture, the NABLA-N network, with better feature
fusion techniques in decoding units for dermoscopic image segmentation tasks.
The NABLA-N network has several advances for segmentation tasks. First, this
model ensures better feature representation for semantic segmentation with a
combination of low to high-level feature maps. Second, this network shows
better quantitative and qualitative results with the same or fewer network
parameters compared to other methods. In addition, the Inception Recurrent
Residual Convolutional Neural Network (IRRCNN) model is used for skin cancer
classification. The proposed NABLA-N network and IRRCNN models are evaluated
for skin cancer segmentation and classification on the benchmark datasets from
the International Skin Imaging Collaboration 2018 (ISIC-2018). The experimental
results show superior performance on segmentation tasks compared to the
Recurrent Residual U-Net (R2U-Net). The classification model shows around 87%
testing accuracy for dermoscopic skin cancer classification on ISIC2018
dataset.","['cs.CV', 'eess.IV']"
The iterative convolution-thresholding method (ICTM) for image segmentation,"In this paper, we propose a novel iterative convolution-thresholding method
(ICTM) that is applicable to a range of variational models for image
segmentation. A variational model usually minimizes an energy functional
consisting of a fidelity term and a regularization term. In the ICTM, the
interface between two different segment domains is implicitly represented by
their characteristic functions. The fidelity term is then usually written as a
linear functional of the characteristic functions and the regularized term is
approximated by a functional of characteristic functions in terms of heat
kernel convolution. This allows us to design an iterative
convolution-thresholding method to minimize the approximate energy. The method
is simple, efficient and enjoys the energy-decaying property. Numerical
experiments show that the method is easy to implement, robust and applicable to
various image segmentation models.",['cs.CV']
Bidirectional Learning for Domain Adaptation of Semantic Segmentation,"Domain adaptation for semantic image segmentation is very necessary since
manually labeling large datasets with pixel-level labels is expensive and time
consuming. Existing domain adaptation techniques either work on limited
datasets, or yield not so good performance compared with supervised learning.
In this paper, we propose a novel bidirectional learning framework for domain
adaptation of segmentation. Using the bidirectional learning, the image
translation model and the segmentation adaptation model can be learned
alternatively and promote to each other. Furthermore, we propose a
self-supervised learning algorithm to learn a better segmentation adaptation
model and in return improve the image translation model. Experiments show that
our method is superior to the state-of-the-art methods in domain adaptation of
segmentation with a big margin. The source code is available at
https://github.com/liyunsheng13/BDL.",['cs.CV']
XNet: A convolutional neural network (CNN) implementation for medical X-Ray image segmentation suitable for small datasets,"X-Ray image enhancement, along with many other medical image processing
applications, requires the segmentation of images into bone, soft tissue, and
open beam regions. We apply a machine learning approach to this problem,
presenting an end-to-end solution which results in robust and efficient
inference. Since medical institutions frequently do not have the resources to
process and label the large quantity of X-Ray images usually needed for neural
network training, we design an end-to-end solution for small datasets, while
achieving state-of-the-art results. Our implementation produces an overall
accuracy of 92%, F1 score of 0.92, and an AUC of 0.98, surpassing classical
image processing techniques, such as clustering and entropy based methods,
while improving upon the output of existing neural networks used for
segmentation in non-medical contexts. The code used for this project is
available online.","['cs.CV', 'cs.AI', 'physics.med-ph']"
Self-Supervised Audio-Visual Co-Segmentation,"Segmenting objects in images and separating sound sources in audio are
challenging tasks, in part because traditional approaches require large amounts
of labeled data. In this paper we develop a neural network model for visual
object segmentation and sound source separation that learns from natural videos
through self-supervision. The model is an extension of recently proposed work
that maps image pixels to sounds. Here, we introduce a learning approach to
disentangle concepts in the neural networks, and assign semantic categories to
network feature channels to enable independent image segmentation and sound
source separation after audio-visual training on videos. Our evaluations show
that the disentangled model outperforms several baselines in semantic
segmentation and sound source separation.","['cs.CV', 'cs.SD', 'eess.AS', 'eess.IV']"
A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation,"Cardiac image segmentation is a critical process for generating personalized
models of the heart and for quantifying cardiac performance parameters. Several
convolutional neural network (CNN) architectures have been proposed to segment
the heart chambers from cardiac cine MR images. Here we propose a multi-task
learning (MTL)-based regularization framework for cardiac MR image
segmentation. The network is trained to perform the main task of semantic
segmentation, along with a simultaneous, auxiliary task of pixel-wise distance
map regression. The proposed distance map regularizer is a decoder network
added to the bottleneck layer of an existing CNN architecture, facilitating the
network to learn robust global features. The regularizer block is removed after
training, so that the original number of network parameters does not change. We
show that the proposed regularization method improves both binary and
multi-class segmentation performance over the corresponding state-of-the-art
CNN architectures on two publicly available cardiac cine MRI datasets,
obtaining average dice coefficient of 0.84$\pm$0.03 and 0.91$\pm$0.04,
respectively. Furthermore, we also demonstrate improved generalization
performance of the distance map regularized network on cross-dataset
segmentation, showing as much as 42% improvement in myocardium Dice coefficient
from 0.56$\pm$0.28 to 0.80$\pm$0.14.",['cs.CV']
Road Crack Detection Using Deep Convolutional Neural Network and Adaptive Thresholding,"Crack is one of the most common road distresses which may pose road safety
hazards. Generally, crack detection is performed by either certified inspectors
or structural engineers. This task is, however, time-consuming, subjective and
labor-intensive. In this paper, we propose a novel road crack detection
algorithm based on deep learning and adaptive image segmentation. Firstly, a
deep convolutional neural network is trained to determine whether an image
contains cracks or not. The images containing cracks are then smoothed using
bilateral filtering, which greatly minimizes the number of noisy pixels.
Finally, we utilize an adaptive thresholding method to extract the cracks from
road surface. The experimental results illustrate that our network can classify
images with an accuracy of 99.92%, and the cracks can be successfully extracted
from the images using our proposed thresholding algorithm.","['cs.CV', 'cs.LG', 'eess.IV']"
Fast Single Image Dehazing via Multilevel Wavelet Transform based Optimization,"The quality of images captured in outdoor environments can be affected by
poor weather conditions such as fog, dust, and atmospheric scattering of other
particles. This problem can bring extra challenges to high-level computer
vision tasks like image segmentation and object detection. However, previous
studies on image dehazing suffer from a huge computational workload and
corruption of the original image, such as over-saturation and halos. In this
paper, we present a novel image dehazing approach based on the optical model
for haze images and regularized optimization. Specifically, we convert the
non-convex, bilinear problem concerning the unknown haze-free image and light
transmission distribution to a convex, linear optimization problem by
estimating the atmosphere light constant. Our method is further accelerated by
introducing a multilevel Haar wavelet transform. The optimization, instead, is
applied to the low frequency sub-band decomposition of the original image. This
dimension reduction significantly improves the processing speed of our method
and exhibits the potential for real-time applications. Experimental results
show that our approach outperforms state-of-the-art dehazing algorithms in
terms of both image reconstruction quality and computational efficiency. For
implementation details, source code can be publicly accessed via
http://github.com/JiaxiHe/Image-and-Video-Dehazing.","['cs.CV', 'eess.IV', 'G.1.6']"
Region homogeneity in the Logarithmic Image Processing framework: application to region growing algorithms,"In order to create an image segmentation method robust to lighting changes,
two novel homogeneity criteria of an image region were studied. Both were
defined using the Logarithmic Image Processing (LIP) framework whose laws model
lighting changes. The first criterion estimates the LIP-additive homogeneity
and is based on the LIP-additive law. It is theoretically insensitive to
lighting changes caused by variations of the camera exposure-time or source
intensity. The second, the LIP-multiplicative homogeneity criterion, is based
on the LIP-multiplicative law and is insensitive to changes due to variations
of the object thickness or opacity. Each criterion is then applied in Revol and
Jourlin's (1997) region growing method which is based on the homogeneity of an
image region. The region growing method becomes therefore robust to the
lighting changes specific to each criterion. Experiments on simulated and on
real images presenting lighting variations prove the robustness of the criteria
to those variations. Compared to a state-of the art method based on the image
component-tree, ours is more robust. These results open the way to numerous
applications where the lighting is uncontrolled or partially controlled.",['cs.CV']
Super Resolution Convolutional Neural Network Models for Enhancing Resolution of Rock Micro-CT Images,"Single Image Super Resolution (SISR) techniques based on Super Resolution
Convolutional Neural Networks (SRCNN) are applied to micro-computed tomography
({\mu}CT) images of sandstone and carbonate rocks. Digital rock imaging is
limited by the capability of the scanning device resulting in trade-offs
between resolution and field of view, and super resolution methods tested in
this study aim to compensate for these limits. SRCNN models SR-Resnet, Enhanced
Deep SR (EDSR), and Wide-Activation Deep SR (WDSR) are used on the Digital Rock
Super Resolution 1 (DRSRD1) Dataset of 4x downsampled images, comprising of
2000 high resolution (800x800) raw micro-CT images of Bentheimer sandstone and
Estaillades carbonate. The trained models are applied to the validation and
test data within the dataset and show a 3-5 dB rise in image quality compared
to bicubic interpolation, with all tested models performing within a 0.1 dB
range. Difference maps indicate that edge sharpness is completely recovered in
images within the scope of the trained model, with only high frequency noise
related detail loss. We find that aside from generation of high-resolution
images, a beneficial side effect of super resolution methods applied to
synthetically downgraded images is the removal of image noise while recovering
edgewise sharpness which is beneficial for the segmentation process. The model
is also tested against real low-resolution images of Bentheimer rock with image
augmentation to account for natural noise and blur. The SRCNN method is shown
to act as a preconditioner for image segmentation under these circumstances
which naturally leads to further future development and training of models that
segment an image directly. Image restoration by SRCNN on the rock images is of
significantly higher quality than traditional methods and suggests SRCNN
methods are a viable processing step in a digital rock workflow.",['cs.CV']
Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology,"Automated digital histopathology image segmentation is an important task to
help pathologists diagnose tumors and cancer subtypes. For pathological
diagnosis of cancer subtypes, pathologists usually change the magnification of
whole-slide images (WSI) viewers. A key assumption is that the importance of
the magnifications depends on the characteristics of the input image, such as
cancer subtypes. In this paper, we propose a novel semantic segmentation
method, called Adaptive-Weighting-Multi-Field-of-View-CNN (AWMF-CNN), that can
adaptively use image features from images with different magnifications to
segment multiple cancer subtype regions in the input image. The proposed method
aggregates several expert CNNs for images of different magnifications by
adaptively changing the weight of each expert depending on the input image. It
leverages information in the images with different magnifications that might be
useful for identifying the subtypes. It outperformed other state-of-the-art
methods in experiments.",['cs.CV']
Panoptic Segmentation,"We propose and study a task we name panoptic segmentation (PS). Panoptic
segmentation unifies the typically distinct tasks of semantic segmentation
(assign a class label to each pixel) and instance segmentation (detect and
segment each object instance). The proposed task requires generating a coherent
scene segmentation that is rich and complete, an important step toward
real-world vision systems. While early work in computer vision addressed
related image/scene parsing tasks, these are not currently popular, possibly
due to lack of appropriate metrics or associated recognition challenges. To
address this, we propose a novel panoptic quality (PQ) metric that captures
performance for all classes (stuff and things) in an interpretable and unified
manner. Using the proposed metric, we perform a rigorous study of both human
and machine performance for PS on three existing datasets, revealing
interesting insights about the task. The aim of our work is to revive the
interest of the community in a more unified view of image segmentation.",['cs.CV']
Interactive Full Image Segmentation by Considering All Regions Jointly,"We address interactive full image annotation, where the goal is to accurately
segment all object and stuff regions in an image. We propose an interactive,
scribble-based annotation framework which operates on the whole image to
produce segmentations for all regions. This enables sharing scribble
corrections across regions, and allows the annotator to focus on the largest
errors made by the machine across the whole image. To realize this, we adapt
Mask-RCNN into a fast interactive segmentation framework and introduce an
instance-aware loss measured at the pixel-level in the full image canvas, which
lets predictions for nearby regions properly compete for space. Finally, we
compare to interactive single object segmentation on the COCO panoptic dataset.
We demonstrate that our interactive full image segmentation approach leads to a
5% IoU gain, reaching 90% IoU at a budget of four extreme clicks and four
corrective scribbles per region.",['cs.CV']
3D Quantum Cuts for Automatic Segmentation of Porous Media in Tomography Images,"Binary segmentation of volumetric images of porous media is a crucial step
towards gaining a deeper understanding of the factors governing biogeochemical
processes at minute scales. Contemporary work primarily revolves around
primitive techniques based on global or local adaptive thresholding that have
known common drawbacks in image segmentation. Moreover, absence of a unified
benchmark prohibits quantitative evaluation, which further clouds the impact of
existing methodologies. In this study, we tackle the issue on both fronts.
Firstly, by drawing parallels with natural image segmentation, we propose a
novel, and automatic segmentation technique, 3D Quantum Cuts (QCuts-3D)
grounded on a state-of-the-art spectral clustering technique. Secondly, we
curate and present a publicly available dataset of 68 multiphase volumetric
images of porous media with diverse solid geometries, along with voxel-wise
ground truth annotations for each constituting phase. We provide comparative
evaluations between QCuts-3D and the current state-of-the-art over this dataset
across a variety of evaluation metrics. The proposed systematic approach
achieves a 26% increase in AUROC while achieving a substantial reduction of the
computational complexity of the state-of-the-art competitors. Moreover,
statistical analysis reveals that the proposed method exhibits significant
robustness against the compositional variations of porous media.",['cs.CV']
Cross-Modal Self-Attention Network for Referring Image Segmentation,"We consider the problem of referring image segmentation. Given an input image
and a natural language expression, the goal is to segment the object referred
by the language expression in the image. Existing works in this area treat the
language expression and the input image separately in their representations.
They do not sufficiently capture long-range correlations between these two
modalities. In this paper, we propose a cross-modal self-attention (CMSA)
module that effectively captures the long-range dependencies between linguistic
and visual features. Our model can adaptively focus on informative words in the
referring expression and important regions in the input image. In addition, we
propose a gated multi-level fusion module to selectively integrate
self-attentive cross-modal features corresponding to different levels in the
image. This module controls the information flow of features at different
levels. We validate the proposed approach on four evaluation datasets. Our
proposed approach consistently outperforms existing state-of-the-art methods.","['cs.CV', 'cs.CL']"
Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds,"Cloud based medical image analysis has become popular recently due to the
high computation complexities of various deep neural network (DNN) based
frameworks and the increasingly large volume of medical images that need to be
processed. It has been demonstrated that for medical images the transmission
from local to clouds is much more expensive than the computation in the clouds
itself. Towards this, 3D image compression techniques have been widely applied
to reduce the data traffic. However, most of the existing image compression
techniques are developed around human vision, i.e., they are designed to
minimize distortions that can be perceived by human eyes. In this paper we will
use deep learning based medical image segmentation as a vehicle and demonstrate
that interestingly, machine and human view the compression quality differently.
Medical images compressed with good quality w.r.t. human vision may result in
inferior segmentation accuracy. We then design a machine vision oriented 3D
image compression framework tailored for segmentation using DNNs. Our method
automatically extracts and retains image features that are most important to
the segmentation. Comprehensive experiments on widely adopted segmentation
frameworks with HVSMR 2016 challenge dataset show that our method can achieve
significantly higher segmentation accuracy at the same compression rate, or
much better compression rate under the same segmentation accuracy, when
compared with the existing JPEG 2000 method. To the best of the authors'
knowledge, this is the first machine vision guided medical image compression
framework for segmentation in the clouds.","['cs.CV', 'cs.LG', 'stat.ML']"
Adaptive Morphological Reconstruction for Seeded Image Segmentation,"Morphological reconstruction (MR) is often employed by seeded image
segmentation algorithms such as watershed transform and power watershed as it
is able to filter seeds (regional minima) to reduce over-segmentation. However,
MR might mistakenly filter meaningful seeds that are required for generating
accurate segmentation and it is also sensitive to the scale because a
single-scale structuring element is employed. In this paper, a novel adaptive
morphological reconstruction (AMR) operation is proposed that has three
advantages. Firstly, AMR can adaptively filter useless seeds while preserving
meaningful ones. Secondly, AMR is insensitive to the scale of structuring
elements because multiscale structuring elements are employed. Finally, AMR has
two attractive properties: monotonic increasingness and convergence that help
seeded segmentation algorithms to achieve a hierarchical segmentation.
Experiments clearly demonstrate that AMR is useful for improving algorithms of
seeded image segmentation and seed-based spectral segmentation. Compared to
several state-of-the-art algorithms, the proposed algorithms provide better
segmentation results requiring less computing time. Source code is available at
https://github.com/SUST-reynole/AMR.",['cs.CV']
Data augmentation using learned transformations for one-shot medical image segmentation,"Image segmentation is an important task in many medical applications. Methods
based on convolutional neural networks attain state-of-the-art accuracy;
however, they typically rely on supervised training with large labeled
datasets. Labeling medical images requires significant expertise and time, and
typical hand-tuned approaches for data augmentation fail to capture the complex
variations in such images.
  We present an automated data augmentation method for synthesizing labeled
medical images. We demonstrate our method on the task of segmenting magnetic
resonance imaging (MRI) brain scans. Our method requires only a single
segmented scan, and leverages other unlabeled scans in a semi-supervised
approach. We learn a model of transformations from the images, and use the
model along with the labeled example to synthesize additional labeled examples.
Each transformation is comprised of a spatial deformation field and an
intensity change, enabling the synthesis of complex effects such as variations
in anatomy and image acquisition procedures. We show that training a supervised
segmenter with these new examples provides significant improvements over
state-of-the-art methods for one-shot biomedical image segmentation. Our code
is available at https://github.com/xamyzhao/brainstorm.",['cs.CV']
CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions,"Referring object detection and referring image segmentation are important
tasks that require joint understanding of visual information and natural
language. Yet there has been evidence that current benchmark datasets suffer
from bias, and current state-of-the-art models cannot be easily evaluated on
their intermediate reasoning process. To address these issues and complement
similar efforts in visual question answering, we build CLEVR-Ref+, a synthetic
diagnostic dataset for referring expression comprehension. The precise
locations and attributes of the objects are readily available, and the
referring expressions are automatically associated with functional programs.
The synthetic nature allows control over dataset bias (through sampling
strategy), and the modular programs enable intermediate reasoning ground truth
without human annotators.
  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we
also propose IEP-Ref, a module network approach that significantly outperforms
other models on our dataset. In particular, we present two interesting and
important findings using IEP-Ref: (1) the module trained to transform feature
maps into segmentation masks can be attached to any intermediate module to
reveal the entire reasoning process step-by-step; (2) even if all training data
has at least one object referred, IEP-Ref can correctly predict no-foreground
when presented with false-premise referring expressions. To the best of our
knowledge, this is the first direct and quantitative proof that neural modules
behave in the way they are intended.","['cs.CV', 'cs.CL', 'cs.LG']"
Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation,"Recently, Neural Architecture Search (NAS) has successfully identified neural
network architectures that exceed human designed ones on large-scale image
classification. In this paper, we study NAS for semantic image segmentation.
Existing works often focus on searching the repeatable cell structure, while
hand-designing the outer network structure that controls the spatial resolution
changes. This choice simplifies the search space, but becomes increasingly
problematic for dense image prediction which exhibits a lot more network level
architectural variations. Therefore, we propose to search the network level
structure in addition to the cell level structure, which forms a hierarchical
architecture search space. We present a network level search space that
includes many popular designs, and develop a formulation that allows efficient
gradient-based architecture search (3 P100 GPU days on Cityscapes images). We
demonstrate the effectiveness of the proposed method on the challenging
Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our
architecture searched specifically for semantic image segmentation, attains
state-of-the-art performance without any ImageNet pretraining.","['cs.CV', 'cs.LG']"
Deep Convolutional Encoder-Decoders with Aggregated Multi-Resolution Skip Connections for Skin Lesion Segmentation,"The prevalence of skin melanoma is rapidly increasing as well as the recorded
death cases of its patients. Automatic image segmentation tools play an
important role in providing standardized computer-assisted analysis for skin
melanoma patients. Current state-of-the-art segmentation methods are based on
fully convolutional neural networks, which utilize an encoder-decoder approach.
However, these methods produce coarse segmentation masks due to the loss of
location information during the encoding layers. Inspired by Pyramid Scene
Parsing Network (PSP-Net), we propose an encoder-decoder model that utilizes
pyramid pooling modules in the deep skip connections which aggregate the global
context and compensate for the lost spatial information. We trained and
validated our approach using ISIC 2018: Skin Lesion Analysis Towards Melanoma
Detection grand challenge dataset. Our approach showed a validation accuracy
with a Jaccard index of 0.837, which outperforms U-Net. We believe that with
this reported reliable accuracy, this method can be introduced for clinical
practice.",['cs.CV']
Few-shot brain segmentation from weakly labeled data with deep heteroscedastic multi-task networks,"In applications of supervised learning applied to medical image segmentation,
the need for large amounts of labeled data typically goes unquestioned. In
particular, in the case of brain anatomy segmentation, hundreds or thousands of
weakly-labeled volumes are often used as training data. In this paper, we first
observe that for many brain structures, a small number of training examples,
(n=9), weakly labeled using Freesurfer 6.0, plus simple data augmentation,
suffice as training data to achieve high performance, achieving an overall mean
Dice coefficient of $0.84 \pm 0.12$ compared to Freesurfer over 28 brain
structures in T1-weighted images of $\approx 4000$ 9-10 year-olds from the
Adolescent Brain Cognitive Development study. We then examine two varieties of
heteroscedastic network as a method for improving classification results. An
existing proposal by Kendall and Gal, which uses Monte-Carlo inference to learn
to predict the variance of each prediction, yields an overall mean Dice of
$0.85 \pm 0.14$ and showed statistically significant improvements over 25 brain
structures. Meanwhile a novel heteroscedastic network which directly learns the
probability that an example has been mislabeled yielded an overall mean Dice of
$0.87 \pm 0.11$ and showed statistically significant improvements over all but
one of the brain structures considered. The loss function associated to this
network can be interpreted as performing a form of learned label smoothing,
where labels are only smoothed where they are judged to be uncertain.","['cs.LG', 'eess.IV', 'stat.ML']"
Geometry in Active Learning for Binary and Multi-class Image Segmentation,"We propose an active learning approach to image segmentation that exploits
geometric priors to speed up and streamline the annotation process. It can be
applied for both background-foreground and multi-class segmentation tasks in 2D
images and 3D image volumes. Our approach combines geometric smoothness priors
in the image space with more traditional uncertainty measures to estimate which
pixels or voxels are the most informative, and thus should to be annotated
next. For multi-class settings, we additionally introduce two novel criteria
for uncertainty. In the 3D case, we use the resulting uncertainty measure to
select voxels lying on a planar patch, which makes batch annotation much more
convenient for the end user compared to the setting where voxels are randomly
distributed in a volume. The planar patch is found using a branch-and-bound
algorithm that looks for a 2D patch in a 3D volume where the most informative
instances are located. We evaluate our approach on Electron Microscopy and
Magnetic Resonance image volumes, as well as on regular images of horses and
faces. We demonstrate a substantial performance increase over other approaches
thanks to the use of geometric priors.",['cs.CV']
Vehicle Image Generation Going Well with The Surroundings,"Since the generative neural networks have made a breakthrough in the image
generation problem, lots of researches on their applications have been studied
such as image restoration, style transfer and image completion. However, there
has been few research generating objects in uncontrolled real-world
environments. In this paper, we propose a novel approach for vehicle image
generation in real-world scenes. Using a subnetwork based on a precedent work
of image completion, our model makes the shape of an object. Details of objects
are trained by an additional colorization and refinement subnetwork, resulting
in a better quality of generated objects. Unlike many other works, our method
does not require any segmentation layout but still makes a plausible vehicle in
the image. We evaluate our method by using images from Berkeley Deep Drive
(BDD) and Cityscape datasets, which are widely used for object detection and
image segmentation problems. The adequacy of the generated images by the
proposed method has also been evaluated using a widely utilized object
detection algorithm and the FID score.",['cs.CV']
Graph Cut Segmentation Methods Revisited with a Quantum Algorithm,"The design and performance of computer vision algorithms are greatly
influenced by the hardware on which they are implemented. CPUs, multi-core
CPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to
be realized. This is notably the case with GPUs, which has significantly
changed the landscape of computer vision research through deep learning. As the
end of Moores law approaches, researchers and hardware manufacturers are
exploring alternative hardware computing paradigms. Quantum computers are a
very promising alternative and offer polynomial or even exponential speed-ups
over conventional computing for some problems. This paper presents a novel
approach to image segmentation that uses new quantum computing hardware.
Segmentation is formulated as a graph cut problem that can be mapped to the
quantum approximate optimization algorithm (QAOA). This algorithm can be
implemented on current and near-term quantum computers. Encouraging results are
presented on artificial and medical imaging data. This represents an important,
practical step towards leveraging quantum computers for computer vision.",['cs.CV']
Feature Fusion Encoder Decoder Network For Automatic Liver Lesion Segmentation,"Liver lesion segmentation is a difficult yet critical task for medical image
analysis. Recently, deep learning based image segmentation methods have
achieved promising performance, which can be divided into three categories: 2D,
2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3D
methods can have very high complexity and 2D methods may not perform
satisfactorily. To obtain competitive performance with low complexity, in this
paper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2D
segmentation model to tackle the challenging problem of liver lesion
segmentation from CT images. Our feature fusion method is based on the
attention mechanism, which fuses high-level features carrying semantic
information with low-level features having image details. Additionally, to
compensate for the information loss during the upsampling process, a dense
upsampling convolution and a residual convolutional structure are proposed. We
tested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS)
Challenge and achieved competitive results compared with other state-of-the-art
methods.","['cs.CV', 'cs.LG']"
Explainable cardiac pathology classification on cine MRI with motion characterization by semi-supervised learning of apparent flow,"We propose a method to classify cardiac pathology based on a novel approach
to extract image derived features to characterize the shape and motion of the
heart. An original semi-supervised learning procedure, which makes efficient
use of a large amount of non-segmented images and a small amount of images
segmented manually by experts, is developed to generate pixel-wise apparent
flow between two time points of a 2D+t cine MRI image sequence. Combining the
apparent flow maps and cardiac segmentation masks, we obtain a local apparent
flow corresponding to the 2D motion of myocardium and ventricular cavities.
This leads to the generation of time series of the radius and thickness of
myocardial segments to represent cardiac motion. These time series of motion
features are reliable and explainable characteristics of pathological cardiac
motion. Furthermore, they are combined with shape-related features to classify
cardiac pathologies. Using only nine feature values as input, we propose an
explainable, simple and flexible model for pathology classification. On ACDC
training set and testing set, the model achieves 95% and 94% respectively as
classification accuracy. Its performance is hence comparable to that of the
state-of-the-art. Comparison with various other models is performed to outline
some advantages of our model.","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']"
CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy,"This paper presents a new approach for relatively accurate brain region of
interest (ROI) detection from dynamic susceptibility contrast (DSC) perfusion
magnetic resonance (MR) images of a human head with abnormal brain anatomy.
Such images produce problems for automatic brain segmentation algorithms, and
as a result, poor perfusion ROI detection affects both quantitative
measurements and visual assessment of perfusion data. In the proposed approach
image segmentation is based on CUSUM filter usage that was adapted to be
applicable to process DSC perfusion MR images. The result of segmentation is a
binary mask of brain ROI that is generated via usage of brain boundary
location. Each point of the boundary between the brain and surrounding tissues
is detected as a change-point by CUSUM filter. Proposed adopted CUSUM filter
operates by accumulating the deviations between the observed and expected
intensities of image points at the time of moving on a trajectory. Motion
trajectory is created by the iterative change of movement direction inside the
background region in order to reach brain region, and vice versa after boundary
crossing. Proposed segmentation approach was evaluated with Dice index
comparing obtained results to the reference standard. Manually marked brain
region pixels (reference standard), as well as visual inspection of detected
with CUSUM filter usage brain ROI, were provided by experienced radiologists.
The results showed that proposed approach is suitable to be used for brain ROI
detection from DSC perfusion MR images of a human head with abnormal brain
anatomy and can, therefore, be applied in the DSC perfusion data analysis.",['cs.CV']
Scene Understanding for Autonomous Manipulation with Deep Learning,"Over the past few years, deep learning techniques have achieved tremendous
success in many visual understanding tasks such as object detection, image
segmentation, and caption generation. Despite this thriving in computer vision
and natural language processing, deep learning has not yet shown significant
impact in robotics. Due to the gap between theory and application, there are
many challenges when applying the results of deep learning to the real robotic
systems. In this study, our long-term goal is to bridge the gap between
computer vision and robotics by developing visual methods that can be used in
real robots. In particular, this work tackles two fundamental visual problems
for autonomous robotic manipulation: affordance detection and fine-grained
action understanding. Theoretically, we propose different deep architectures to
further improves the state of the art in each problem. Empirically, we show
that the outcomes of our proposed methods can be applied in real robots and
allow them to perform useful manipulation tasks.","['cs.CV', 'cs.RO']"
MobileNetV2: Inverted Residuals and Linear Bottlenecks,"In this paper we describe a new mobile architecture, MobileNetV2, that
improves the state of the art performance of mobile models on multiple tasks
and benchmarks as well as across a spectrum of different model sizes. We also
describe efficient ways of applying these mobile models to object detection in
a novel framework we call SSDLite. Additionally, we demonstrate how to build
mobile semantic segmentation models through a reduced form of DeepLabv3 which
we call Mobile DeepLabv3.
  The MobileNetV2 architecture is based on an inverted residual structure where
the input and output of the residual block are thin bottleneck layers opposite
to traditional residual models which use expanded representations in the input
an MobileNetV2 uses lightweight depthwise convolutions to filter features in
the intermediate expansion layer. Additionally, we find that it is important to
remove non-linearities in the narrow layers in order to maintain
representational power. We demonstrate that this improves performance and
provide an intuition that led to this design. Finally, our approach allows
decoupling of the input/output domains from the expressiveness of the
transformation, which provides a convenient framework for further analysis. We
measure our performance on Imagenet classification, COCO object detection, VOC
image segmentation. We evaluate the trade-offs between accuracy, and number of
operations measured by multiply-adds (MAdd), as well as the number of
parameters",['cs.CV']
Learning Disentangled Representations of Satellite Image Time Series,"In this paper, we investigate how to learn a suitable representation of
satellite image time series in an unsupervised manner by leveraging large
amounts of unlabeled data. Additionally , we aim to disentangle the
representation of time series into two representations: a shared representation
that captures the common information between the images of a time series and an
exclusive representation that contains the specific information of each image
of the time series. To address these issues, we propose a model that combines a
novel component called cross-domain autoencoders with the variational
autoencoder (VAE) and generative ad-versarial network (GAN) methods. In order
to learn disentangled representations of time series, our model learns the
multimodal image-to-image translation task. We train our model using satellite
image time series from the Sentinel-2 mission. Several experiments are carried
out to evaluate the obtained representations. We show that these disentangled
representations can be very useful to perform multiple tasks such as image
classification, image retrieval, image segmentation and change detection.","['cs.CV', 'cs.LG']"
Interactive segmentation of medical images through fully convolutional neural networks,"Image segmentation plays an essential role in medicine for both diagnostic
and interventional tasks. Segmentation approaches are either manual,
semi-automated or fully-automated. Manual segmentation offers full control over
the quality of the results, but is tedious, time consuming and prone to
operator bias. Fully automated methods require no human effort, but often
deliver sub-optimal results without providing users with the means to make
corrections. Semi-automated approaches keep users in control of the results by
providing means for interaction, but the main challenge is to offer a good
trade-off between precision and required interaction. In this paper we present
a deep learning (DL) based semi-automated segmentation approach that aims to be
a ""smart"" interactive tool for region of interest delineation in medical
images. We demonstrate its use for segmenting multiple organs on computed
tomography (CT) of the abdomen. Our approach solves some of the most pressing
clinical challenges: (i) it requires only one to a few user clicks to deliver
excellent 2D segmentations in a fast and reliable fashion; (ii) it can
generalize to previously unseen structures and ""corner cases""; (iii) it
delivers results that can be corrected quickly in a smart and intuitive way up
to an arbitrary degree of precision chosen by the user and (iv) ensures high
accuracy. We present our approach and compare it to other techniques and
previous work to show the advantages brought by our method.",['cs.CV']
Efficient Smoothing of Dilated Convolutions for Image Segmentation,"Dilated Convolutions have been shown to be highly useful for the task of
image segmentation. By introducing gaps into convolutional filters, they enable
the use of larger receptive fields without increasing the original kernel size.
Even though this allows for the inexpensive capturing of features at different
scales, the structure of the dilated convolutional filter leads to a loss of
information. We hypothesise that inexpensive modifications to Dilated
Convolutional Neural Networks, such as additional averaging layers, could
overcome this limitation. In this project we test this hypothesis by evaluating
the effect of these modifications for a state-of-the art image segmentation
system and compare them to existing approaches with the same objective. Our
experiments show that our proposed methods improve the performance of dilated
convolutions for image segmentation. Crucially, our modifications achieve these
results at a much lower computational cost than previous smoothing approaches.",['cs.CV']
Segmentation of Roots in Soil with U-Net,"Plant root research can provide a way to attain stress-tolerant crops that
produce greater yield in a diverse array of conditions. Phenotyping roots in
soil is often challenging due to the roots being difficult to access and the
use of time consuming manual methods. Rhizotrons allow visual inspection of
root growth through transparent surfaces. Agronomists currently manually label
photographs of roots obtained from rhizotrons using a line-intersect method to
obtain root length density and rooting depth measurements which are essential
for their experiments. We investigate the effectiveness of an automated image
segmentation method based on the U-Net Convolutional Neural Network (CNN)
architecture to enable such measurements. We design a data-set of 50 annotated
Chicory (Cichorium intybus L.) root images which we use to train, validate and
test the system and compare against a baseline built using the Frangi
vesselness filter. We obtain metrics using manual annotations and
line-intersect counts. Our results on the held out data show our proposed
automated segmentation system to be a viable solution for detecting and
quantifying roots. We evaluate our system using 867 images for which we have
obtained line-intersect counts, attaining a Spearman rank correlation of 0.9748
and an $r^2$ of 0.9217. We also achieve an $F_1$ of 0.7 when comparing the
automated segmentation to the manual annotations, with our automated
segmentation system producing segmentations with higher quality than the manual
annotations for large portions of the image.",['cs.CV']
SuperPatchMatch: an Algorithm for Robust Correspondences using Superpixel Patches,"Superpixels have become very popular in many computer vision applications.
Nevertheless, they remain underexploited since the superpixel decomposition may
produce irregular and non stable segmentation results due to the dependency to
the image content. In this paper, we first introduce a novel structure, a
superpixel-based patch, called SuperPatch. The proposed structure, based on
superpixel neighborhood, leads to a robust descriptor since spatial information
is naturally included. The generalization of the PatchMatch method to
SuperPatches, named SuperPatchMatch, is introduced. Finally, we propose a
framework to perform fast segmentation and labeling from an image database, and
demonstrate the potential of our approach since we outperform, in terms of
computational cost and accuracy, the results of state-of-the-art methods on
both face labeling and medical image segmentation.",['cs.CV']
Multi-label Cloud Segmentation Using a Deep Network,"Different empirical models have been developed for cloud detection. There is
a growing interest in using the ground-based sky/cloud images for this purpose.
Several methods exist that perform binary segmentation of clouds. In this
paper, we propose to use a deep learning architecture (U-Net) to perform
multi-label sky/cloud image segmentation. The proposed approach outperforms
recent literature by a large margin.",['cs.CV']
Optimal Transport for Multi-source Domain Adaptation under Target Shift,"In this paper, we propose to tackle the problem of reducing discrepancies
between multiple domains referred to as multi-source domain adaptation and
consider it under the target shift assumption: in all domains we aim to solve a
classification problem with the same output classes, but with labels'
proportions differing across them. This problem, generally ignored in the vast
majority papers on domain adaptation papers, is nevertheless critical in
real-world applications, and we theoretically show its impact on the adaptation
success. To address this issue, we design a method based on optimal transport,
a theory that has been successfully used to tackle adaptation problems in
machine learning. Our method performs multi-source adaptation and target shift
correction simultaneously by learning the class probabilities of the unlabeled
target sample and the coupling allowing to align two (or more) probability
distributions. Experiments on both synthetic and real-world data related to
satellite image segmentation task show the superiority of the proposed method
over the state-of-the-art.",['stat.ML']
FreeLabel: A Publicly Available Annotation Tool based on Freehand Traces,"Large-scale annotation of image segmentation datasets is often prohibitively
expensive, as it usually requires a huge number of worker hours to obtain
high-quality results. Abundant and reliable data has been, however, crucial for
the advances on image understanding tasks achieved by deep learning models. In
this paper, we introduce FreeLabel, an intuitive open-source web interface that
allows users to obtain high-quality segmentation masks with just a few freehand
scribbles, in a matter of seconds. The efficacy of FreeLabel is quantitatively
demonstrated by experimental results on the PASCAL dataset as well as on a
dataset from the agricultural domain. Designed to benefit the computer vision
community, FreeLabel can be used for both crowdsourced or private annotation
and has a modular structure that can be easily adapted for any image dataset.",['cs.CV']
Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative,"A fully automated knee MRI segmentation method to study osteoarthritis (OA)
was developed using a novel hierarchical set of random forests (RF) classifiers
to learn the appearance of cartilage regions and their boundaries. A
neighborhood approximation forest is used first to provide contextual feature
to the second-level RF classifier that also considers local features and
produces location-specific costs for the layered optimal graph image
segmentation of multiple objects and surfaces (LOGISMOS) framework. Double echo
steady state (DESS) MRIs used in this work originated from the Osteoarthritis
Initiative (OAI) study. Trained on 34 MRIs with varying degrees of OA, the
performance of the learning-based method tested on 108 MRIs showed a
significant reduction in segmentation errors (\emph{p}$<$0.05) compared with
the conventional gradient-based and single-stage RF-learned costs. The 3D
LOGISMOS was extended to longitudinal-3D (4D) to simultaneously segment
multiple follow-up visits of the same patient. As such, data from all
time-points of the temporal sequence contribute information to a single optimal
solution that utilizes both spatial 3D and temporal contexts. 4D LOGISMOS
validation on 108 MRIs from baseline and 12 month follow-up scans of 54
patients showed a significant reduction in segmentation errors
(\emph{p}$<$0.01) compared to 3D. Finally, the potential of 4D LOGISMOS was
further explored on the same 54 patients using 5 annual follow-up scans
demonstrating a significant improvement of measuring cartilage thickness
(\emph{p}$<$0.01) compared to the sequential 3D approach.","['cs.CV', 'cs.LG']"
Multi-stream 3D FCN with Multi-scale Deep Supervision for Multi-modality Isointense Infant Brain MR Image Segmentation,"We present a method to address the challenging problem of segmentation of
multi-modality isointense infant brain MR images into white matter (WM), gray
matter (GM), and cerebrospinal fluid (CSF). Our method is based on
context-guided, multi-stream fully convolutional networks (FCN), which after
training, can directly map a whole volumetric data to its volume-wise labels.
In order to alleviate the poten-tial gradient vanishing problem during
training, we designed multi-scale deep supervision. Furthermore, context
infor-mation was used to further improve the performance of our method.
Validated on the test data of the MICCAI 2017 Grand Challenge on 6-month infant
brain MRI segmentation (iSeg-2017), our method achieved an average Dice Overlap
Coefficient of 95.4%, 91.6% and 89.6% for CSF, GM and WM, respectively.",['cs.CV']
Nonlinear Markov Random Fields Learned via Backpropagation,"Although convolutional neural networks (CNNs) currently dominate competitions
on image segmentation, for neuroimaging analysis tasks, more classical
generative approaches based on mixture models are still used in practice to
parcellate brains. To bridge the gap between the two, in this paper we propose
a marriage between a probabilistic generative model, which has been shown to be
robust to variability among magnetic resonance (MR) images acquired via
different imaging protocols, and a CNN. The link is in the prior distribution
over the unknown tissue classes, which are classically modelled using a Markov
random field. In this work we model the interactions among neighbouring pixels
by a type of recurrent CNN, which can encode more complex spatial interactions.
We validate our proposed model on publicly available MR data, from different
centres, and show that it generalises across imaging protocols. This result
demonstrates a successful and principled inclusion of a CNN in a generative
model, which in turn could be adapted by any probabilistic generative approach
for image segmentation.","['cs.CV', 'cs.LG', 'stat.ML']"
Exploit fully automatic low-level segmented PET data for training high-level deep learning algorithms for the corresponding CT data,"We present an approach for fully automatic urinary bladder segmentation in CT
images with artificial neural networks in this study. Automatic medical image
analysis has become an invaluable tool in the different treatment stages of
diseases. Especially medical image segmentation plays a vital role, since
segmentation is often the initial step in an image analysis pipeline. Since
deep neural networks have made a large impact on the field of image processing
in the past years, we use two different deep learning architectures to segment
the urinary bladder. Both of these architectures are based on pre-trained
classification networks that are adapted to perform semantic segmentation.
Since deep neural networks require a large amount of training data,
specifically images and corresponding ground truth labels, we furthermore
propose a method to generate such a suitable training data set from Positron
Emission Tomography/Computed Tomography image data. This is done by applying
thresholding to the Positron Emission Tomography data for obtaining a ground
truth and by utilizing data augmentation to enlarge the dataset. In this study,
we discuss the influence of data augmentation on the segmentation results, and
compare and evaluate the proposed architectures in terms of qualitative and
quantitative segmentation performance. The results presented in this study
allow concluding that deep neural networks can be considered a promising
approach to segment the urinary bladder in CT images.",['cs.CV']
CE-Net: Context Encoder Network for 2D Medical Image Segmentation,"Medical image segmentation is an important step in medical image analysis.
With the rapid development of convolutional neural network in image processing,
deep learning has been used for medical image segmentation, such as optic disc
segmentation, blood vessel detection, lung segmentation, cell segmentation,
etc. Previously, U-net based approaches have been proposed. However, the
consecutive pooling and strided convolutional operations lead to the loss of
some spatial information. In this paper, we propose a context encoder network
(referred to as CE-Net) to capture more high-level information and preserve
spatial information for 2D medical image segmentation. CE-Net mainly contains
three major components: a feature encoder module, a context extractor and a
feature decoder module. We use pretrained ResNet block as the fixed feature
extractor. The context extractor module is formed by a newly proposed dense
atrous convolution (DAC) block and residual multi-kernel pooling (RMP) block.
We applied the proposed CE-Net to different 2D medical image segmentation
tasks. Comprehensive results show that the proposed method outperforms the
original U-Net method and other state-of-the-art methods for optic disc
segmentation, vessel detection, lung segmentation, cell contour segmentation
and retinal optical coherence tomography layer segmentation.",['cs.CV']
Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation,"Difficult image segmentation problems, for instance left atrium MRI, can be
addressed by incorporating shape priors to find solutions that are consistent
with known objects. Nonetheless, a single multivariate Gaussian is not an
adequate model in cases with significant nonlinear shape variation or where the
prior distribution is multimodal. Nonparametric density estimation is more
general, but has a ravenous appetite for training samples and poses serious
challenges in optimization, especially in high dimensional spaces. Here, we
propose a maximum-a-posteriori formulation that relies on a generative image
model by incorporating both local intensity and global shape priors. We use
deep autoencoders to capture the complex intensity distribution while avoiding
the careful selection of hand-crafted features. We formulate the shape prior as
a mixture of Gaussians and learn the corresponding parameters in a
high-dimensional shape space rather than pre-projecting onto a low-dimensional
subspace. In segmentation, we treat the identity of the mixture component as a
latent variable and marginalize it within a generalized
expectation-maximization framework. We present a conditional maximization-based
scheme that alternates between a closed-form solution for component-specific
shape parameters that provides a global update-based optimization strategy, and
an intensity-based energy minimization that translates the global notion of a
nonlinear shape prior into a set of local penalties. We demonstrate our
approach on the left atrial segmentation from gadolinium-enhanced MRI, which is
useful in quantifying the atrial geometry in patients with atrial fibrillation.","['cs.CV', 'cs.CG', 'cs.LG', 'stat.ML']"
The Method of Multimodal MRI Brain Image Segmentation Based on Differential Geometric Features,"Accurate segmentation of brain tissue in magnetic resonance images (MRI) is a
diffcult task due to different types of brain abnormalities. Using information
and features from multimodal MRI including T1, T1-weighted inversion recovery
(T1-IR) and T2-FLAIR and differential geometric features including the Jacobian
determinant(JD) and the curl vector(CV) derived from T1 modality can result in
a more accurate analysis of brain images. In this paper, we use the
differential geometric information including JD and CV as image characteristics
to measure the differences between different MRI images, which represent local
size changes and local rotations of the brain image, and we can use them as one
CNN channel with other three modalities (T1-weighted, T1-IR and T2-FLAIR) to
get more accurate results of brain segmentation. We test this method on two
datasets including IBSR dataset and MRBrainS datasets based on the deep
voxelwise residual network, namely VoxResNet, and obtain excellent improvement
over single modality or three modalities and increases average
DSC(Cerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM)) by about
1.5% on the well-known MRBrainS18 dataset and about 2.5% on the IBSR dataset.
Moreover, we discuss that one modality combined with its JD or CV information
can replace the segmentation effect of three modalities, which can provide
medical conveniences for doctor to diagnose because only to extract T1-modality
MRI image of patients. Finally, we also compare the segmentation performance of
our method in two networks, VoxResNet and U-Net network. The results show
VoxResNet has a better performance than U-Net network with our method in brain
MRI segmentation. We believe the proposed method can advance the performance in
brain segmentation and clinical diagnosis.",['cs.CV']
Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated Temporal Fully-Convolutional Networks,"Human motion capture data has been widely used in data-driven character
animation. In order to generate realistic, natural-looking motions, most
data-driven approaches require considerable efforts of pre-processing,
including motion segmentation and annotation. Existing (semi-) automatic
solutions either require hand-crafted features for motion segmentation or do
not produce the semantic annotations required for motion synthesis and building
large-scale motion databases. In addition, human labeled annotation data
suffers from inter- and intra-labeler inconsistencies by design. We propose a
semi-automatic framework for semantic segmentation of motion capture data based
on supervised machine learning techniques. It first transforms a motion capture
sequence into a ``motion image'' and applies a convolutional neural network for
image segmentation. Dilated temporal convolutions enable the extraction of
temporal information from a large receptive field. Our model outperforms two
state-of-the-art models for action segmentation, as well as a popular network
for sequence modeling. Most of all, our method is very robust under noisy and
inaccurate training labels and thus can handle human errors during the labeling
process.","['cs.CV', 'cs.GR', 'cs.LG']"
FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference,"The main obstacle to weakly supervised semantic image segmentation is the
difficulty of obtaining pixel-level information from coarse image-level
annotations. Most methods based on image-level annotations use localization
maps obtained from the classifier, but these only focus on the small
discriminative parts of objects and do not capture precise boundaries.
FickleNet explores diverse combinations of locations on feature maps created by
generic deep neural networks. It selects hidden units randomly and then uses
them to obtain activation scores for image classification. FickleNet implicitly
learns the coherence of each location in the feature maps, resulting in a
localization map which identifies both discriminative and other parts of
objects. The ensemble effects are obtained from a single network by selecting
random hidden unit pairs, which means that a variety of localization maps are
generated from a single image. Our approach does not require any additional
training steps and only adds a simple layer to a standard convolutional neural
network; nevertheless it outperforms recent comparable techniques on the Pascal
VOC 2012 benchmark in both weakly and semi-supervised settings.",['cs.CV']
HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation,"Recently, dense connections have attracted substantial attention in computer
vision because they facilitate gradient flow and implicit deep supervision
during training. Particularly, DenseNet, which connects each layer to every
other layer in a feed-forward fashion, has shown impressive performances in
natural image classification tasks. We propose HyperDenseNet, a 3D fully
convolutional neural network that extends the definition of dense connectivity
to multi-modal segmentation problems. Each imaging modality has a path, and
dense connections occur not only between the pairs of layers within the same
path, but also between those across different paths. This contrasts with the
existing multi-modal CNN approaches, in which modeling several modalities
relies entirely on a single joint layer (or level of abstraction) for fusion,
typically either at the input or at the output of the network. Therefore, the
proposed network has total freedom to learn more complex combinations between
the modalities, within and in-between all the levels of abstraction, which
increases significantly the learning representation. We report extensive
evaluations over two different and highly competitive multi-modal brain tissue
segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing
on 6-month infant data and the latter on adult images. HyperDenseNet yielded
significant improvements over many state-of-the-art segmentation networks,
ranking at the top on both benchmarks. We further provide a comprehensive
experimental analysis of features re-use, which confirms the importance of
hyper-dense connections in multi-modal representation learning. Our code is
publicly available at https://www.github.com/josedolz/HyperDenseNet.",['cs.CV']
SPDA: Superpixel-based Data Augmentation for Biomedical Image Segmentation,"Supervised training a deep neural network aims to ""teach"" the network to
mimic human visual perception that is represented by image-and-label pairs in
the training data. Superpixelized (SP) images are visually perceivable to
humans, but a conventionally trained deep learning model often performs poorly
when working on SP images. To better mimic human visual perception, we think it
is desirable for the deep learning model to be able to perceive not only raw
images but also SP images. In this paper, we propose a new superpixel-based
data augmentation (SPDA) method for training deep learning models for
biomedical image segmentation. Our method applies a superpixel generation
scheme to all the original training images to generate superpixelized images.
The SP images thus obtained are then jointly used with the original training
images to train a deep learning model. Our experiments of SPDA on four
biomedical image datasets show that SPDA is effective and can consistently
improve the performance of state-of-the-art fully convolutional networks for
biomedical image segmentation in 2D and 3D images. Additional studies also
demonstrate that SPDA can practically reduce the generalization gap.","['cs.CV', 'cs.AI', 'cs.LG']"
Salient object detection on hyperspectral images using features learned from unsupervised segmentation task,"Various saliency detection algorithms from color images have been proposed to
mimic eye fixation or attentive object detection response of human observers
for the same scenes. However, developments on hyperspectral imaging systems
enable us to obtain redundant spectral information of the observed scenes from
the reflected light source from objects. A few studies using low-level features
on hyperspectral images demonstrated that salient object detection can be
achieved. In this work, we proposed a salient object detection model on
hyperspectral images by applying manifold ranking (MR) on self-supervised
Convolutional Neural Network (CNN) features (high-level features) from
unsupervised image segmentation task. Self-supervision of CNN continues until
clustering loss or saliency maps converges to a defined error between each
iteration. Finally, saliency estimations is done as the saliency map at last
iteration when the self-supervision procedure terminates with convergence.
Experimental evaluations demonstrated that proposed saliency detection
algorithm on hyperspectral images is outperforming state-of-the-arts
hyperspectral saliency models including the original MR based saliency model.",['cs.CV']
"Class-independent sequential full image segmentation, using a convolutional net that finds a segment within an attention region, given a pointer pixel within this segment","This work examines the use of a fully convolutional net (FCN) to find an
image segment, given a pixel within this segment region. The net receives an
image, a point in the image and a region of interest (RoI ) mask. The net
output is a binary mask of the segment in which the point is located. The
region where the segment can be found is contained within the input RoI mask.
Full image segmentation can be achieved by running this net sequentially,
region-by-region on the image, and stitching the output segments into a single
segmentation map. This simple method addresses two major challenges of image
segmentation: 1) Segmentation of unknown categories that were not included in
the training set. 2) Segmentation of both individual object instances (things)
and non-objects (stuff), such as sky and vegetation. Hence, if the pointer
pixel is located within a person in a group, the net will output a mask that
covers that individual person; if the pointer point is located within the sky
region, the net returns the region of the sky in the image. This is true even
if no example for sky or person appeared in the training set. The net was
tested and trained on the COCO panoptic dataset and achieved 67% IOU for
segmentation of familiar classes (that were part of the net training set) and
53% IOU for segmentation of unfamiliar classes (that were not included in the
training).",['cs.CV']
Dense 3D Visual Mapping via Semantic Simplification,"Dense 3D visual mapping estimates as many as possible pixel depths, for each
image. This results in very dense point clouds that often contain redundant and
noisy information, especially for surfaces that are roughly planar, for
instance, the ground or the walls in the scene. In this paper we leverage on
semantic image segmentation to discriminate which regions of the scene require
simplification and which should be kept at high level of details. We propose
four different point cloud simplification methods which decimate the perceived
point cloud by relying on class-specific local and global statistics still
maintaining more points in the proximity of class boundaries to preserve the
infra-class edges and discontinuities. 3D dense model is obtained by fusing the
point clouds in a 3D Delaunay Triangulation to deal with variable point cloud
density. In the experimental evaluation we have shown that, by leveraging on
semantics, it is possible to simplify the model and diminish the noise
affecting the point clouds.",['cs.CV']
3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation,"Segmentation of colorectal cancerous regions from 3D Magnetic Resonance (MR)
images is a crucial procedure for radiotherapy which conventionally requires
accurate delineation of tumour boundaries at an expense of labor, time and
reproducibility. While deep learning based methods serve good baselines in 3D
image segmentation tasks, small applicable patch size limits effective
receptive field and degrades segmentation performance. In addition, Regions of
interest (RoIs) localization from large whole volume 3D images serves as a
preceding operation that brings about multiple benefits in terms of speed,
target completeness, reduction of false positives. Distinct from sliding window
or non-joint localization-segmentation based models, we propose a novel
multitask framework referred to as 3D RoI-aware U-Net (3D RU-Net), for RoI
localization and in-region segmentation where the two tasks share one backbone
encoder network. With the region proposals from the encoder, we crop
multi-level RoI in-region features from the encoder to form a GPU
memory-efficient decoder for detailpreserving segmentation and therefore
enlarged applicable volume size and effective receptive field. To effectively
train the model, we designed a Dice formulated loss function for the
global-to-local multi-task learning procedure. Based on the efficiency gains,
we went on to ensemble models with different receptive fields to achieve even
higher performance costing minor extra computational expensiveness. Extensive
experiments were conducted on 64 cancerous cases with a four-fold
cross-validation, and the results showed significant superiority in terms of
accuracy and efficiency over conventional frameworks. In conclusion, the
proposed method has a huge potential for extension to other 3D object
segmentation tasks from medical images due to its inherent generalizability.
The code for the proposed method is publicly available.",['cs.CV']
Optimal Surface Segmentation with Convex Priors in Irregularly Sampled Space,"Optimal surface segmentation is a state-of-the-art method used for
segmentation of multiple globally optimal surfaces in volumetric datasets. The
method is widely used in numerous medical image segmentation applications.
However, nodes in the graph based optimal surface segmentation method typically
encode uniformly distributed orthogonal voxels of the volume. Thus the
segmentation cannot attain an accuracy greater than a single unit voxel, i.e.
the distance between two adjoining nodes in graph space. Segmentation accuracy
higher than a unit voxel is achievable by exploiting partial volume information
in the voxels which shall result in non-equidistant spacing between adjoining
graph nodes. This paper reports a generalized graph based multiple surface
segmentation method with convex priors which can optimally segment the target
surfaces in an irregularly sampled space. The proposed method allows
non-equidistant spacing between the adjoining graph nodes to achieve subvoxel
segmentation accuracy by utilizing the partial volume information in the
voxels. The partial volume information in the voxels is exploited by computing
a displacement field from the original volume data to identify the
subvoxel-accurate centers within each voxel resulting in non-equidistant
spacing between the adjoining graph nodes. The smoothness of each surface
modeled as a convex constraint governs the connectivity and regularity of the
surface. We employ an edge-based graph representation to incorporate the
necessary constraints and the globally optimal solution is obtained by
computing a minimum s-t cut. The proposed method was validated on 10
intravascular multi-frame ultrasound image datasets for subvoxel segmentation
accuracy. In all cases, the approach yielded highly accurate results. Our
approach can be readily extended to higher-dimensional segmentations.",['cs.CV']
Fast-SCNN: Fast Semantic Segmentation Network,"The encoder-decoder framework is state-of-the-art for offline semantic image
segmentation. Since the rise in autonomous systems, real-time computation is
increasingly desirable. In this paper, we introduce fast segmentation
convolutional neural network (Fast-SCNN), an above real-time semantic
segmentation model on high resolution image data (1024x2048px) suited to
efficient computation on embedded devices with low memory. Building on existing
two-branch methods for fast segmentation, we introduce our `learning to
downsample' module which computes low-level features for multiple resolution
branches simultaneously. Our network combines spatial detail at high resolution
with deep features extracted at lower resolution, yielding an accuracy of 68.0%
mean intersection over union at 123.5 frames per second on Cityscapes. We also
show that large scale pre-training is unnecessary. We thoroughly validate our
metric in experiments with ImageNet pre-training and the coarse labeled data of
Cityscapes. Finally, we show even faster computation with competitive results
on subsampled inputs, without any network modifications.",['cs.CV']
Brain MRI Segmentation using Rule-Based Hybrid Approach,"Medical image segmentation being a substantial component of image processing
plays a significant role to analyze gross anatomy, to locate an infirmity and
to plan the surgical procedures. Segmentation of brain Magnetic Resonance
Imaging (MRI) is of considerable importance for the accurate diagnosis.
However, precise and accurate segmentation of brain MRI is a challenging task.
Here, we present an efficient framework for segmentation of brain MR images.
For this purpose, Gabor transform method is used to compute features of brain
MRI. Then, these features are classified by using four different classifiers
i.e., Incremental Supervised Neural Network (ISNN), K-Nearest Neighbor (KNN),
Probabilistic Neural Network (PNN), and Support Vector Machine (SVM).
Performance of these classifiers is investigated over different images of brain
MRI and the variation in the performance of these classifiers is observed for
different brain tissues. Thus, we proposed a rule-based hybrid approach to
segment brain MRI. Experimental results show that the performance of these
classifiers varies over each tissue MRI and the proposed rule-based hybrid
approach exhibits better segmentation of brain MRI tissues.",['cs.CV']
MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation,"In recent years Deep Learning has brought about a breakthrough in Medical
Image Segmentation. U-Net is the most prominent deep network in this regard,
which has been the most popular architecture in the medical imaging community.
Despite outstanding overall performance in segmenting multimodal medical
images, from extensive experimentations on challenging datasets, we found out
that the classical U-Net architecture seems to be lacking in certain aspects.
Therefore, we propose some modifications to improve upon the already
state-of-the-art U-Net model. Hence, following the modifications we develop a
novel architecture MultiResUNet as the potential successor to the successful
U-Net architecture. We have compared our proposed architecture MultiResUNet
with the classical U-Net on a vast repertoire of multimodal medical images.
Albeit slight improvements in the cases of ideal images, a remarkable gain in
performance has been attained for challenging images. We have evaluated our
model on five different datasets, each with their own unique challenges, and
have obtained a relative improvement in performance of 10.15%, 5.07%, 2.63%,
1.41%, and 0.62% respectively.",['cs.CV']
Super-realtime facial landmark detection and shape fitting by deep regression of shape model parameters,"We present a method for highly efficient landmark detection that combines
deep convolutional neural networks with well established model-based fitting
algorithms. Motivated by established model-based fitting methods such as active
shapes, we use a PCA of the landmark positions to allow generative modeling of
facial landmarks. Instead of computing the model parameters using iterative
optimization, the PCA is included in a deep neural network using a novel layer
type. The network predicts model parameters in a single forward pass, thereby
allowing facial landmark detection at several hundreds of frames per second.
Our architecture allows direct end-to-end training of a model-based landmark
detection method and shows that deep neural networks can be used to reliably
predict model parameters directly without the need for an iterative
optimization. The method is evaluated on different datasets for facial landmark
detection and medical image segmentation. PyTorch code is freely available at
https://github.com/justusschock/shapenet","['cs.CV', 'eess.IV']"
FocusNet: An attention-based Fully Convolutional Network for Medical Image Segmentation,"We propose a novel technique to incorporate attention within convolutional
neural networks using feature maps generated by a separate convolutional
autoencoder. Our attention architecture is well suited for incorporation with
deep convolutional networks. We evaluate our model on benchmark segmentation
datasets in skin cancer segmentation and lung lesion segmentation. Results show
highly competitive performance when compared with U-Net and it's residual
variant.",['cs.CV']
Learning to segment with image-level supervision,"Deep convolutional networks have achieved the state-of-the-art for semantic
image segmentation tasks. However, training these networks requires access to
densely labeled images, which are known to be very expensive to obtain. On the
other hand, the web provides an almost unlimited source of images annotated at
the image level. How can one utilize this much larger weakly annotated set for
tasks that require dense labeling? Prior work often relied on localization
cues, such as saliency maps, objectness priors, bounding boxes etc., to address
this challenging problem. In this paper, we propose a model that generates
auxiliary labels for each image, while simultaneously forcing the output of the
CNN to satisfy the mean-field constraints imposed by a conditional random
field. We show that one can enforce the CRF constraints by forcing the
distribution at each pixel to be close to the distribution of its neighbors.
This is in stark contrast with methods that compute a recursive expansion of
the mean-field distribution using a recurrent architecture and train the
resultant distribution. Instead, the proposed model adds an extra loss term to
the output of the CNN, and hence, is faster than recursive implementations. We
achieve the state-of-the-art for weakly supervised semantic image segmentation
on VOC 2012 dataset, assuming no manually labeled pixel level information is
available. Furthermore, the incorporation of conditional random fields in CNN
incurs little extra time during training.",['cs.CV']
Instance Segmentation as Image Segmentation Annotation,"The instance segmentation problem intends to precisely detect and delineate
objects in images. Most of the current solutions rely on deep convolutional
neural networks but despite this fact proposed solutions are very diverse. Some
solutions approach the problem as a network problem, where they use several
networks or specialize a single network to solve several tasks. A different
approach tries to solve the problem as an annotation problem, where the
instance information is encoded in a mathematical representation. This work
proposes a solution based in the DCME technique to solve the instance
segmentation with a single segmentation network. Different from others, the
segmentation network decoder is not specialized in a multi-task network.
Instead, the network encoder is repurposed to classify image objects, reducing
the computational cost of the solution.","['cs.CV', 'cs.LG', 'stat.ML']"
Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images,"In the deep metric learning approach to image segmentation, a convolutional
net densely generates feature vectors at the pixels of an image. Pairs of
feature vectors are trained to be similar or different, depending on whether
the corresponding pixels belong to same or different ground truth segments. To
segment a new image, the feature vectors are computed and clustered. Both
empirically and theoretically, it is unclear whether or when deep metric
learning is superior to the more conventional approach of directly predicting
an affinity graph with a convolutional net. We compare the two approaches using
brain images from serial section electron microscopy images, which constitute
an especially challenging example of instance segmentation. We first show that
seed-based postprocessing of the feature vectors, as originally proposed,
produces inferior accuracy because it is difficult for the convolutional net to
predict feature vectors that remain uniform across large objects. Then we
consider postprocessing by thresholding a nearest neighbor graph followed by
connected components. In this case, segmentations from a ""metric graph"" turn
out to be competitive or even superior to segmentations from a directly
predicted affinity graph. To explain these findings theoretically, we invoke
the property that the metric function satisfies the triangle inequality. Then
we show with an example where this constraint suppresses noise, causing
connected components to more robustly segment a metric graph than an
unconstrained affinity graph.",['cs.CV']
Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks,"Despite the state-of-the-art performance for medical image segmentation, deep
convolutional neural networks (CNNs) have rarely provided uncertainty
estimations regarding their segmentation outputs, e.g., model (epistemic) and
image-based (aleatoric) uncertainties. In this work, we analyze these different
types of uncertainties for CNN-based 2D and 3D medical image segmentation
tasks. We additionally propose a test-time augmentation-based aleatoric
uncertainty to analyze the effect of different transformations of the input
image on the segmentation output. Test-time augmentation has been previously
used to improve segmentation accuracy, yet not been formulated in a consistent
mathematical framework. Hence, we also propose a theoretical formulation of
test-time augmentation, where a distribution of the prediction is estimated by
Monte Carlo simulation with prior distributions of parameters in an image
acquisition model that involves image transformations and noise. We compare and
combine our proposed aleatoric uncertainty with model uncertainty. Experiments
with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic
Resonance Images (MRI) showed that 1) the test-time augmentation-based
aleatoric uncertainty provides a better uncertainty estimation than calculating
the test-time dropout-based model uncertainty alone and helps to reduce
overconfident incorrect predictions, and 2) our test-time augmentation
outperforms a single-prediction baseline and dropout-based multiple
predictions.",['cs.CV']
Explicit topological priors for deep-learning based image segmentation using persistent homology,"We present a novel method to explicitly incorporate topological prior
knowledge into deep learning based segmentation, which is, to our knowledge,
the first work to do so. Our method uses the concept of persistent homology, a
tool from topological data analysis, to capture high-level topological
characteristics of segmentation results in a way which is differentiable with
respect to the pixelwise probability of being assigned to a given class. The
topological prior knowledge consists of the sequence of desired Betti numbers
of the segmentation. As a proof-of-concept we demonstrate our approach by
applying it to the problem of left-ventricle segmentation of cardiac MR images
of 500 subjects from the UK Biobank dataset, where we show that it improves
segmentation performance in terms of topological correctness without
sacrificing pixelwise accuracy.",['cs.CV']
Towards increased trustworthiness of deep learning segmentation methods on cardiac MRI,"Current state-of-the-art deep learning segmentation methods have not yet made
a broad entrance into the clinical setting in spite of high demand for such
automatic methods. One important reason is the lack of reliability caused by
models that fail unnoticed and often locally produce anatomically implausible
results that medical experts would not make. This paper presents an automatic
image segmentation method based on (Bayesian) dilated convolutional networks
(DCNN) that generate segmentation masks and spatial uncertainty maps for the
input image at hand. The method was trained and evaluated using segmentation of
the left ventricle (LV) cavity, right ventricle (RV) endocardium and myocardium
(Myo) at end-diastole (ED) and end-systole (ES) in 100 cardiac 2D MR scans from
the MICCAI 2017 Challenge (ACDC). Combining segmentations and uncertainty maps
and employing a human-in-the-loop setting, we provide evidence that image areas
indicated as highly uncertain regarding the obtained segmentation almost
entirely cover regions of incorrect segmentations. The fused information can be
harnessed to increase segmentation performance. Our results reveal that we can
obtain valuable spatial uncertainty maps with low computational effort using
DCNNs.",['cs.CV']
Automated Quality Control in Image Segmentation: Application to the UK Biobank Cardiac MR Imaging Study,"Background: The trend towards large-scale studies including population
imaging poses new challenges in terms of quality control (QC). This is a
particular issue when automatic processing tools, e.g. image segmentation
methods, are employed to derive quantitative measures or biomarkers for later
analyses. Manual inspection and visual QC of each segmentation isn't feasible
at large scale. However, it's important to be able to automatically detect when
a segmentation method fails so as to avoid inclusion of wrong measurements into
subsequent analyses which could lead to incorrect conclusions. Methods: To
overcome this challenge, we explore an approach for predicting segmentation
quality based on Reverse Classification Accuracy, which enables us to
discriminate between successful and failed segmentations on a per-cases basis.
We validate this approach on a new, large-scale manually-annotated set of 4,800
cardiac magnetic resonance scans. We then apply our method to a large cohort of
7,250 cardiac MRI on which we have performed manual QC. Results: We report
results used for predicting segmentation quality metrics including Dice
Similarity Coefficient (DSC) and surface-distance measures. As initial
validation, we present data for 400 scans demonstrating 99% accuracy for
classifying low and high quality segmentations using predicted DSC scores. As
further validation we show high correlation between real and predicted scores
and 95% classification accuracy on 4,800 scans for which manual segmentations
were available. We mimic real-world application of the method on 7,250 cardiac
MRI where we show good agreement between predicted quality metrics and manual
visual QC scores. Conclusions: We show that RCA has the potential for accurate
and fully automatic segmentation QC on a per-case basis in the context of
large-scale population imaging as in the UK Biobank Imaging Study.",['cs.CV']
Joint shape learning and segmentation for medical images using a minimalistic deep network,"Recently, state-of-the-art results have been achieved in semantic
segmentation using fully convolutional networks (FCNs). Most of these networks
employ encoder-decoder style architecture similar to U-Net and are trained with
images and the corresponding segmentation maps as a pixel-wise classification
task. Such frameworks only exploit class information by using the ground truth
segmentation maps. In this paper, we propose a multi-task learning framework
with the main aim of exploiting structural and spatial information along with
the class information. We modify the decoder part of the FCN to exploit class
information and the structural information as well. We intend to do this while
also keeping the parameters of the network as low as possible. We obtain the
structural information using either of the two ways: i) using the contour map
and ii) using the distance map, both of which can be obtained from ground truth
segmentation maps with no additional annotation costs. We also explore
different ways in which distance maps can be computed and study the effects of
different distance maps on the segmentation performance. We also experiment
extensively on two different medical image segmentation applications: i.e i)
using color fundus images for optic disc and cup segmentation and ii) using
endoscopic images for polyp segmentation. Through our experiments, we report
results comparable to, and in some cases performing better than the current
state-of-the-art architectures and with an order of 2x reduction in the number
of parameters.",['cs.CV']
Semi-Supervised Image-to-Image Translation,"Image-to-image translation is a long-established and a difficult problem in
computer vision. In this paper we propose an adversarial based model for
image-to-image translation. The regular deep neural-network based methods
perform the task of image-to-image translation by comparing gram matrices and
using image segmentation which requires human intervention. Our generative
adversarial network based model works on a conditional probability approach.
This approach makes the image translation independent of any local, global and
content or style features. In our approach we use a bidirectional
reconstruction model appended with the affine transform factor that helps in
conserving the content and photorealism as compared to other models. The
advantage of using such an approach is that the image-to-image translation is
semi-supervised, independant of image segmentation and inherits the properties
of generative adversarial networks tending to produce realistic. This method
has proven to produce better results than Multimodal Unsupervised
Image-to-image translation.",['cs.CV']
Unsupervised Automated Event Detection using an Iterative Clustering based Segmentation Approach,"A class of vision problems, less commonly studied, consists of detecting
objects in imagery obtained from physics-based experiments. These objects can
span in 4D (x, y, z, t) and are visible as disturbances (caused due to physical
phenomena) in the image with background distribution being approximately
uniform. Such objects, occasionally referred to as `events', can be considered
as high energy blobs in the image. Unlike the images analyzed in conventional
vision problems, very limited features are associated with such events, and
their shape, size and count can vary significantly. This poses a challenge on
the use of pre-trained models obtained from supervised approaches.
  In this paper, we propose an unsupervised approach involving iterative
clustering based segmentation (ICS) which can detect target objects (events) in
real-time. In this approach, a test image is analyzed over several cycles, and
one event is identified per cycle. Each cycle consists of the following steps:
(1) image segmentation using a modified k-means clustering method, (2)
elimination of empty (with no events) segments based on statistical analysis of
each segment, (3) merging segments that overlap (correspond to same event), and
(4) selecting the strongest event. These four steps are repeated until all the
events have been identified. The ICS approach consists of a few
hyper-parameters that have been chosen based on statistical study performed
over a set of test images. The applicability of ICS method is demonstrated on
several 2D and 3D test examples.","['cs.CV', 'cs.AI', 'cs.LG']"
UltraCompression: Framework for High Density Compression of Ultrasound Volumes using Physics Modeling Deep Neural Networks,"Ultrasound image compression by preserving speckle-based key information is a
challenging task. In this paper, we introduce an ultrasound image compression
framework with the ability to retain realism of speckle appearance despite
achieving very high-density compression factors. The compressor employs a
tissue segmentation method, transmitting segments along with transducer
frequency, number of samples and image size as essential information required
for decompression. The decompressor is based on a convolutional network trained
to generate patho-realistic ultrasound images which convey essential
information pertinent to tissue pathology visible in the images. We demonstrate
generalizability of the building blocks using two variants to build the
compressor. We have evaluated the quality of decompressed images using
distortion losses as well as perception loss and compared it with other off the
shelf solutions. The proposed method achieves a compression ratio of $725:1$
while preserving the statistical distribution of speckles. This enables image
segmentation on decompressed images to achieve dice score of $0.89 \pm 0.11$,
which evidently is not so accurately achievable when images are compressed with
current standards like JPEG, JPEG 2000, WebP and BPG. We envision this frame
work to serve as a roadmap for speckle image compression standards.",['cs.CV']
Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation,"The Encoder-Decoder architecture is a main stream deep learning model for
biomedical image segmentation. The encoder fully compresses the input and
generates encoded features, and the decoder then produces dense predictions
using encoded features. However, decoders are still under-explored in such
architectures. In this paper, we comprehensively study the state-of-the-art
Encoder-Decoder architectures, and propose a new universal decoder, called
cascade decoder, to improve semantic segmentation accuracy. Our cascade decoder
can be embedded into existing networks and trained altogether in an end-to-end
fashion. The cascade decoder structure aims to conduct more effective decoding
of hierarchically encoded features and is more compatible with common encoders
than the known decoders. We replace the decoders of state-of-the-art models
with our cascade decoder for several challenging biomedical image segmentation
tasks, and the considerable improvements achieved demonstrate the efficacy of
our new decoding method.",['cs.CV']
Automated Deep Photo Style Transfer,"Photorealism is a complex concept that cannot easily be formulated
mathematically. Deep Photo Style Transfer is an attempt to transfer the style
of a reference image to a content image while preserving its photorealism. This
is achieved by introducing a constraint that prevents distortions in the
content image and by applying the style transfer independently for semantically
different parts of the images. In addition, an automated segmentation process
is presented that consists of a neural network based segmentation method
followed by a semantic grouping step. To further improve the results a measure
for image aesthetics is used and elaborated. If the content and the style image
are sufficiently similar, the result images look very realistic. With the
automation of the image segmentation the pipeline becomes completely
independent from any user interaction, which allows for new applications.",['cs.CV']
Unsupervised domain adaptation for medical imaging segmentation with self-ensembling,"Recent advances in deep learning methods have come to define the
state-of-the-art for many medical imaging applications, surpassing even human
judgment in several tasks. Those models, however, when trained to reduce the
empirical risk on a single domain, fail to generalize when applied to other
domains, a very common scenario in medical imaging due to the variability of
images and anatomical structures, even across the same imaging modality. In
this work, we extend the method of unsupervised domain adaptation using
self-ensembling for the semantic segmentation task and explore multiple facets
of the method on a small and realistic publicly-available magnetic resonance
(MRI) dataset. Through an extensive evaluation, we show that self-ensembling
can indeed improve the generalization of the models even when using a small
amount of unlabelled data.",['cs.CV']
Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples,"In recent years, deep learning has shown performance breakthroughs in many
applications, such as image detection, image segmentation, pose estimation, and
speech recognition. However, this comes with a major concern: deep networks
have been found to be vulnerable to adversarial examples. Adversarial examples
are slightly modified inputs that are intentionally designed to cause a
misclassification by the model. In the domains of images and speech, the
modifications are so small that they are not seen or heard by humans, but
nevertheless greatly affect the classification of the model.
  Deep learning models have been successfully applied to malware detection. In
this domain, generating adversarial examples is not straightforward, as small
modifications to the bytes of the file could lead to significant changes in its
functionality and validity. We introduce a novel loss function for generating
adversarial examples specifically tailored for discrete input sets, such as
executable bytes. We modify malicious binaries so that they would be detected
as benign, while preserving their original functionality, by injecting a small
sequence of bytes (payload) in the binary file. We applied this approach to an
end-to-end convolutional deep learning malware detection model and show a high
rate of detection evasion. Moreover, we show that our generated payload is
robust enough to be transferable within different locations of the same file
and across different files, and that its entropy is low and similar to that of
benign data sections.","['cs.LG', 'cs.CR']"
Image Recognition of Tea Leaf Diseases Based on Convolutional Neural Network,"In order to identify and prevent tea leaf diseases effectively, convolution
neural network (CNN) was used to realize the image recognition of tea disease
leaves. Firstly, image segmentation and data enhancement are used to preprocess
the images, and then these images were input into the network for training.
Secondly, to reach a higher recognition accuracy of CNN, the learning rate and
iteration numbers were adjusted frequently and the dropout was added properly
in the case of over-fitting. Finally, the experimental results show that the
recognition accuracy of CNN is 93.75%, while the accuracy of SVM and BP neural
network is 89.36% and 87.69% respectively. Therefore, the recognition algorithm
based on CNN is better in classification and can improve the recognition
efficiency of tea leaf diseases effectively.","['cs.CV', 'cs.LG']"
Interactive Image Segmentation using Label Propagation through Complex Networks,"Interactive image segmentation is a topic of many studies in image
processing. In a conventional approach, a user marks some pixels of the
object(s) of interest and background, and an algorithm propagates these labels
to the rest of the image. This paper presents a new graph-based method for
interactive segmentation with two stages. In the first stage, nodes
representing pixels are connected to their $k$-nearest neighbors to build a
complex network with the small-world property to propagate the labels quickly.
In the second stage, a regular network in a grid format is used to refine the
segmentation on the object borders. Despite its simplicity, the proposed method
can perform the task with high accuracy. Computer simulations are performed
using some real-world images to show its effectiveness in both two-classes and
multi-classes problems. It is also applied to all the images from the Microsoft
GrabCut dataset for comparison, and the segmentation accuracy is comparable to
those achieved by some state-of-the-art methods, while it is faster than them.
In particular, it outperforms some recent approaches when the user input is
composed only by a few ""scribbles"" draw over the objects. Its computational
complexity is only linear on the image size at the best-case scenario and
linearithmic in the worst case.",['cs.CV']
Flow Based Self-supervised Pixel Embedding for Image Segmentation,"We propose a new self-supervised approach to image feature learning from
motion cue. This new approach leverages recent advances in deep learning in two
directions: 1) the success of training deep neural network in estimating
optical flow in real data using synthetic flow data; and 2) emerging work in
learning image features from motion cues, such as optical flow. Building on
these, we demonstrate that image features can be learned in self-supervision by
first training an optical flow estimator with synthetic flow data, and then
learning image features from the estimated flows in real motion data. We
demonstrate and evaluate this approach on an image segmentation task. Using the
learned image feature representation, the network performs significantly better
than the ones trained from scratch in few-shot segmentation tasks.",['cs.CV']
Optimal Decision-Making in Mixed-Agent Partially Observable Stochastic Environments via Reinforcement Learning,"Optimal decision making with limited or no information in stochastic
environments where multiple agents interact is a challenging topic in the realm
of artificial intelligence. Reinforcement learning (RL) is a popular approach
for arriving at optimal strategies by predicating stimuli, such as the reward
for following a strategy, on experience. RL is heavily explored in the
single-agent context, but is a nascent concept in multiagent problems. To this
end, I propose several principled model-free and partially model-based
reinforcement learning approaches for several multiagent settings. In the realm
of normative reinforcement learning, I introduce scalable extensions to Monte
Carlo exploring starts for partially observable Markov Decision Processes
(POMDP), dubbed MCES-P, where I expand the theory and algorithm to the
multiagent setting. I first examine MCES-P with probably approximately correct
(PAC) bounds in the context of multiagent setting, showing MCESP+PAC holds in
the presence of other agents. I then propose a more sample-efficient
methodology for antagonistic settings, MCESIP+PAC. For cooperative settings, I
extend MCES-P to the Multiagent POMDP, dubbed MCESMP+PAC. I then explore the
use of reinforcement learning as a methodology in searching for optima in
realistic and latent model environments. First, I explore a parameterized
Q-learning approach in modeling humans learning to reason in an uncertain,
multiagent environment. Next, I propose an implementation of MCES-P, along with
image segmentation, to create an adaptive team-based reinforcement learning
technique to positively identify the presence of phenotypically-expressed water
and pathogen stress in crop fields.","['cs.LG', 'cs.AI']"
Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks,"This paper offers three new, open-source, deep learning-based iris
segmentation methods, and the methodology how to use irregular segmentation
masks in a conventional Gabor-wavelet-based iris recognition. To train and
validate the methods, we used a wide spectrum of iris images acquired by
different teams and different sensors and offered publicly, including data
taken from CASIA-Iris-Interval-v4, BioSec, ND-Iris-0405, UBIRIS,
Warsaw-BioBase-Post-Mortem-Iris v2.0 (post-mortem iris images), and
ND-TWINS-2009-2010 (iris images acquired from identical twins). This varied
training data should increase the generalization capabilities of the proposed
segmentation techniques. In database-disjoint training and testing, we show
that deep learning-based segmentation outperforms the conventional (OSIRIS)
segmentation in terms of Intersection over Union calculated between the
obtained results and manually annotated ground-truth. Interestingly, the
Gabor-based iris matching is not always better when deep learning-based
segmentation is used, and is on par with the method employing Daugman's based
segmentation.",['cs.CV']
Impact of Ground Truth Annotation Quality on Performance of Semantic Image Segmentation of Traffic Conditions,"Preparation of high-quality datasets for the urban scene understanding is a
labor-intensive task, especially, for datasets designed for the autonomous
driving applications. The application of the coarse ground truth (GT)
annotations of these datasets without detriment to the accuracy of semantic
image segmentation (by the mean intersection over union - mIoU) could simplify
and speedup the dataset preparation and model fine tuning before its practical
application. Here the results of the comparative analysis for semantic
segmentation accuracy obtained by PSPNet deep learning architecture are
presented for fine and coarse annotated images from Cityscapes dataset. Two
scenarios were investigated: scenario 1 - the fine GT images for training and
prediction, and scenario 2 - the fine GT images for training and the coarse GT
images for prediction. The obtained results demonstrated that for the most
important classes the mean accuracy values of semantic image segmentation for
coarse GT annotations are higher than for the fine GT ones, and the standard
deviation values are vice versa. It means that for some applications some
unimportant classes can be excluded and the model can be tuned further for some
classes and specific regions on the coarse GT dataset without loss of the
accuracy even. Moreover, this opens the perspectives to use deep neural
networks for the preparation of such coarse GT datasets.","['cs.CV', 'cs.LG', 'stat.ML']"
Annotation-cost Minimization for Medical Image Segmentation using Suggestive Mixed Supervision Fully Convolutional Networks,"For medical image segmentation, most fully convolutional networks (FCNs) need
strong supervision through a large sample of high-quality dense segmentations,
which is taxing in terms of costs, time and logistics involved. This burden of
annotation can be alleviated by exploiting weak inexpensive annotations such as
bounding boxes and anatomical landmarks. However, it is very difficult to
\textit{a priori} estimate the optimal balance between the number of
annotations needed for each supervision type that leads to maximum performance
with the least annotation cost. To optimize this cost-performance trade off, we
present a budget-based cost-minimization framework in a mixed-supervision
setting via dense segmentations, bounding boxes, and landmarks. We propose a
linear programming (LP) formulation combined with uncertainty and similarity
based ranking strategy to judiciously select samples to be annotated next for
optimal performance. In the results section, we show that our proposed method
achieves comparable performance to state-of-the-art approaches with
significantly reduced cost of annotations.",['cs.CV']
Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning,"Recently there has been a lot of work on pruning filters from deep
convolutional neural networks (CNNs) with the intention of reducing
computations.The key idea is to rank the filters based on a certain criterion
(say, l1-norm) and retain only the top ranked filters. Once the low scoring
filters are pruned away the remainder of the network is fine tuned and is shown
to give performance comparable to the original unpruned network. In this work,
we report experiments which suggest that the comparable performance of the
pruned network is not due to the specific criterion chosen but due to the
inherent plasticity of deep neural networks which allows them to recover from
the loss of pruned filters once the rest of the filters are fine-tuned.
Specifically we show counter-intuitive results wherein by randomly pruning
25-50% filters from deep CNNs we are able to obtain the same performance as
obtained by using state-of-the-art pruning methods. We empirically validate our
claims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also
evaluate a real world scenario where a CNN trained on all 1000 ImageNet classes
needs to be tested on only a small set of classes at test time (say, only
animals). We create a new benchmark dataset from ImageNet to evaluate such
class specific pruning and show that even here a random pruning strategy gives
close to state-of-the-art performance. Unlike existing approaches which mainly
focus on the task of image classification, in this work we also report results
on object detection and image segmentation. We show that using a simple random
pruning strategy we can achieve significant speed up in object detection (74%
improvement in fps) while retaining the same accuracy as that of the original
Faster RCNN model. Similarly we show that the performance of a pruned
Segmentation Network (SegNet) is actually very similar to that of the original
unpruned SegNet.","['cs.LG', 'cs.CV', 'cs.NE']"
Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images,"Convolutional Neural Networks (CNNs) have achieved state-of-the-art
performance in many different 2D medical image analysis tasks. In clinical
practice, however, a large part of the medical imaging data available is in 3D.
This has motivated the development of 3D CNNs for volumetric image segmentation
in order to benefit from more spatial context. Due to GPU memory restrictions
caused by moving to fully 3D, state-of-the-art methods depend on
subvolume/patch processing and the size of the input patch is usually small,
limiting the incorporation of larger context information for a better
performance. In this paper, we propose a novel Holistic Decomposition
Convolution (HDC), for an effective and efficient semantic segmentation of
volumetric images. HDC consists of a periodic down-shuffling operation followed
by a conventional 3D convolution. HDC has the advantage of significantly
reducing the size of the data for sub-sequential processing while using all the
information available in the input irrespective of the down-shuffling factors.
Results obtained from comprehensive experiments conducted on hip T1 MR images
and intervertebral disc T2 MR images demonstrate the efficacy of the present
approach.",['cs.CV']
The algorithm of formation of a training set for an artificial neural network for image segmentation,"This article suggests an algorithm of formation a training set for artificial
neural network in case of image segmentation. The distinctive feature of this
algorithm is that it using only one image for segmentation. The segmentation
performs using three-layer perceptron. The main method of the segmentation is a
method of region growing. Neural network is using for get a decision to include
pixel into an area or not. Impulse noise is using for generation of a training
set. Pixels damaged by noise are not related to the same region. Suggested
method has been tested with help of computer experiment in automatic and
interactive modes.","['cs.CV', 'cs.GR', 'cs.NE']"
A Gentle Introduction to Deep Learning in Medical Image Processing,"This paper tries to give a gentle introduction to deep learning in medical
image processing, proceeding from theoretical foundations to applications. We
first discuss general reasons for the popularity of deep learning, including
several major breakthroughs in computer science. Next, we start reviewing the
fundamental basics of the perceptron and neural networks, along with some
fundamental theory that is often omitted. Doing so allows us to understand the
reasons for the rise of deep learning in many application domains. Obviously
medical image processing is one of these areas which has been largely affected
by this rapid progress, in particular in image detection and recognition, image
segmentation, image registration, and computer-aided diagnosis. There are also
recent trends in physical simulation, modelling, and reconstruction that have
led to astonishing results. Yet, some of these approaches neglect prior
knowledge and hence bear the risk of producing implausible results. These
apparent weaknesses highlight current limitations of deep learning. However, we
also briefly discuss promising approaches that might be able to resolve these
problems in the future.",['cs.CV']
Unsupervised Meta-learning of Figure-Ground Segmentation via Imitating Visual Effects,"This paper presents a ""learning to learn"" approach to figure-ground image
segmentation. By exploring webly-abundant images of specific visual effects,
our method can effectively learn the visual-effect internal representations in
an unsupervised manner and uses this knowledge to differentiate the figure from
the ground in an image. Specifically, we formulate the meta-learning process as
a compositional image editing task that learns to imitate a certain visual
effect and derive the corresponding internal representation. Such a generative
process can help instantiate the underlying figure-ground notion and enables
the system to accomplish the intended image segmentation. Whereas existing
generative methods are mostly tailored to image synthesis or style transfer,
our approach offers a flexible learning mechanism to model a general concept of
figure-ground segmentation from unorganized images that have no explicit
pixel-level annotations. We validate our approach via extensive experiments on
six datasets to demonstrate that the proposed model can be end-to-end trained
without ground-truth pixel labeling yet outperforms the existing methods of
unsupervised segmentation tasks.",['cs.CV']
Fast and Accurate 3D Medical Image Segmentation with Data-swapping Method,"Deep neural network models used for medical image segmentation are large
because they are trained with high-resolution three-dimensional (3D) images.
Graphics processing units (GPUs) are widely used to accelerate the trainings.
However, the memory on a GPU is not large enough to train the models. A popular
approach to tackling this problem is patch-based method, which divides a large
image into small patches and trains the models with these small patches.
However, this method would degrade the segmentation quality if a target object
spans multiple patches. In this paper, we propose a novel approach for 3D
medical image segmentation that utilizes the data-swapping, which swaps out
intermediate data from GPU memory to CPU memory to enlarge the effective GPU
memory size, for training high-resolution 3D medical images without patching.
We carefully tuned parameters in the data-swapping method to obtain the best
training performance for 3D U-Net, a widely used deep neural network model for
medical image segmentation. We applied our tuning to train 3D U-Net with
full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result,
communication overhead, which is the most important issue, was reduced by
17.1%. Compared with the patch-based method for patches of 128 x 128 x 128
voxels, our training for full-size images achieved improvement on the mean Dice
score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core
sub-region, respectively. The total training time was reduced from 164 hours to
47 hours, resulting in 3.53 times of acceleration.","['cs.LG', 'cs.CV', 'cs.PF', 'stat.ML', 'C.4; I.2.6; I.2.10; I.4.6; I.4.9; J.4']"
SwipeCut: Interactive Segmentation with Diversified Seed Proposals,"Interactive image segmentation algorithms rely on the user to provide
annotations as the guidance. When the task of interactive segmentation is
performed on a small touchscreen device, the requirement of providing precise
annotations could be cumbersome to the user. We design an efficient seed
proposal method that actively proposes annotation seeds for the user to label.
The user only needs to check which ones of the query seeds are inside the
region of interest (ROI). We enforce the sparsity and diversity criteria on the
selection of the query seeds. At each round of interaction the user is only
presented with a small number of informative query seeds that are far apart
from each other. As a result, we are able to derive a user friendly interaction
mechanism for annotation on small touchscreen devices. The user merely has to
swipe through on the ROI-relevant query seeds, which should be easy since those
gestures are commonly used on a touchscreen. The performance of our algorithm
is evaluated on six publicly available datasets. The evaluation results show
that our algorithm achieves high segmentation accuracy, with short response
time and less user feedback.",['cs.CV']
Pixel Objectness: Learning to Segment Generic Objects Automatically in Images and Videos,"We propose an end-to-end learning framework for segmenting generic objects in
both images and videos. Given a novel image or video, our approach produces a
pixel-level mask for all ""object-like"" regions---even for object categories
never seen during training. We formulate the task as a structured prediction
problem of assigning an object/background label to each pixel, implemented
using a deep fully convolutional network. When applied to a video, our model
further incorporates a motion stream, and the network learns to combine both
appearance and motion and attempts to extract all prominent objects whether
they are moving or not. Beyond the core model, a second contribution of our
approach is how it leverages varying strengths of training annotations.
Pixel-level annotations are quite difficult to obtain, yet crucial for training
a deep network approach for segmentation. Thus we propose ways to exploit
weakly labeled data for learning dense foreground segmentation. For images, we
show the value in mixing object category examples with image-level labels
together with relatively few images with boundary-level annotations. For video,
we show how to bootstrap weakly annotated videos together with the network
trained for image segmentation. Through experiments on multiple challenging
image and video segmentation benchmarks, our method offers consistently strong
results and improves the state-of-the-art for fully automatic segmentation of
generic (unseen) objects. In addition, we demonstrate how our approach benefits
image retrieval and image retargeting, both of which flourish when given our
high-quality foreground maps. Code, models, and videos are
at:http://vision.cs.utexas.edu/projects/pixelobjectness/",['cs.CV']
Accelerated Inference in Markov Random Fields via Smooth Riemannian Optimization,"Markov Random Fields (MRFs) are a popular model for several pattern
recognition and reconstruction problems in robotics and computer vision.
Inference in MRFs is intractable in general and related work resorts to
approximation algorithms. Among those techniques, semidefinite programming
(SDP) relaxations have been shown to provide accurate estimates while scaling
poorly with the problem size and being typically slow for practical
applications. Our first contribution is to design a dual ascent method to solve
standard SDP relaxations that takes advantage of the geometric structure of the
problem to speed up computation. This technique, named Dual Ascent Riemannian
Staircase (DARS), is able to solve large problem instances in seconds. Our
second contribution is to develop a second and faster approach. The backbone of
this second approach is a novel SDP relaxation combined with a fast and
scalable solver based on smooth Riemannian optimization. We show that this
approach, named Fast Unconstrained SEmidefinite Solver (FUSES), can solve large
problems in milliseconds. Contrarily to local MRF solvers, e.g., loopy belief
propagation, our approaches do not require an initial guess. Moreover, we
leverage recent results from optimization theory to provide per-instance
sub-optimality guarantees. We demonstrate the proposed approaches in
multi-class image segmentation problems. Extensive experimental evidence shows
that (i) FUSES and DARS produce near-optimal solutions, attaining an objective
within 0.1% of the optimum, (ii) FUSES and DARS are remarkably faster than
general-purpose SDP solvers, and FUSES is more than two orders of magnitude
faster than DARS while attaining similar solution quality, (iii) FUSES is
faster than local search methods while being a global solver.","['cs.CV', '65K05, 62F10, 65D19']"
Asymmetric Loss Functions and Deep Densely Connected Networks for Highly Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis Lesion Detection,"Fully convolutional deep neural networks have been asserted to be fast and
precise frameworks with great potential in image segmentation. One of the major
challenges in training such networks raises when data is unbalanced, which is
common in many medical imaging applications such as lesion segmentation where
lesion class voxels are often much lower in numbers than non-lesion voxels. A
trained network with unbalanced data may make predictions with high precision
and low recall, being severely biased towards the non-lesion class which is
particularly undesired in most medical applications where FNs are more
important than FPs. Various methods have been proposed to address this problem,
more recently similarity loss functions and focal loss. In this work we trained
fully convolutional deep neural networks using an asymmetric similarity loss
function to mitigate the issue of data imbalance and achieve much better
tradeoff between precision and recall. To this end, we developed a 3D
FC-DenseNet with large overlapping image patches as input and an asymmetric
similarity loss layer based on Tversky index (using Fbeta scores). We used
large overlapping image patches as inputs for intrinsic and extrinsic data
augmentation, a patch selection algorithm, and a patch prediction fusion
strategy using B-spline weighted soft voting to account for the uncertainty of
prediction in patch borders. We applied this method to MS lesion segmentation
based on two different datasets of MSSEG and ISBI longitudinal MS lesion
segmentation challenge, where we achieved top performance in both challenges.
Our network trained with focal loss ranked first according to the ISBI
challenge overall score and resulted in the lowest reported lesion false
positive rate among all submitted methods. Our network trained with the
asymmetric similarity loss led to the lowest surface distance and the best
lesion true positive rate.",['cs.CV']
NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks,"The graph Laplacian is a standard tool in data science, machine learning, and
image processing. The corresponding matrix inherits the complex structure of
the underlying network and is in certain applications densely populated. This
makes computations, in particular matrix-vector products, with the graph
Laplacian a hard task. A typical application is the computation of a number of
its eigenvalues and eigenvectors. Standard methods become infeasible as the
number of nodes in the graph is too large. We propose the use of the fast
summation based on the nonequispaced fast Fourier transform (NFFT) to perform
the dense matrix-vector product with the graph Laplacian fast without ever
forming the whole matrix. The enormous flexibility of the NFFT algorithm allows
us to embed the accelerated multiplication into Lanczos-based eigenvalues
routines or iterative linear system solvers and even consider other than the
standard Gaussian kernels. We illustrate the feasibility of our approach on a
number of test problems from image segmentation to semi-supervised learning
based on graph-based PDEs. In particular, we compare our approach with the
Nystr\""om method. Moreover, we present and test an enhanced, hybrid version of
the Nystr\""om method, which internally uses the NFFT.","['cs.LG', 'math.NA', 'stat.ML', '68R10, 05C50, 65F15, 65T50, 68T05, 62H30']"
Weakly Supervised Instance Segmentation Using Hybrid Network,"Weakly-supervised instance segmentation, which could greatly save labor and
time cost of pixel mask annotation, has attracted increasing attention in
recent years. The commonly used pipeline firstly utilizes conventional image
segmentation methods to automatically generate initial masks and then use them
to train an off-the-shelf segmentation network in an iterative way. However,
the initial generated masks usually contains a notable proportion of invalid
masks which are mainly caused by small object instances. Directly using these
initial masks to train segmentation model is harmful for the performance. To
address this problem, we propose a hybrid network in this paper. In our
architecture, there is a principle segmentation network which is used to handle
the normal samples with valid generated masks. In addition, a complementary
branch is added to handle the small and dim objects without valid masks.
Experimental results indicate that our method can achieve significantly
performance improvement both on the small object instances and large ones, and
outperforms all state-of-the-art methods.",['cs.CV']
Deep Neural Networks Motivated by Partial Differential Equations,"Partial differential equations (PDEs) are indispensable for modeling many
physical phenomena and also commonly used for solving image processing tasks.
In the latter area, PDE-based approaches interpret image data as
discretizations of multivariate functions and the output of image processing
algorithms as solutions to certain PDEs. Posing image processing problems in
the infinite dimensional setting provides powerful tools for their analysis and
solution. Over the last few decades, the reinterpretation of classical image
processing problems through the PDE lens has been creating multiple celebrated
approaches that benefit a vast area of tasks including image segmentation,
denoising, registration, and reconstruction.
  In this paper, we establish a new PDE-interpretation of a class of deep
convolutional neural networks (CNN) that are commonly used to learn from
speech, image, and video data. Our interpretation includes convolution residual
neural networks (ResNet), which are among the most promising approaches for
tasks such as image classification having improved the state-of-the-art
performance in prestigious benchmark challenges. Despite their recent
successes, deep ResNets still face some critical challenges associated with
their design, immense computational costs and memory requirements, and lack of
understanding of their reasoning.
  Guided by well-established PDE theory, we derive three new ResNet
architectures that fall into two new classes: parabolic and hyperbolic CNNs. We
demonstrate how PDE theory can provide new insights and algorithms for deep
learning and demonstrate the competitiveness of three new CNN architectures
using numerical experiments.","['cs.LG', 'math.OC', 'stat.ML', '65K10, 68T45']"
A New Ensemble Learning Framework for 3D Biomedical Image Segmentation,"3D image segmentation plays an important role in biomedical image analysis.
Many 2D and 3D deep learning models have achieved state-of-the-art segmentation
performance on 3D biomedical image datasets. Yet, 2D and 3D models have their
own strengths and weaknesses, and by unifying them together, one may be able to
achieve more accurate results. In this paper, we propose a new ensemble
learning framework for 3D biomedical image segmentation that combines the
merits of 2D and 3D models. First, we develop a fully convolutional network
based meta-learner to learn how to improve the results from 2D and 3D models
(base-learners). Then, to minimize over-fitting for our sophisticated
meta-learner, we devise a new training method that uses the results of the
base-learners as multiple versions of ""ground truths"". Furthermore, since our
new meta-learner training scheme does not depend on manual annotation, it can
utilize abundant unlabeled 3D image data to further improve the model.
Extensive experiments on two public datasets (the HVSMR 2016 Challenge dataset
and the mouse piriform cortex dataset) show that our approach is effective
under fully-supervised, semi-supervised, and transductive settings, and attains
superior performance over state-of-the-art image segmentation methods.",['cs.CV']
Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation,"The most recent fast and accurate image segmentation methods are built upon
fully convolutional deep neural networks. In this paper, we propose new deep
learning strategies for DenseNets to improve segmenting images with subtle
differences in intensity values and features. We aim to segment brain tissue on
infant brain MRI at about 6 months of age where white matter and gray matter of
the developing brain show similar T1 and T2 relaxation times, thus appear to
have similar intensity values on both T1- and T2-weighted MRI scans. Brain
tissue segmentation at this age is, therefore, very challenging. To this end,
we propose an exclusive multi-label training strategy to segment the mutually
exclusive brain tissues with similarity loss functions that automatically
balance the training based on class prevalence. Using our proposed training
strategy based on similarity loss functions and patch prediction fusion we
decrease the number of parameters in the network, reduce the complexity of the
training process focusing the attention on less number of tasks, while
mitigating the effects of data imbalance between labels and inaccuracies near
patch borders. By taking advantage of these strategies we were able to perform
fast image segmentation (90 seconds per 3D volume), using a network with less
parameters than many state-of-the-art networks, overcoming issues such as
3Dvs2D training and large vs small patch size selection, while achieving the
top performance in segmenting brain tissue among all methods tested in first
and second round submissions of the isointense infant brain MRI segmentation
(iSeg) challenge according to the official challenge test results. Our proposed
strategy improves the training process through balanced training and by
reducing its complexity while providing a trained model that works for any size
input image and is fast and more accurate than many state-of-the-art methods.",['cs.CV']
Automatically Segmenting the Left Atrium from Cardiac Images Using Successive 3D U-Nets and a Contour Loss,"Radiological imaging offers effective measurement of anatomy, which is useful
in disease diagnosis and assessment. Previous study has shown that the left
atrial wall remodeling can provide information to predict treatment outcome in
atrial fibrillation. Nevertheless, the segmentation of the left atrial
structures from medical images is still very time-consuming. Current advances
in neural network may help creating automatic segmentation models that reduce
the workload for clinicians. In this preliminary study, we propose automated,
two-stage, three-dimensional U-Nets with convolutional neural network, for the
challenging task of left atrial segmentation. Unlike previous two-dimensional
image segmentation methods, we use 3D U-Nets to obtain the heart cavity
directly in 3D. The dual 3D U-Net structure consists of, a first U-Net to
coarsely segment and locate the left atrium, and a second U-Net to accurately
segment the left atrium under higher resolution. In addition, we introduce a
Contour loss based on additional distance information to adjust the final
segmentation. We randomly split the data into training datasets (80 subjects)
and validation datasets (20 subjects) to train multiple models, with different
augmentation setting. Experiments show that the average Dice coefficients for
validation datasets are around 0.91 - 0.92, the sensitivity around 0.90-0.94
and the specificity 0.99. Compared with traditional Dice loss, models trained
with Contour loss in general offer smaller Hausdorff distance with similar Dice
coefficient, and have less connected components in predictions. Finally, we
integrate several trained models in an ensemble prediction to segment testing
datasets.",['cs.CV']
"""Double-DIP"": Unsupervised Image Decomposition via Coupled Deep-Image-Priors","Many seemingly unrelated computer vision tasks can be viewed as a special
case of image decomposition into separate layers. For example, image
segmentation (separation into foreground and background layers); transparent
layer separation (into reflection and transmission layers); Image dehazing
(separation into a clear image and a haze map), and more. In this paper we
propose a unified framework for unsupervised layer decomposition of a single
image, based on coupled ""Deep-image-Prior"" (DIP) networks. It was shown
[Ulyanov et al] that the structure of a single DIP generator network is
sufficient to capture the low-level statistics of a single image. We show that
coupling multiple such DIPs provides a powerful tool for decomposing images
into their basic components, for a wide variety of applications. This
capability stems from the fact that the internal statistics of a mixture of
layers is more complex than the statistics of each of its individual
components. We show the power of this approach for Image-Dehazing, Fg/Bg
Segmentation, Watermark-Removal, Transparency Separation in images and video,
and more. These capabilities are achieved in a totally unsupervised way, with
no training examples other than the input image/video itself.","['cs.CV', 'cs.LG']"
Enhancing Label-Driven Deep Deformable Image Registration with Local Distance Metrics for State-of-the-Art Cardiac Motion Tracking,"While deep learning has achieved significant advances in accuracy for medical
image segmentation, its benefits for deformable image registration have so far
remained limited to reduced computation times. Previous work has either focused
on replacing the iterative optimization of distance and smoothness terms with
CNN-layers or using supervised approaches driven by labels. Our method is the
first to combine the complementary strengths of global semantic information
(represented by segmentation labels) and local distance metrics that help align
surrounding structures. We demonstrate significant higher Dice scores (of
86.5\%) for deformable cardiac image registration compared to classic
registration (79.0\%) as well as label-driven deep learning frameworks
(83.4\%).",['cs.CV']
MDU-Net: Multi-scale Densely Connected U-Net for biomedical image segmentation,"Radiologist is ""doctor's doctor"", biomedical image segmentation plays a
central role in quantitative analysis, clinical diagnosis, and medical
intervention. In the light of the fully convolutional networks (FCN) and U-Net,
deep convolutional networks (DNNs) have made significant contributions in
biomedical image segmentation applications. In this paper, based on U-Net, we
propose MDUnet, a multi-scale densely connected U-net for biomedical image
segmentation. we propose three different multi-scale dense connections for U
shaped architectures encoder, decoder and across them. The highlights of our
architecture is directly fuses the neighboring different scale feature maps
from both higher layers and lower layers to strengthen feature propagation in
current layer. Which can largely improves the information flow encoder, decoder
and across them. Multi-scale dense connections, which means containing shorter
connections between layers close to the input and output, also makes much
deeper U-net possible. We adopt the optimal model based on the experiment and
propose a novel Multi-scale Dense U-Net (MDU-Net) architecture with
quantization. Which reduce overfitting in MDU-Net for better accuracy. We
evaluate our purpose model on the MICCAI 2015 Gland Segmentation dataset
(GlaS). The three multi-scale dense connections improve U-net performance by up
to 1.8% on test A and 3.5% on test B in the MICCAI Gland dataset. Meanwhile the
MDU-net with quantization achieves the superiority over U-Net performance by up
to 3% on test A and 4.1% on test B.",['cs.CV']
Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features,"Accurate segmentation of different sub-regions of gliomas including
peritumoral edema, necrotic core, enhancing and non-enhancing tumor core from
multimodal MRI scans has important clinical relevance in diagnosis, prognosis
and treatment of brain tumors. However, due to the highly heterogeneous
appearance and shape, segmentation of the sub-regions is very challenging.
Recent development using deep learning models has proved its effectiveness in
the past several brain segmentation challenges as well as other semantic and
medical image segmentation problems. Most models in brain tumor segmentation
use a 2D/3D patch to predict the class label for the center voxel and variant
patch sizes and scales are used to improve the model performance. However, it
has low computation efficiency and also has limited receptive field. U-Net is a
widely used network structure for end-to-end segmentation and can be used on
the entire image or extracted patches to provide classification labels over the
entire input voxels so that it is more efficient and expect to yield better
performance with larger input size. Furthermore, instead of picking the best
network structure, an ensemble of multiple models, trained on different dataset
or different hyper-parameters, can generally improve the segmentation
performance. In this study we propose to use an ensemble of 3D U-Nets with
different hyper-parameters for brain tumor segmentation. Preliminary results
showed effectiveness of this model. In addition, we developed a linear model
for survival prediction using extracted imaging and non-imaging features,
which, despite the simplicity, can effectively reduce overfitting and
regression errors.",['cs.CV']
SUSAN: Segment Unannotated image Structure using Adversarial Network,"Segmentation of magnetic resonance (MR) images is a fundamental step in many
medical imaging-based applications. The recent implementation of deep
convolutional neural networks (CNNs) in image processing has been shown to have
significant impacts on medical image segmentation. Network training of
segmentation CNNs typically requires images and paired annotation data
representing pixel-wise tissue labels referred to as masks. However, the
supervised training of highly efficient CNNs with deeper structure and more
network parameters requires a large number of training images and paired tissue
masks. Thus, there is great need to develop a generalized CNN-based
segmentation method which would be applicable for a wide variety of MR image
datasets with different tissue contrasts. The purpose of this study was to
develop and evaluate a generalized CNN-based method for fully-automated
segmentation of different MR image datasets using a single set of annotated
training data. A technique called cycle-consistent generative adversarial
network (CycleGAN) is applied as the core of the proposed method to perform
image-to-image translation between MR image datasets with different tissue
contrasts. A joint segmentation network is incorporated into the adversarial
network to obtain additional segmentation functionality. The proposed method
was evaluated for segmenting bone and cartilage on two clinical knee MR image
datasets acquired at our institution using only a single set of annotated data
from a publicly available knee MR image dataset. The new technique may further
improve the applicability and efficiency of CNN-based segmentation of medical
images while eliminating the need for large amounts of annotated training data.","['cs.CV', 'cs.AI']"
On the Computational Inefficiency of Large Batch Sizes for Stochastic Gradient Descent,"Increasing the mini-batch size for stochastic gradient descent offers
significant opportunities to reduce wall-clock training time, but there are a
variety of theoretical and systems challenges that impede the widespread
success of this technique. We investigate these issues, with an emphasis on
time to convergence and total computational cost, through an extensive
empirical analysis of network training across several architectures and problem
domains, including image classification, image segmentation, and language
modeling. Although it is common practice to increase the batch size in order to
fully exploit available computational resources, we find a substantially more
nuanced picture. Our main finding is that across a wide range of network
architectures and problem domains, increasing the batch size beyond a certain
point yields no decrease in wall-clock time to convergence for \emph{either}
train or test loss. This batch size is usually substantially below the capacity
of current systems. We show that popular training strategies for large batch
size optimization begin to fail before we can populate all available compute
resources, and we show that the point at which these methods break down depends
more on attributes like model architecture and data complexity than it does
directly on the size of the dataset.","['cs.LG', 'cs.DC', 'stat.ML']"
DSCnet: Replicating Lidar Point Clouds with Deep Sensor Cloning,"Convolutional neural networks (CNNs) have become increasingly popular for
solving a variety of computer vision tasks, ranging from image classification
to image segmentation. Recently, autonomous vehicles have created a demand for
depth information, which is often obtained using hardware sensors such as Light
detection and ranging (LIDAR). Although it can provide precise distance
measurements, most LIDARs are still far too expensive to sell in mass-produced
consumer vehicles, which has motivated methods to generate depth information
from commodity automotive sensors like cameras.
  In this paper, we propose an approach called Deep Sensor Cloning (DSC). The
idea is to use Convolutional Neural Networks in conjunction with inexpensive
sensors to replicate the 3D point-clouds that are created by expensive LIDARs.
To accomplish this, we develop a new dataset (DSDepth) and a new family of CNN
architectures (DSCnets). While previous tasks such as KITTI depth prediction
use an interpolated RGB-D images as ground-truth for training, we instead use
DSCnets to directly predict LIDAR point-clouds. When we compare the output of
our models to a $75,000 LIDAR, we find that our most accurate DSCnet achieves a
relative error of 5.77% using a single camera and 4.69% using stereo cameras.",['cs.CV']
Foreground Clustering for Joint Segmentation and Localization in Videos and Images,"This paper presents a novel framework in which video/image segmentation and
localization are cast into a single optimization problem that integrates
information from low level appearance cues with that of high level localization
cues in a very weakly supervised manner. The proposed framework leverages two
representations at different levels, exploits the spatial relationship between
bounding boxes and superpixels as linear constraints and simultaneously
discriminates between foreground and background at bounding box and superpixel
level. Different from previous approaches that mainly rely on discriminative
clustering, we incorporate a foreground model that minimizes the histogram
difference of an object across all image frames. Exploiting the geometric
relation between the superpixels and bounding boxes enables the transfer of
segmentation cues to improve localization output and vice-versa. Inclusion of
the foreground model generalizes our discriminative framework to video data
where the background tends to be similar and thus, not discriminative. We
demonstrate the effectiveness of our unified framework on the YouTube Object
video dataset, Internet Object Discovery dataset and Pascal VOC 2007.","['cs.CV', 'cs.GR', 'cs.LG']"
Marginal Weighted Maximum Log-likelihood for Efficient Learning of Perturb-and-Map models,"We consider the structured-output prediction problem through probabilistic
approaches and generalize the ""perturb-and-MAP"" framework to more challenging
weighted Hamming losses, which are crucial in applications. While in principle
our approach is a straightforward marginalization, it requires solving many
related MAP inference problems. We show that for log-supermodular pairwise
models these operations can be performed efficiently using the machinery of
dynamic graph cuts. We also propose to use double stochastic gradient descent,
both on the data and on the perturbations, for efficient learning. Our
framework can naturally take weak supervision (e.g., partial labels) into
account. We conduct a set of experiments on medium-scale character recognition
and image segmentation, showing the benefits of our algorithms.","['stat.ML', 'cs.LG']"
Stack-U-Net: Refinement Network for Image Segmentation on the Example of Optic Disc and Cup,"In this work, we propose a special cascade network for image segmentation,
which is based on the U-Net networks as building blocks and the idea of the
iterative refinement. The model was mainly applied to achieve higher
recognition quality for the task of finding borders of the optic disc and cup,
which are relevant to the presence of glaucoma. Compared to a single U-Net and
the state-of-the-art methods for the investigated tasks, very high segmentation
quality has been achieved without a need for increasing the volume of datasets.
Our experiments include comparison with the best-known methods on publicly
available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a
private data set collected in collaboration with University of California San
Francisco Medical School. The analysis of the architecture details is
presented, and it is argued that the model can be employed for a broad scope of
image segmentation problems of similar nature.",['cs.CV']
Semi-Supervised Multi-Organ Segmentation via Deep Multi-Planar Co-Training,"In multi-organ segmentation of abdominal CT scans, most existing fully
supervised deep learning algorithms require lots of voxel-wise annotations,
which are usually difficult, expensive, and slow to obtain. In comparison,
massive unlabeled 3D CT volumes are usually easily accessible. Current
mainstream works to address the semi-supervised biomedical image segmentation
problem are mostly graph-based. By contrast, deep network based semi-supervised
learning methods have not drawn much attention in this field. In this work, we
propose Deep Multi-Planar Co-Training (DMPCT), whose contributions can be
divided into two folds: 1) The deep model is learned in a co-training style
which can mine consensus information from multiple planes like the sagittal,
coronal, and axial planes; 2) Multi-planar fusion is applied to generate more
reliable pseudo-labels, which alleviates the errors occurring in the
pseudo-labels and thus can help to train better segmentation networks.
Experiments are done on our newly collected large dataset with 100 unlabeled
cases as well as 210 labeled cases where 16 anatomical structures are manually
annotated by four radiologists and confirmed by a senior expert. The results
suggest that DMPCT significantly outperforms the fully supervised method by
more than 4% especially when only a small set of annotations is used.",['cs.CV']
Automatic Brain Structures Segmentation Using Deep Residual Dilated U-Net,"Brain image segmentation is used for visualizing and quantifying anatomical
structures of the brain. We present an automated ap-proach using 2D deep
residual dilated networks which captures rich context information of different
tissues for the segmentation of eight brain structures. The proposed system was
evaluated in the MICCAI Brain Segmentation Challenge and ranked 9th out of 22
teams. We further compared the method with traditional U-Net using
leave-one-subject-out cross-validation setting on the public dataset.
Experimental results shows that the proposed method outperforms traditional
U-Net (i.e. 80.9% vs 78.3% in averaged Dice score, 4.35mm vs 11.59mm in
averaged robust Hausdorff distance) and is computationally efficient.",['cs.CV']
Deep feature transfer between localization and segmentation tasks,"In this paper, we propose a new pre-training scheme for U-net based image
segmentation. We first train the encoding arm as a localization network to
predict the center of the target, before extending it into a U-net architecture
for segmentation. We apply our proposed method to the problem of segmenting the
optic disc from fundus photographs. Our work shows that the features learned by
encoding arm can be transferred to the segmentation network to reduce the
annotation burden. We propose that an approach could have broad utility for
medical image segmentation, and alleviate the burden of delineating complex
structures by pre-training on annotations that are much easier to acquire.",['cs.CV']
Deep Learning Approach for Building Detection in Satellite Multispectral Imagery,"Building detection from satellite multispectral imagery data is being a
fundamental but a challenging problem mainly because it requires correct
recovery of building footprints from high-resolution images. In this work, we
propose a deep learning approach for building detection by applying numerous
enhancements throughout the process. Initial dataset is preprocessed by 2-sigma
percentile normalization. Then data preparation includes ensemble modelling
where 3 models were created while incorporating OpenStreetMap data. Binary
Distance Transformation (BDT) is used for improving data labeling process and
the U-Net (Convolutional Networks for Biomedical Image Segmentation) is
modified by adding batch normalization wrappers. Afterwards, it is explained
how each component of our approach is correlated with the final detection
accuracy. Finally, we compare our results with winning solutions of SpaceNet 2
competition for real satellite multispectral images of Vegas, Paris, Shanghai
and Khartoum, demonstrating the importance of our solution for achieving higher
building detection accuracy.",['cs.CV']
Validating Hyperspectral Image Segmentation,"Hyperspectral satellite imaging attracts enormous research attention in the
remote sensing community, hence automated approaches for precise segmentation
of such imagery are being rapidly developed. In this letter, we share our
observations on the strategy for validating hyperspectral image segmentation
algorithms currently followed in the literature, and show that it can lead to
over-optimistic experimental insights. We introduce a new routine for
generating segmentation benchmarks, and use it to elaborate ready-to-use
hyperspectral training-test data partitions. They can be utilized for fair
validation of new and existing algorithms without any training-test data
leakage.","['cs.CV', 'cs.LG']"
Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks,"The training of many existing end-to-end steering angle prediction models
heavily relies on steering angles as the supervisory signal. Without learning
from much richer contexts, these methods are susceptible to the presence of
sharp road curves, challenging traffic conditions, strong shadows, and severe
lighting changes. In this paper, we considerably improve the accuracy and
robustness of predictions through heterogeneous auxiliary networks feature
mimicking, a new and effective training method that provides us with much
richer contextual signals apart from steering direction. Specifically, we train
our steering angle predictive model by distilling multi-layer knowledge from
multiple heterogeneous auxiliary networks that perform related but different
tasks, e.g., image segmentation or optical flow estimation. As opposed to
multi-task learning, our method does not require expensive annotations of
related tasks on the target set. This is made possible by applying contemporary
off-the-shelf networks on the target set and mimicking their features in
different layers after transformation. The auxiliary networks are discarded
after training without affecting the runtime efficiency of our model. Our
approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming
the previous best by a large margin of 12.8% and 52.1%, respectively.
Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.",['cs.CV']
RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans,"Automatic extraction of liver and tumor from CT volumes is a challenging task
due to their heterogeneous and diffusive shapes. Recently, 2D and 3D deep
convolutional neural networks have become popular in medical image segmentation
tasks because of the utilization of large labeled datasets to learn
hierarchical features. However, 3D networks have some drawbacks due to their
high cost on computational resources. In this paper, we propose a 3D hybrid
residual attention-aware segmentation method, named RA-UNet, to precisely
extract the liver volume of interests (VOI) and segment tumors from the liver
VOI. The proposed network has a basic architecture as a 3D U-Net which extracts
contextual information combining low-level feature maps with high-level ones.
Attention modules are stacked so that the attention-aware features change
adaptively as the network goes ""very deep"" and this is made possible by
residual learning. This is the first work that an attention residual mechanism
is used to process medical volumetric images. We evaluated our framework on the
public MICCAI 2017 Liver Tumor Segmentation dataset and the 3DIRCADb dataset.
The results show that our architecture outperforms other state-of-the-art
methods. We also extend our RA-UNet to brain tumor segmentation on the
BraTS2018 and BraTS2017 datasets, and the results indicate that RA-UNet
achieves good performance on a brain tumor segmentation task as well.",['cs.CV']
Novel approach to locate region of interest in mammograms for Breast cancer,"Locating region of interest for breast cancer masses in the mammographic
image is a challenging problem in medical image processing. In this research
work, the keen idea is to efficiently extract suspected mass region for further
examination. In particular to this fact breast boundary segmentation on sliced
rgb image using modified intensity based approach followed by quad tree based
division to spot out suspicious area are proposed in the paper. To evaluate the
performance DDSM standard dataset are experimented and achieved acceptable
accuracy.",['cs.CV']
Few-shot 3D Multi-modal Medical Image Segmentation using Generative Adversarial Learning,"We address the problem of segmenting 3D multi-modal medical images in
scenarios where very few labeled examples are available for training.
Leveraging the recent success of adversarial learning for semi-supervised
segmentation, we propose a novel method based on Generative Adversarial
Networks (GANs) to train a segmentation model with both labeled and unlabeled
images. The proposed method prevents over-fitting by learning to discriminate
between true and fake patches obtained by a generator network. Our work extends
current adversarial learning approaches, which focus on 2D single-modality
images, to the more challenging context of 3D volumes of multiple modalities.
The proposed method is evaluated on the problem of segmenting brain MRI from
the iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement
is reported, compared to state-of-art segmentation networks trained in a
fully-supervised manner. In addition, our work presents a comprehensive
analysis of different GAN architectures for semi-supervised segmentation,
showing recent techniques like feature matching to yield a higher performance
than conventional adversarial training approaches. Our code is publicly
available at https://github.com/arnab39/FewShot_GAN-Unet3D",['cs.CV']
Virtual-to-Real: Learning to Control in Visual Semantic Segmentation,"Collecting training data from the physical world is usually time-consuming
and even dangerous for fragile robots, and thus, recent advances in robot
learning advocate the use of simulators as the training platform.
Unfortunately, the reality gap between synthetic and real visual data prohibits
direct migration of the models trained in virtual worlds to the real world.
This paper proposes a modular architecture for tackling the virtual-to-real
problem. The proposed architecture separates the learning model into a
perception module and a control policy module, and uses semantic image
segmentation as the meta representation for relating these two modules. The
perception module translates the perceived RGB image to semantic image
segmentation. The control policy module is implemented as a deep reinforcement
learning agent, which performs actions based on the translated image
segmentation. Our architecture is evaluated in an obstacle avoidance task and a
target following task. Experimental results show that our architecture
significantly outperforms all of the baseline methods in both virtual and real
environments, and demonstrates a faster learning curve than them. We also
present a detailed analysis for a variety of variant configurations, and
validate the transferability of our modular architecture.","['cs.CV', 'cs.RO', 'cs.SY']"
CEREALS - Cost-Effective REgion-based Active Learning for Semantic Segmentation,"State of the art methods for semantic image segmentation are trained in a
supervised fashion using a large corpus of fully labeled training images.
However, gathering such a corpus is expensive, due to human annotation effort,
in contrast to gathering unlabeled data. We propose an active learning-based
strategy, called CEREALS, in which a human only has to hand-label a few,
automatically selected, regions within an unlabeled image corpus. This
minimizes human annotation effort while maximizing the performance of a
semantic image segmentation method. The automatic selection procedure is
achieved by: a) using a suitable information measure combined with an estimate
about human annotation effort, which is inferred from a learned cost model, and
b) exploiting the spatial coherency of an image. The performance of CEREALS is
demonstrated on Cityscapes, where we are able to reduce the annotation effort
to 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a model
that was trained with the fully annotated training set of Cityscapes.",['cs.CV']
Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images,"Objective: Deformable image registration is a fundamental problem in medical
image analysis, with applications such as longitudinal studies, population
modeling, and atlas based image segmentation. Registration is often phrased as
an optimization problem, i.e., finding a deformation field that is optimal
according to a given objective function. Discrete, combinatorial, optimization
techniques have successfully been employed to solve the resulting optimization
problem. Specifically, optimization based on $\alpha$-expansion with minimal
graph cuts has been proposed as a powerful tool for image registration. The
high computational cost of the graph-cut based optimization approach, however,
limits the utility of this approach for registration of large volume images.
Methods: Here, we propose to accelerate graph-cut based deformable registration
by dividing the image into overlapping sub-regions and restricting the
$\alpha$-expansion moves to a single sub-region at a time. Results: We
demonstrate empirically that this approach can achieve a large reduction in
computation time -- from days to minutes -- with only a small penalty in terms
of solution quality. Conclusion: The reduction in computation time provided by
the proposed method makes graph cut based deformable registration viable for
large volume images. Significance: Graph cut based image registration has
previously been shown to produce excellent results, but the high computational
cost has hindered the adoption of the method for registration of large medical
volume images. Our proposed method lifts this restriction, requiring only a
small fraction of the computational cost to produce results of comparable
quality.",['cs.CV']
Saliency guided deep network for weakly-supervised image segmentation,"Weakly-supervised image segmentation is an important task in computer vision.
A key problem is how to obtain high quality objects location from image-level
category. Classification activation mapping is a common method which can be
used to generate high-precise object location cues. However these location cues
are generally very sparse and small such that they can not provide effective
information for image segmentation. In this paper, we propose a saliency guided
image segmentation network to resolve this problem. We employ a self-attention
saliency method to generate subtle saliency maps, and render the location cues
grow as seeds by seeded region growing method to expand pixel-level labels
extent. In the process of seeds growing, we use the saliency values to weight
the similarity between pixels to control the growing. Therefore saliency
information could help generate discriminative object regions, and the effects
of wrong salient pixels can be suppressed efficiently. Experimental results on
a common segmentation dataset PASCAL VOC2012 demonstrate the effectiveness of
our method.",['cs.CV']
A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation,"We propose a generalized focal loss function based on the Tversky index to
address the issue of data imbalance in medical image segmentation. Compared to
the commonly used Dice loss, our loss function achieves a better trade off
between precision and recall when training on small structures such as lesions.
To evaluate our loss function, we improve the attention U-Net model by
incorporating an image pyramid to preserve contextual features. We experiment
on the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84% and
21.4% of the images area and improve segmentation accuracy when compared to the
standard U-Net by 25.7% and 3.6%, respectively.",['cs.CV']
Prostate Segmentation using 2D Bridged U-net,"In this paper, we focus on three problems in deep learning based medical
image segmentation. Firstly, U-net, as a popular model for medical image
segmentation, is difficult to train when convolutional layers increase even
though a deeper network usually has a better generalization ability because of
more learnable parameters. Secondly, the exponential ReLU (ELU), as an
alternative of ReLU, is not much different from ReLU when the network of
interest gets deep. Thirdly, the Dice loss, as one of the pervasive loss
functions for medical image segmentation, is not effective when the prediction
is close to ground truth and will cause oscillation during training. To address
the aforementioned three problems, we propose and validate a deeper network
that can fit medical image datasets that are usually small in the sample size.
Meanwhile, we propose a new loss function to accelerate the learning process
and a combination of different activation functions to improve the network
performance. Our experimental results suggest that our network is comparable or
superior to state-of-the-art methods.",['cs.CV']
"A Novel Extension to Fuzzy Connectivity for Body Composition Analysis: Applications in Thigh, Brain, and Whole Body Tissue Segmentation","Magnetic resonance imaging (MRI) is the non-invasive modality of choice for
body tissue composition analysis due to its excellent soft tissue contrast and
lack of ionizing radiation. However, quantification of body composition
requires an accurate segmentation of fat, muscle and other tissues from MR
images, which remains a challenging goal due to the intensity overlap between
them. In this study, we propose a fully automated, data-driven image
segmentation platform that addresses multiple difficulties in segmenting MR
images such as varying inhomogeneity, non-standardness, and noise, while
producing high-quality definition of different tissues. In contrast to most
approaches in the literature, we perform segmentation operation by combining
three different MRI contrasts and a novel segmentation tool which takes into
account variability in the data. The proposed system, based on a novel affinity
definition within the fuzzy connectivity (FC) image segmentation family,
prevents the need for user intervention and reparametrization of the
segmentation algorithms. In order to make the whole system fully automated, we
adapt an affinity propagation clustering algorithm to roughly identify tissue
regions and image background. We perform a thorough evaluation of the proposed
algorithm's individual steps as well as comparison with several approaches from
the literature for the main application of muscle/fat separation. Furthermore,
whole-body tissue composition and brain tissue delineation were conducted to
show the generalization ability of the proposed system. This new automated
platform outperforms other state-of-the-art segmentation approaches both in
accuracy and efficiency.","['cs.CV', 'cs.LG', 'q-bio.QM', 'q-bio.TO']"
A Novel Domain Adaptation Framework for Medical Image Segmentation,"We propose a segmentation framework that uses deep neural networks and
introduce two innovations. First, we describe a biophysics-based domain
adaptation method. Second, we propose an automatic method to segment white and
gray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regarding
our first innovation, we use a domain adaptation framework that combines a
novel multispecies biophysical tumor growth model with a generative adversarial
model to create realistic looking synthetic multimodal MR images with known
segmentation. Regarding our second innovation, we propose an automatic approach
to enrich available segmentation data by computing the segmentation for healthy
tissues. This segmentation, which is done using diffeomorphic image
registration between the BraTS training data and a set of prelabeled atlases,
provides more information for training and reduces the class imbalance problem.
Our overall approach is not specific to any particular neural network and can
be used in conjunction with existing solutions. We demonstrate the performance
improvement using a 2D U-Net for the BraTS'18 segmentation challenge. Our
biophysics based domain adaptation achieves better results, as compared to the
existing state-of-the-art GAN model used to create synthetic data for training.","['cs.CV', 'cs.LG', 'stat.ML']"
Image Segmentation using Unsupervised Watershed Algorithm with an Over-segmentation Reduction Technique,"Image segmentation is the process of partitioning an image into meaningful
segments. The meaning of the segments is subjective due to the definition of
homogeneity is varied based on the users perspective hence the automation of
the segmentation is challenging. Watershed is a popular segmentation technique
which assumes topographic map in an image, with the brightness of each pixel
representing its height, and finds the lines that run along the tops of ridges.
The results from the algorithm typically suffer from over segmentation due to
the lack of knowledge of the objects being classified. This paper presents an
approach to reduce the over segmentation of watershed algorithm by assuming
that the different adjacent segments of an object have similar color
distribution. The approach demonstrates an improvement over conventional
watershed algorithm.",['cs.CV']
Light-Weight RefineNet for Real-Time Semantic Segmentation,"We consider an important task of effective and efficient semantic image
segmentation. In particular, we adapt a powerful semantic segmentation
architecture, called RefineNet, into the more compact one, suitable even for
tasks requiring real-time performance on high-resolution inputs. To this end,
we identify computationally expensive blocks in the original setup, and propose
two modifications aimed to decrease the number of parameters and floating point
operations. By doing that, we achieve more than twofold model reduction, while
keeping the performance levels almost intact. Our fastest model undergoes a
significant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on
512x512 inputs with solid 81.1% mean iou performance on the test set of PASCAL
VOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7%
mean iou on the same dataset. Alternatively, we showcase that our approach is
easily mixable with light-weight classification networks: we attain 79.2% mean
iou on PASCAL VOC using a model that contains only 3.3M parameters and performs
only 9.3B floating point operations.",['cs.CV']
Deep Geodesic Learning for Segmentation and Anatomical Landmarking,"In this paper, we propose a novel deep learning framework for anatomy
segmentation and automatic landmark- ing. Specifically, we focus on the
challenging problem of mandible segmentation from cone-beam computed tomography
(CBCT) scans and identification of 9 anatomical landmarks of the mandible on
the geodesic space. The overall approach employs three inter-related steps. In
step 1, we propose a deep neu- ral network architecture with carefully designed
regularization, and network hyper-parameters to perform image segmentation
without the need for data augmentation and complex post- processing refinement.
In step 2, we formulate the landmark localization problem directly on the
geodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose
to use a long short-term memory (LSTM) network to identify closely- spaced
landmarks, which is rather difficult to obtain using other standard detection
networks. The proposed fully automated method showed superior efficacy compared
to the state-of-the- art mandible segmentation and landmarking approaches in
craniofacial anomalies and diseased states. We used a very challenging CBCT
dataset of 50 patients with a high-degree of craniomaxillofacial (CMF)
variability that is realistic in clinical practice. Complementary to the
quantitative analysis, the qualitative visual inspection was conducted for
distinct CBCT scans from 250 patients with high anatomical variability. We have
also shown feasibility of the proposed work in an independent dataset from
MICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance.
Lastly, we present an in-depth analysis of the proposed deep networks with
respect to the choice of hyper-parameters such as pooling and activation
functions.","['cs.CV', 'cs.LG', 'stat.ML']"
Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime,"This work addresses the problem of semantic image segmentation of nighttime
scenes. Although considerable progress has been made in semantic image
segmentation, it is mainly related to daytime scenarios. This paper proposes a
novel method to progressive adapt the semantic models trained on daytime
scenes, along with large-scale annotations therein, to nighttime scenes via the
bridge of twilight time -- the time between dawn and sunrise, or between sunset
and dusk. The goal of the method is to alleviate the cost of human annotation
for nighttime images by transferring knowledge from standard daytime
conditions. In addition to the method, a new dataset of road scenes is
compiled; it consists of 35,000 images ranging from daytime to twilight time
and to nighttime. Also, a subset of the nighttime images are densely annotated
for method evaluation. Our experiments show that our method is effective for
model adaptation from daytime scenes to nighttime scenes, without using extra
human annotation.",['cs.CV']
A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation,"In this paper, we propose a novel fully convolutional two-stream fusion
network (FCTSFN) for interactive image segmentation. The proposed network
includes two sub-networks: a two-stream late fusion network (TSLFN) that
predicts the foreground at a reduced resolution, and a multi-scale refining
network (MSRN) that refines the foreground at full resolution. The TSLFN
includes two distinct deep streams followed by a fusion network. The intuition
is that, since user interactions are more direct information on
foreground/background than the image itself, the two-stream structure of the
TSLFN reduces the number of layers between the pure user interaction features
and the network output, allowing the user interactions to have a more direct
impact on the segmentation result. The MSRN fuses the features from different
layers of TSLFN with different scales, in order to seek the local to global
information on the foreground to refine the segmentation result at full
resolution. We conduct comprehensive experiments on four benchmark datasets.
The results show that the proposed network achieves competitive performance
compared to current state-of-the-art interactive image segmentation methods",['cs.CV']
Learning Discriminators as Energy Networks in Adversarial Learning,"We propose a novel framework for structured prediction via adversarial
learning. Existing adversarial learning methods involve two separate networks,
i.e., the structured prediction models and the discriminative models, in the
training. The information captured by discriminative models complements that in
the structured prediction models, but few existing researches have studied on
utilizing such information to improve structured prediction models at the
inference stage. In this work, we propose to refine the predictions of
structured prediction models by effectively integrating discriminative models
into the prediction. Discriminative models are treated as energy-based models.
Similar to the adversarial learning, discriminative models are trained to
estimate scores which measure the quality of predicted outputs, while
structured prediction models are trained to predict contrastive outputs with
maximal energy scores. In this way, the gradient vanishing problem is
ameliorated, and thus we are able to perform inference by following the ascent
gradient directions of discriminative models to refine structured prediction
models. The proposed method is able to handle a range of tasks, e.g.,
multi-label classification and image segmentation. Empirical results on these
two tasks validate the effectiveness of our learning method.","['cs.CV', 'cs.CR', 'cs.LG']"
Augmented Mitotic Cell Count using Field Of Interest Proposal,"Histopathological prognostication of neoplasia including most tumor grading
systems are based upon a number of criteria. Probably the most important is the
number of mitotic figures which are most commonly determined as the mitotic
count (MC), i.e. number of mitotic figures within 10 consecutive high power
fields. Often the area with the highest mitotic activity is to be selected for
the MC. However, since mitotic activity is not known in advance, an arbitrary
choice of this region is considered one important cause for high variability in
the prognostication and grading.
  In this work, we present an algorithmic approach that first calculates a
mitotic cell map based upon a deep convolutional network. This map is in a
second step used to construct a mitotic activity estimate. Lastly, we select
the image segment representing the size of ten high power fields with the
overall highest mitotic activity as a region proposal for an expert MC
determination. We evaluate the approach using a dataset of 32 completely
annotated whole slide images, where 22 were used for training of the network
and 10 for test. We find a correlation of r=0.936 in mitotic count estimate.",['cs.CV']
Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation,"Left atrium shape has been shown to be an independent predictor of recurrence
after atrial fibrillation (AF) ablation. Shape-based representation is
imperative to such an estimation process, where correspondence-based
representation offers the most flexibility and ease-of-computation for
population-level shape statistics. Nonetheless, population-level shape
representations in the form of image segmentation and correspondence models
derived from cardiac MRI require significant human resources with sufficient
anatomy-specific expertise. In this paper, we propose a machine learning
approach that uses deep networks to estimate AF recurrence by predicting shape
descriptors directly from MRI images, with NO image pre-processing involved. We
also propose a novel data augmentation scheme to effectively train a deep
network in a limited training data setting. We compare this new method of
estimating shape descriptors from images with the state-of-the-art
correspondence-based shape modeling that requires image segmentation and
correspondence optimization. Results show that the proposed method and the
current state-of-the-art produce statistically similar outcomes on AF
recurrence, eliminating the need for expensive pre-processing pipelines and
associated human labor.","['cs.LG', 'cs.CV', 'stat.ML']"
Multi-Level Contextual Network for Biomedical Image Segmentation,"Accurate and reliable image segmentation is an essential part of biomedical
image analysis. In this paper, we consider the problem of biomedical image
segmentation using deep convolutional neural networks. We propose a new
end-to-end network architecture that effectively integrates local and global
contextual patterns of histologic primitives to obtain a more reliable
segmentation result. Specifically, we introduce a deep fully convolution
residual network with a new skip connection strategy to control the contextual
information passed forward. Moreover, our trained model is also computationally
inexpensive due to its small number of network parameters. We evaluate our
method on two public datasets for epithelium segmentation and tubule
segmentation tasks. Our experimental results show that the proposed method
provides a fast and effective way of producing a pixel-wise dense prediction of
biomedical images.",['cs.CV']
Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS,"Being able to effectively identify clouds and monitor their evolution is one
important step toward more accurate quantitative precipitation estimation and
forecast. In this study, a new gradient-based cloud-image segmentation
technique is developed using tools from image processing techniques. This
method integrates morphological image gradient magnitudes to separable cloud
systems and patches boundaries. A varying scale-kernel is implemented to reduce
the sensitivity of image segmentation to noise and capture objects with various
finenesses of the edges in remote-sensing images. The proposed method is
flexible and extendable from single- to multi-spectral imagery. Case studies
were carried out to validate the algorithm by applying the proposed
segmentation algorithm to synthetic radiances for channels of the Geostationary
Operational Environmental Satellites (GOES-R) simulated by a high-resolution
weather prediction model. The proposed method compares favorably with the
existing cloud-patch-based segmentation technique implemented in the
PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using
Artificial Neural Network - Cloud Classification System) rainfall retrieval
algorithm. Evaluation of event-based images indicates that the proposed
algorithm has potential to improve rain detection and estimation skills with an
average of more than 45% gain comparing to the segmentation technique used in
PERSIANN-CCS and identifying cloud regions as objects with accuracy rates up to
98%.",['cs.CV']
nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation,"The U-Net was presented in 2015. With its straight-forward and successful
architecture it quickly evolved to a commonly used benchmark in medical image
segmentation. The adaptation of the U-Net to novel problems, however, comprises
several degrees of freedom regarding the exact architecture, preprocessing,
training and inference. These choices are not independent of each other and
substantially impact the overall performance. The present paper introduces the
nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on
the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away
superfluous bells and whistles of many proposed network designs and instead
focus on the remaining aspects that make out the performance and
generalizability of a method. We evaluate the nnU-Net in the context of the
Medical Segmentation Decathlon challenge, which measures segmentation
performance in ten disciplines comprising distinct entities, image modalities,
image geometries and dataset sizes, with no manual adjustments between datasets
allowed. At the time of manuscript submission, nnU-Net achieves the highest
mean dice scores across all classes and seven phase 1 tasks (except class 1 in
BrainTumour) in the online leaderboard of the challenge.",['cs.CV']
Image Reconstruction Using Deep Learning,"This paper proposes a deep learning architecture that attains statistically
significant improvements over traditional algorithms in Poisson image denoising
espically when the noise is strong. Poisson noise commonly occurs in low-light
and photon- limited settings, where the noise can be most accurately modeled by
the Poission distribution. Poisson noise traditionally prevails only in
specific fields such as astronomical imaging. However, with the booming market
of surveillance cameras, which commonly operate in low-light environments, or
mobile phones, which produce noisy night scene pictures due to lower-grade
sensors, the necessity for an advanced Poisson image denoising algorithm has
increased. Deep learning has achieved amazing breakthroughs in other imaging
problems, such image segmentation and recognition, and this paper proposes a
deep learning denoising network that outperforms traditional algorithms in
Poisson denoising especially when the noise is strong. The architecture
incorporates a hybrid of convolutional and deconvolutional layers along with
symmetric connections. The denoising network achieved statistically significant
0.38dB, 0.68dB, and 1.04dB average PSNR gains over benchmark traditional
algorithms in experiments with image peak values 4, 2, and 1. The denoising
network can also operate with shorter computational time while still
outperforming the benchmark algorithm by tuning the reconstruction stride
sizes.",['cs.CV']
Diagnostics in Semantic Segmentation,"Over the past years, computer vision community has contributed to enormous
progress in semantic image segmentation, a per-pixel classification task,
crucial for dense scene understanding and rapidly becoming vital in lots of
real-world applications, including driverless cars and medical imaging. Most
recent models are now reaching previously unthinkable numbers (e.g., 89% mean
iou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and a
range of other metrics provide the general picture of model performance, in
this paper we aim to extend them into other meaningful and important for
applications characteristics, answering such questions as 'how accurate the
model segmentation is on small objects in the general scene?', or 'what are the
sources of uncertainty that cause the model to make an erroneous prediction?'.
Besides establishing a methodology that covers the performance of a single
model from different perspectives, we also showcase several extensions that can
be worth pursuing in order to further improve current results in semantic
segmentation.",['cs.CV']
Unsupervised Image to Sequence Translation with Canvas-Drawer Networks,"Encoding images as a series of high-level constructs, such as brush strokes
or discrete shapes, can often be key to both human and machine understanding.
In many cases, however, data is only available in pixel form. We present a
method for generating images directly in a high-level domain (e.g. brush
strokes), without the need for real pairwise data. Specifically, we train a
""canvas"" network to imitate the mapping of high-level constructs to pixels,
followed by a high-level ""drawing"" network which is optimized through this
mapping towards solving a desired image recreation or translation task. We
successfully discover sequential vector representations of symbols, large
sketches, and 3D objects, utilizing only pixel data. We display applications of
our method in image segmentation, and present several ablation studies
comparing various configurations.",['cs.CV']
3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes,"With the introduction of fully convolutional neural networks, deep learning
has raised the benchmark for medical image segmentation on both speed and
accuracy, and different networks have been proposed for 2D and 3D segmentation
with promising results. Nevertheless, most networks only handle relatively
small numbers of labels (<10), and there are very limited works on handling
highly unbalanced object sizes especially in 3D segmentation. In this paper, we
propose a network architecture and the corresponding loss function which
improve segmentation of very small structures. By combining skip connections
and deep supervision with respect to the computational feasibility of 3D
segmentation, we propose a fast converging and computationally efficient
network architecture for accurate segmentation. Furthermore, inspired by the
concept of focal loss, we propose an exponential logarithmic loss which
balances the labels not only by their relative sizes but also by their
segmentation difficulties. We achieve an average Dice coefficient of 82% on
brain segmentation with 20 labels, with the ratio of the smallest to largest
object sizes as 0.14%. Less than 100 epochs are required to reach such
accuracy, and segmenting a 128x128x128 volume only takes around 0.4 s.",['cs.CV']
Modern Convex Optimization to Medical Image Analysis,"Recently, diagnosis, therapy and monitoring of human diseases involve a
variety of imaging modalities, such as magnetic resonance imaging(MRI),
computed tomography(CT), Ultrasound(US) and Positron-emission tomography(PET)
as well as a variety of modern optical techniques. Over the past two decade, it
has been recognized that advanced image processing techniques provide valuable
information to physicians for diagnosis, image guided therapy and surgery, and
monitoring of the treated organ to the therapy. Many researchers and companies
have invested significant efforts in the developments of advanced medical image
analysis methods; especially in the two core studies of medical image
segmentation and registration, segmentations of organs and lesions are used to
quantify volumes and shapes used in diagnosis and monitoring treatment;
registration of multimodality images of organs improves detection, diagnosis
and staging of diseases as well as image-guided surgery and therapy,
registration of images obtained from the same modality are used to monitor
progression of therapy. These challenging clinical-motivated applications
introduce novel and sophisticated mathematical problems which stimulate
developments of advanced optimization and computing methods, especially convex
optimization attaining optimum in a global sense, hence, bring an enormous
spread of research topics for recent computational medical image analysis.
Particularly, distinct from the usual image processing, most medical images
have a big volume of acquired data, often in 3D or 4D (3D + t) along with great
noises or incomplete image information, and form the challenging large-scale
optimization problems; how to process such poor 'big data' of medical images
efficiently and solve the corresponding optimization problems robustly are the
key factors of modern medical image analysis.",['cs.CV']
Recent progress in semantic image segmentation,"Semantic image segmentation, which becomes one of the key applications in
image processing and computer vision domain, has been used in multiple domains
such as medical area and intelligent transportation. Lots of benchmark datasets
are released for researchers to verify their algorithms. Semantic segmentation
has been studied for many years. Since the emergence of Deep Neural Network
(DNN), segmentation has made a tremendous progress. In this paper, we divide
semantic image segmentation methods into two categories: traditional and recent
DNN method. Firstly, we briefly summarize the traditional method as well as
datasets released for segmentation, then we comprehensively investigate recent
methods based on DNN which are described in the eight aspects: fully
convolutional network, upsample ways, FCN joint with CRF methods, dilated
convolution approaches, progresses in backbone network, pyramid methods,
Multi-level feature and multi-stage method, supervised, weakly-supervised and
unsupervised methods. Finally, a conclusion in this area is drawn.",['cs.CV']
Mask Editor : an Image Annotation Tool for Image Segmentation Tasks,"Deep convolutional neural network (DCNN) is the state-of-the-art method for
image segmentation, which is one of key challenging computer vision tasks.
However, DCNN requires a lot of training images with corresponding image masks
to get a good segmentation result. Image annotation software which is easy to
use and allows fast image mask generation is in great demand. To the best of
our knowledge, all existing image annotation software support only drawing
bounding polygons, bounding boxes, or bounding ellipses to mark target objects.
These existing software are inefficient when targeting objects that have
irregular shapes (e.g., defects in fabric images or tire images). In this paper
we design an easy-to-use image annotation software called Mask Editor for image
mask generation. Mask Editor allows drawing any bounding curve to mark objects
and improves efficiency to mark objects with irregular shapes. Mask Editor also
supports drawing bounding polygons, drawing bounding boxes, drawing bounding
ellipses, painting, erasing, super-pixel-marking, image cropping, multi-class
masks, mask loading, and mask modifying.",['cs.CV']
A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors,"Tumor detection in biomedical imaging is a time-consuming process for medical
professionals and is not without errors. Thus in recent decades, researchers
have developed algorithmic techniques for image processing using a wide variety
of mathematical methods, such as statistical modeling, variational techniques,
and machine learning. In this paper, we propose a semi-automatic method for
liver segmentation of 2D CT scans into three labels denoting healthy, vessel,
or tumor tissue based on graph cuts. First, we create a feature vector for each
pixel in a novel way that consists of the 59 intensity values in the time
series data and propose a simplified perimeter cost term in the energy
functional. We normalize the data and perimeter terms in the functional to
expedite the graph cut without having to optimize the scaling parameter
$\lambda$. In place of a training process, predetermined tissue means are
computed based on sample regions identified by expert radiologists. The
proposed method also has the advantage of being relatively simple to implement
computationally. It was evaluated against the ground truth on a clinical CT
dataset of 10 tumors and yielded segmentations with a mean Dice similarity
coefficient (DSC) of .77 and mean volume overlap error (VOE) of 36.7%. The
average processing time was 1.25 minutes per slice.",['cs.CV']
Searching for Efficient Multi-Scale Architectures for Dense Image Prediction,"The design of neural network architectures is an important component for
achieving state-of-the-art performance with machine learning systems across a
broad array of tasks. Much work has endeavored to design and build
architectures automatically through clever construction of a search space
paired with simple learning algorithms. Recent progress has demonstrated that
such meta-learning methods may exceed scalable human-invented architectures on
image classification tasks. An open question is the degree to which such
methods may generalize to new domains. In this work we explore the construction
of meta-learning techniques for dense image prediction focused on the tasks of
scene parsing, person-part segmentation, and semantic image segmentation.
Constructing viable search spaces in this domain is challenging because of the
multi-scale representation of visual information and the necessity to operate
on high resolution imagery. Based on a survey of techniques in dense image
prediction, we construct a recursive search space and demonstrate that even
with efficient random search, we can identify architectures that outperform
human-invented architectures and achieve state-of-the-art performance on three
dense prediction tasks including 82.7\% on Cityscapes (street scene parsing),
71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL
VOC 2012 (semantic image segmentation). Additionally, the resulting
architecture is more computationally efficient, requiring half the parameters
and half the computational cost as previous state of the art systems.","['cs.CV', 'cs.LG', 'stat.ML']"
Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer,"Robot perception systems need to perform reliable image segmentation in
real-time on noisy, raw perception data. State-of-the-art segmentation
approaches use large CNN models and carefully constructed datasets; however,
these models focus on accuracy at the cost of real-time inference. Furthermore,
the standard semantic segmentation datasets are not large enough for training
CNNs without augmentation and are not representative of noisy, uncurated robot
perception data. We propose improving the performance of real-time segmentation
frameworks on robot perception data by transferring features learned from
synthetic segmentation data. We show that pretraining real-time segmentation
architectures with synthetic segmentation data instead of ImageNet improves
fine-tuning performance by reducing the bias learned in pretraining and closing
the \textit{transfer gap} as a result. Our experiments show that our real-time
robot perception models pretrained on synthetic data outperform those
pretrained on ImageNet for every scale of fine-tuning data examined. Moreover,
the degree to which synthetic pretraining outperforms ImageNet pretraining
increases as the availability of robot data decreases, making our approach
attractive for robotics domains where dataset collection is hard and/or
expensive.","['cs.CV', 'cs.RO']"
Interactive Binary Image Segmentation with Edge Preservation,"Binary image segmentation plays an important role in computer vision and has
been widely used in many applications such as image and video editing, object
extraction, and photo composition. In this paper, we propose a novel
interactive binary image segmentation method based on the Markov Random Field
(MRF) framework and the fast bilateral solver (FBS) technique. Specifically, we
employ the geodesic distance component to build the unary term. To ensure both
computation efficiency and effective responsiveness for interactive
segmentation, superpixels are used in computing geodesic distances instead of
pixels. Furthermore, we take a bilateral affinity approach for the pairwise
term in order to preserve edge information and denoise. Through the alternating
direction strategy, the MRF energy minimization problem is divided into two
subproblems, which then can be easily solved by steepest gradient descent (SGD)
and FBS respectively. Experimental results on the VGG interactive image
segmentation dataset show that the proposed algorithm outperforms several
state-of-the-art ones, and in particular, it can achieve satisfactory
edge-smooth segmentation results even when the foreground and background color
appearances are quite indistinctive.",['cs.CV']
Computation of Total Kidney Volume from CT images in Autosomal Dominant Polycystic Kidney Disease using Multi-Task 3D Convolutional Neural Networks,"Autosomal Dominant Polycystic Kidney Disease (ADPKD) characterized by
progressive growth of renal cysts is the most prevalent and potentially lethal
monogenic renal disease, affecting one in every 500-100 people. Total Kidney
Volume (TKV) and its growth computed from Computed Tomography images has been
accepted as an essential prognostic marker for renal function loss. Due to
large variation in shape and size of kidney in ADPKD, existing methods to
compute TKV (i.e. to segment ADKP) including those based on 2D convolutional
neural networks are not accurate enough to be directly useful in clinical
practice. In this work, we propose multi-task 3D Convolutional Neural Networks
to segment ADPK and achieve a mean DICE score of 0.95 and mean absolute
percentage TKV error of 3.86. Additionally, to solve the challenge of class
imbalance, we propose to simply bootstrap cross entropy loss and compare
results with recently prevalent dice loss in medical image segmentation
community.",['cs.CV']
YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark,"Learning long-term spatial-temporal features are critical for many video
analysis tasks. However, existing video segmentation methods predominantly rely
on static image segmentation techniques, and methods capturing temporal
dependency for segmentation have to depend on pretrained optical flow models,
leading to suboptimal solutions for the problem. End-to-end sequential learning
to explore spatialtemporal features for video segmentation is largely limited
by the scale of available video segmentation datasets, i.e., even the largest
video segmentation dataset only contains 90 short video clips. To solve this
problem, we build a new large-scale video object segmentation dataset called
YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains
4,453 YouTube video clips and 94 object categories. This is by far the largest
video object segmentation dataset to our knowledge and has been released at
http://youtube-vos.org. We further evaluate several existing state-of-the-art
video object segmentation algorithms on this dataset which aims to establish
baselines for the development of new algorithms in the future.","['cs.CV', 'cs.AI']"
Retinal Vessel Segmentation under Extreme Low Annotation: A Generative Adversarial Network Approach,"Contemporary deep learning based medical image segmentation algorithms
require hours of annotation labor by domain experts. These data hungry deep
models perform sub-optimally in the presence of limited amount of labeled data.
In this paper, we present a data efficient learning framework using the recent
concept of Generative Adversarial Networks; this allows a deep neural network
to perform significantly better than its fully supervised counterpart in low
annotation regime. The proposed method is an extension of our previous work
with the addition of a new unsupervised adversarial loss and a structured
prediction based architecture. To the best of our knowledge, this work is the
first demonstration of an adversarial framework based structured prediction
model for medical image segmentation. Though generic, we apply our method for
segmentation of blood vessels in retinal fundus images. We experiment with
extreme low annotation budget (0.8 - 1.6% of contemporary annotation size). On
DRIVE and STARE datasets, the proposed method outperforms our previous method
and other fully supervised benchmark models by significant margins especially
with very low number of annotated examples. In addition, our systematic
ablation studies suggest some key recipes for successfully training GAN based
semi-supervised algorithms with an encoder-decoder style network architecture.",['cs.CV']
Iris recognition in cases of eye pathology,"This chapter provides insight on how iris recognition, one of the leading
biometric identification technologies in the world, can be impacted by
pathologies and illnesses present in the eye, what are the possible
repercussions of this influence, and what are the possible means for taking
such effects into account when matching iris samples.
  To make this study possible, a special database of iris images has been used,
representing more than 20 different medical conditions of the ocular region
(including cataract, glaucoma, rubeosis iridis, synechiae, iris defects,
corneal pathologies and other) and containing almost 3000 samples collected
from 230 distinct irises. Then, with the use of four different iris recognition
methods, a series of experiments has been conducted, concluding in several
important observations.
  One of the most popular ocular disorders worldwide - the cataract - is shown
to worsen genuine comparison scores when results obtained from
cataract-affected eyes are compared to those coming from healthy irises. An
analysis devoted to different types of impact on eye structures caused by
diseases is also carried out with significant results. The enrollment process
is highly sensitive to those eye conditions that make the iris obstructed or
introduce geometrical distortions. Disorders affecting iris geometry, or
producing obstructions are exceptionally capable of degrading the genuine
comparison scores, so that the performance of the entire biometric system can
be influenced. Experiments also reveal that imperfect execution of the image
segmentation stage is the most prominent contributor to recognition errors.",['cs.CV']
Image Segmentation with Pseudo-marginal MCMC Sampling and Nonparametric Shape Priors,"In this paper, we propose an efficient pseudo-marginal Markov chain Monte
Carlo (MCMC) sampling approach to draw samples from posterior shape
distributions for image segmentation. The computation time of the proposed
approach is independent from the size of the training set used to learn the
shape prior distribution nonparametrically. Therefore, it scales well for very
large data sets. Our approach is able to characterize the posterior probability
density in the space of shapes through its samples, and to return multiple
solutions, potentially from different modes of a multimodal probability
density, which would be encountered, e.g., in segmenting objects from multiple
shape classes. Experimental results demonstrate the potential of the proposed
approach.",['cs.CV']
YouTube-VOS: Sequence-to-Sequence Video Object Segmentation,"Learning long-term spatial-temporal features are critical for many video
analysis tasks. However, existing video segmentation methods predominantly rely
on static image segmentation techniques, and methods capturing temporal
dependency for segmentation have to depend on pretrained optical flow models,
leading to suboptimal solutions for the problem. End-to-end sequential learning
to explore spatial-temporal features for video segmentation is largely limited
by the scale of available video segmentation datasets, i.e., even the largest
video segmentation dataset only contains 90 short video clips. To solve this
problem, we build a new large-scale video object segmentation dataset called
YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains
3,252 YouTube video clips and 78 categories including common objects and human
activities. This is by far the largest video object segmentation dataset to our
knowledge and we have released it at https://youtube-vos.org. Based on this
dataset, we propose a novel sequence-to-sequence network to fully exploit
long-term spatial-temporal information in videos for segmentation. We
demonstrate that our method is able to achieve the best results on our
YouTube-VOS test set and comparable results on DAVIS 2016 compared to the
current state-of-the-art methods. Experiments show that the large scale dataset
is indeed a key factor to the success of our model.",['cs.CV']
Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera,"This paper delivers a new database of iris images collected in visible light
using a mobile phone's camera and presents results of experiments involving
existing commercial and open-source iris recognition methods, namely: IriCore,
VeriEye, MIRLIN and OSIRIS. Several important observations are made.
  First, we manage to show that after simple preprocessing, such images offer
good visibility of iris texture even in heavily-pigmented irides. Second, for
all four methods, the enrollment stage is not much affected by the fact that
different type of data is used as input. This translates to zero or
close-to-zero Failure To Enroll, i.e., cases when templates could not be
extracted from the samples. Third, we achieved good matching accuracy, with
correct genuine match rate exceeding 94.5% for all four methods, while
simultaneously being able to maintain zero false match rate in every case.
Correct genuine match rate of over 99.5% was achieved using one of the
commercial methods, showing that such images can be used with the existing
biometric solutions with minimum additional effort required. Finally, the
experiments revealed that incorrect image segmentation is the most prevalent
cause of recognition accuracy decrease.
  To our best knowledge, this is the first database of iris images captured
using a mobile device, in which image quality exceeds this of a near-infrared
illuminated iris images, as defined in ISO/IEC 19794-6 and 29794-6 documents.
This database will be publicly available to all researchers.",['cs.CV']
Cataract influence on iris recognition performance,"This paper presents the experimental study revealing weaker performance of
the automatic iris recognition methods for cataract-affected eyes when compared
to healthy eyes. There is little research on the topic, mostly incorporating
scarce databases that are often deficient in images representing more than one
illness. We built our own database, acquiring 1288 eye images of 37 patients of
the Medical University of Warsaw. Those images represent several common ocular
diseases, such as cataract, along with less ordinary conditions, such as iris
pattern alterations derived from illness or eye trauma. Images were captured in
near-infrared light (used in biometrics) and for selected cases also in visible
light (used in ophthalmological diagnosis). Since cataract is a disorder that
is most populated by samples in the database, in this paper we focus solely on
this illness. To assess the extent of the performance deterioration we use
three iris recognition methodologies (commercial and academic solutions) to
calculate genuine match scores for healthy eyes and those influenced by
cataract. Results show a significant degradation in iris recognition
reliability manifesting by worsening the genuine scores in all three matchers
used in this study (12% of genuine score increase for an academic matcher, up
to 175% of genuine score increase obtained for an example commercial matcher).
This increase in genuine scores affected the final false non-match rate in two
matchers. To our best knowledge this is the only study of such kind that
employs more than one iris matcher, and analyzes the iris image segmentation as
a potential source of decreased reliability.",['cs.CV']
A Simplified Approach to Deep Learning for Image Segmentation,"Leaping into the rapidly developing world of deep learning is an exciting and
sometimes confusing adventure. All of the advice and tutorials available can be
hard to organize and work through, especially when training specific models on
specific datasets, different from those originally used to train the network.
In this short guide, we aim to walk the reader through the techniques that we
have used to successfully train two deep neural networks for pixel-wise
classification, including some data management and augmentation approaches for
working with image data that may be insufficiently annotated or relatively
homogenous.",['cs.CV']
Understanding Neural Pathways in Zebrafish through Deep Learning and High Resolution Electron Microscope Data,"The tracing of neural pathways through large volumes of image data is an
incredibly tedious and time-consuming process that significantly encumbers
progress in neuroscience. We are exploring deep learning's potential to
automate segmentation of high-resolution scanning electron microscope (SEM)
image data to remove that barrier. We have started with neural pathway tracing
through 5.1GB of whole-brain serial-section slices from larval zebrafish
collected by the Center for Brain Science at Harvard University. This kind of
manual image segmentation requires years of careful work to properly trace the
neural pathways in an organism as small as a zebrafish larva (approximately 5mm
in total body length). In automating this process, we would vastly improve
productivity, leading to faster data analysis and breakthroughs in
understanding the complexity of the brain. We will build upon prior attempts to
employ deep learning for automatic image segmentation extending methods for
unconventional deep learning data.",['cs.CV']
Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks,"In this work, a region-based Deep Convolutional Neural Network framework is
proposed for document structure learning. The contribution of this work
involves efficient training of region based classifiers and effective
ensembling for document image classification. A primary level of `inter-domain'
transfer learning is used by exporting weights from a pre-trained VGG16
architecture on the ImageNet dataset to train a document classifier on whole
document images. Exploiting the nature of region based influence modelling, a
secondary level of `intra-domain' transfer learning is used for rapid training
of deep learning models for image segments. Finally, stacked generalization
based ensembling is utilized for combining the predictions of the base deep
neural network models. The proposed method achieves state-of-the-art accuracy
of 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks
set by existing algorithms.","['cs.CV', 'cs.LG']"
TreeSegNet: Adaptive Tree CNNs for Subdecimeter Aerial Image Segmentation,"For the task of subdecimeter aerial imagery segmentation, fine-grained
semantic segmentation results are usually difficult to obtain because of
complex remote sensing content and optical conditions. Recently, convolutional
neural networks (CNNs) have shown outstanding performance on this task.
Although many deep neural network structures and techniques have been applied
to improve the accuracy, few have paid attention to better differentiating the
easily confused classes. In this paper, we propose TreeSegNet which adopts an
adaptive network to increase the classification rate at the pixelwise level.
Specifically, based on the infrastructure of DeepUNet, a Tree-CNN block in
which each node represents a ResNeXt unit is constructed adaptively according
to the confusion matrix and the proposed TreeCutting algorithm. By transporting
feature maps through concatenating connections, the Tree-CNN block fuses
multiscale features and learns best weights for the model. In experiments on
the ISPRS 2D semantic labeling Potsdam dataset, the results obtained by
TreeSegNet are better than those of other published state-of-the-art methods.
Detailed comparison and analysis show that the improvement brought by the
adaptive Tree-CNN block is significant.",['cs.CV']
Segmentation of Microscopy Data for finding Nuclei in Divergent Images,"Every year millions of people die due to disease of Cancer. Due to its
invasive nature it is very complex to cure even in primary stages. Hence, only
method to survive this disease completely is via forecasting by analyzing the
early mutation in cells of the patient biopsy. Cell Segmentation can be used to
find cell which have left their nuclei. This enables faster cure and high rate
of survival. Cell counting is a hard, yet tedious task that would greatly
benefit from automation. To accomplish this task, segmentation of cells need to
be accurate. In this paper, we have improved the learning of training data by
our network. It can annotate precise masks on test data. we examine the
strength of activation functions in medical image segmentation task by
improving learning rates by our proposed Carving Technique. Identifying the
cells nuclei is the starting point for most analyses, identifying nuclei allows
researchers to identify each individual cell in a sample, and by measuring how
cells react to various treatments, the researcher can understand the underlying
biological processes at work. Experimental results shows the efficiency of the
proposed work.","['cs.CV', 'cs.LG']"
Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,"Spatial pyramid pooling module or encode-decoder structure are used in deep
neural networks for semantic segmentation task. The former networks are able to
encode multi-scale contextual information by probing the incoming features with
filters or pooling operations at multiple rates and multiple effective
fields-of-view, while the latter networks can capture sharper object boundaries
by gradually recovering the spatial information. In this work, we propose to
combine the advantages from both methods. Specifically, our proposed model,
DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module
to refine the segmentation results especially along object boundaries. We
further explore the Xception model and apply the depthwise separable
convolution to both Atrous Spatial Pyramid Pooling and decoder modules,
resulting in a faster and stronger encoder-decoder network. We demonstrate the
effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,
achieving the test set performance of 89.0\% and 82.1\% without any
post-processing. Our paper is accompanied with a publicly available reference
implementation of the proposed models in Tensorflow at
\url{https://github.com/tensorflow/models/tree/master/research/deeplab}.",['cs.CV']
Multi-level Activation for Segmentation of Hierarchically-nested Classes,"For many biological image segmentation tasks, including topological
knowledge, such as the nesting of classes, can greatly improve results.
However, most `out-of-the-box' CNN models are still blind to such prior
information. In this paper, we propose a novel approach to encode this
information, through a multi-level activation layer and three compatible
losses. We benchmark all of them on nuclei segmentation in bright-field
microscopy cell images from the 2018 Data Science Bowl challenge, offering an
exemplary segmentation task with cells and nested subcellular structures. Our
scheme greatly speeds up learning, and outperforms standard multi-class
classification with soft-max activation and a previously proposed method
stemming from it, improving the Dice score significantly (p-values<0.007). Our
approach is conceptually simple, easy to implement and can be integrated in any
CNN architecture. It can be generalized to a higher number of classes, with or
without further relations of containment.",['cs.CV']
Weakly-Supervised Learning-Based Feature Localization in Confocal Laser Endomicroscopy Glioma Images,"Confocal Laser Endomicroscope (CLE) is a novel handheld fluorescence imaging
device that has shown promise for rapid intraoperative diagnosis of brain tumor
tissue. Currently CLE is capable of image display only and lacks an automatic
system to aid the surgeon in analyzing the images. The goal of this project was
to develop a computer-aided diagnostic approach for CLE imaging of human glioma
with feature localization function. Despite the tremendous progress in object
detection and image segmentation methods in recent years, most of such methods
require large annotated datasets for training. However, manual annotation of
thousands of histopathological images by physicians is costly and time
consuming. To overcome this problem, we propose a Weakly-Supervised Learning
(WSL)-based model for feature localization that trains on image-level
annotations, and then localizes incidences of a class-of-interest in the test
image. We developed a novel convolutional neural network for diagnostic
features localization from CLE images by employing a novel multiscale
activation map that is laterally inhibited and collaterally integrated. To
validate our method, we compared proposed model's output to the manual
annotation performed by four neurosurgeons on test images. Proposed model
achieved 88% mean accuracy and 86% mean intersection over union on intermediate
features and 87% mean accuracy and 88% mean intersection over union on
restrictive fine features, while outperforming other state of the art methods
tested. This system can improve accuracy and efficiency in characterization of
CLE images of glioma tissue during surgery, augment intraoperative
decision-making process regarding tumor margin and affect resection rates.",['cs.CV']
Concept Mask: Large-Scale Segmentation from Semantic Concepts,"Existing works on semantic segmentation typically consider a small number of
labels, ranging from tens to a few hundreds. With a large number of labels,
training and evaluation of such task become extremely challenging due to
correlation between labels and lack of datasets with complete annotations. We
formulate semantic segmentation as a problem of image segmentation given a
semantic concept, and propose a novel system which can potentially handle an
unlimited number of concepts, including objects, parts, stuff, and attributes.
We achieve this using a weakly and semi-supervised framework leveraging
multiple datasets with different levels of supervision. We first train a deep
neural network on a 6M stock image dataset with only image-level labels to
learn visual-semantic embedding on 18K concepts. Then, we refine and extend the
embedding network to predict an attention map, using a curated dataset with
bounding box annotations on 750 concepts. Finally, we train an attention-driven
class agnostic segmentation network using an 80-category fully annotated
dataset. We perform extensive experiments to validate that the proposed system
performs competitively to the state of the art on fully supervised concepts,
and is capable of producing accurate segmentations for weakly learned and
unseen concepts.",['cs.CV']
Holographic Visualisation of Radiology Data and Automated Machine Learning-based Medical Image Segmentation,"Within this thesis we propose a platform for combining Augmented Reality (AR)
hardware with machine learning in a user-oriented pipeline, offering to the
medical staff an intuitive 3D visualization of volumetric Computed Tomography
(CT) and Magnetic Resonance Imaging (MRI) medical image segmentations inside
the AR headset, that does not need human intervention for loading, processing
and segmentation of medical images. The AR visualization, based on Microsoft
HoloLens, employs a modular and thus scalable frontend-backend architecture for
real-time visualizations on multiple AR headsets. As Convolutional Neural
Networks (CNNs) have lastly demonstrated superior performance for the machine
learning task of image semantic segmentation, the pipeline also includes a
fully automated CNN algorithm for the segmentation of the liver from CT scans.
The model is based on the Deep Retinal Image Understanding (DRIU) model which
is a Fully Convolutional Network with side outputs from feature maps with
different resolution, extracted at different stages of the network. The
algorithm is 2.5D which means that the input is a set of consecutive scan
slices. The experiments have been performed on the Liver Tumor Segmentation
Challenge (LiTS) dataset for liver segmentation and demonstrated good results
and flexibility. While multiple approaches exist in the domain, only few of
them have focused on overcoming the practical aspects which still largely hold
this technology away from the operating rooms. In line with this, we also are
next planning an evaluation from medical doctors and radiologists in a
real-world environment.","['cs.CV', 'cs.LG']"
Unsupervised learning of foreground object detection,"Unsupervised learning poses one of the most difficult challenges in computer
vision today. The task has an immense practical value with many applications in
artificial intelligence and emerging technologies, as large quantities of
unlabeled videos can be collected at relatively low cost. In this paper, we
address the unsupervised learning problem in the context of detecting the main
foreground objects in single images. We train a student deep network to predict
the output of a teacher pathway that performs unsupervised object discovery in
videos or large image collections. Our approach is different from published
methods on unsupervised object discovery. We move the unsupervised learning
phase during training time, then at test time we apply the standard
feed-forward processing along the student pathway. This strategy has the
benefit of allowing increased generalization possibilities during training,
while remaining fast at testing. Our unsupervised learning algorithm can run
over several generations of student-teacher training. Thus, a group of student
networks trained in the first generation collectively create the teacher at the
next generation. In experiments our method achieves top results on three
current datasets for object discovery in video, unsupervised image segmentation
and saliency detection. At test time the proposed system is fast, being one to
two orders of magnitude faster than published unsupervised methods.",['cs.CV']
An Iterative Boundary Random Walks Algorithm for Interactive Image Segmentation,"The interactive image segmentation algorithm can provide an intelligent ways
to understand the intention of user input. Many interactive methods have the
problem of that ask for large number of user input. To efficient produce
intuitive segmentation under limited user input is important for industrial
application. In this paper, we reveal a positive feedback system on image
segmentation to show the pixels of self-learning. Two approaches, iterative
random walks and boundary random walks, are proposed for segmentation
potential, which is the key step in feedback system. Experiment results on
image segmentation indicates that proposed algorithms can obtain more efficient
input to random walks. And higher segmentation performance can be obtained by
applying the iterative boundary random walks algorithm.",['cs.CV']
Dilated Convolutions in Neural Networks for Left Atrial Segmentation in 3D Gadolinium Enhanced-MRI,"Segmentation of the left atrial chamber and assessing its morphology, are
essential for improving our understanding of atrial fibrillation, the most
common type of cardiac arrhythmia. Automation of this process in 3D gadolinium
enhanced-MRI (GE-MRI) data is desirable, as manual delineation is
time-consuming, challenging and observer-dependent. Recently, deep
convolutional neural networks (CNNs) have gained tremendous traction and
achieved state-of-the-art results in medical image segmentation. However, it is
difficult to incorporate local and global information without using contracting
(pooling) layers, which in turn reduces segmentation accuracy for smaller
structures. In this paper, we propose a 3D CNN for volumetric segmentation of
the left atrial chamber in LGE-MRI. Our network is based on the well known
U-Net architecture. We employ a 3D fully convolutional network, with dilated
convolutions in the lowest level of the network, and residual connections
between encoder blocks to incorporate local and global knowledge. The results
show that including global context through the use of dilated convolutions,
helps in domain adaptation, and the overall segmentation accuracy is improved
in comparison to a 3D U-Net.",['cs.CV']
A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation,"In this paper, we adopt 3D Convolutional Neural Networks to segment
volumetric medical images. Although deep neural networks have been proven to be
very effective on many 2D vision tasks, it is still challenging to apply them
to 3D tasks due to the limited amount of annotated 3D data and limited
computational resources. We propose a novel 3D-based coarse-to-fine framework
to effectively and efficiently tackle these challenges. The proposed 3D-based
framework outperforms the 2D counterpart to a large margin since it can
leverage the rich spatial infor- mation along all three axes. We conduct
experiments on two datasets which include healthy and pathological pancreases
respectively, and achieve the current state-of-the-art in terms of
Dice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset,
we outperform the previous best by an average of over 2%, and the worst case is
improved by 7% to reach almost 70%, which indicates the reliability of our
framework in clinical applications.",['cs.CV']
Neural Multi-Atlas Label Fusion: Application to Cardiac MR Images,"Multi-atlas segmentation approach is one of the most widely-used image
segmentation techniques in biomedical applications. There are two major
challenges in this category of methods, i.e., atlas selection and label fusion.
In this paper, we propose a novel multi-atlas segmentation method that
formulates multi-atlas segmentation in a deep learning framework for better
solving these challenges. The proposed method, dubbed deep fusion net (DFN), is
a deep architecture that integrates a feature extraction subnet and a non-local
patch-based label fusion (NL-PLF) subnet in a single network. The network
parameters are learned by end-to-end training for automatically learning deep
features that enable optimal performance in a NL-PLF framework. The learned
deep features are further utilized in defining a similarity measure for atlas
selection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09
for left ventricle segmentation, our approach achieved 0.833 in averaged Dice
metric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on
LV-09 dataset, comparing favorably with the other automatic left ventricle
segmentation methods. We also tested our approach on Cardiac Atlas Project
(CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method
achieved 0.815 in ADM, ranking highest at the time of writing.",['cs.CV']
A Network Structure to Explicitly Reduce Confusion Errors in Semantic Segmentation,"Confusing classes that are ubiquitous in real world often degrade performance
for many vision related applications like object detection, classification, and
segmentation. The confusion errors are not only caused by similar visual
patterns but also amplified by various factors during the training of our
designed models, such as reduced feature resolution in the encoding process or
imbalanced data distributions. A large amount of deep learning based network
structures has been proposed in recent years to deal with these individual
factors and improve network performance. However, to our knowledge, no existing
work in semantic image segmentation is designed to tackle confusion errors
explicitly. In this paper, we present a novel and general network structure
that reduces confusion errors in more direct manner and apply the network for
semantic segmentation. There are two major contributions in our network
structure: 1) We ensemble subnets with heterogeneous output spaces based on the
discriminative confusing groups. The training for each subnet can distinguish
confusing classes within the group without affecting unrelated classes outside
the group. 2) We propose an improved cross-entropy loss function that maximizes
the probability assigned to the correct class and penalizes the probabilities
assigned to the confusing classes at the same time. Our network structure is a
general structure and can be easily adapted to any other networks to further
reduce confusion errors. Without any changes in the feature encoder and
post-processing steps, our experiments demonstrate consistent and significant
improvements on different baseline models on Cityscapes and PASCAL VOC datasets
(e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38).",['cs.CV']
Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation,"Semantic Segmentation using deep convolutional neural network pose more
complex challenge for any GPU intensive task. As it has to compute million of
parameters, it results to huge memory consumption. Moreover, extracting finer
features and conducting supervised training tends to increase the complexity.
With the introduction of Fully Convolutional Neural Network, which uses finer
strides and utilizes deconvolutional layers for upsampling, it has been a go to
for any image segmentation task. In this paper, we propose two segmentation
architecture which not only needs one-third the parameters to compute but also
gives better accuracy than the similar architectures. The model weights were
transferred from the popular neural net like VGG19 and VGG16 which were trained
on Imagenet classification data-set. Then we transform all the fully connected
layers to convolutional layers and use dilated convolution for decreasing the
parameters. Lastly, we add finer strides and attach four skip architectures
which are element-wise summed with the deconvolutional layers in steps. We
train and test on different sparse and fine data-sets like Pascal VOC2012,
Pascal-Context and NYUDv2 and show how better our model performs in this tasks.
On the other hand our model has a faster inference time and consumes less
memory for training and testing on NVIDIA Pascal GPUs, making it more efficient
and less memory consuming architecture for pixel-wise segmentation.",['cs.CV']
A multi-contrast MRI approach to thalamus segmentation,"Thalamic alterations are relevant to many neurological disorders including
Alzheimer's disease, Parkinson's disease and multiple sclerosis. Routine
interventions to improve symptom severity in movement disorders, for example,
often consist of surgery or deep brain stimulation to diencephalic nuclei.
Therefore, accurate delineation of grey matter thalamic subregions is of the
upmost clinical importance. MRI is highly appropriate for structural
segmentation as it provides different views of the anatomy from a single
scanning session. Though with several contrasts potentially available, it is
also of increasing importance to develop new image segmentation techniques that
can operate multi-spectrally. We hereby propose a new segmentation method for
use with multi-modality data, which we evaluated for automated segmentation of
major thalamic subnuclear groups using T1-, T2*-weighted and quantitative
susceptibility mapping (QSM) information. The proposed method consists of four
steps: highly iterative image co-registration, manual segmentation on the
average training-data template, supervised learning for pattern recognition,
and a final convex optimisation step imposing further spatial constraints to
refine the solution. This led to solutions in greater agreement with manual
segmentation than the standard Morel atlas based approach. Furthermore, we show
that the multi-contrast approach boosts segmentation performances. We then
investigated whether prior knowledge using the training-template contours could
further improve convex segmentation accuracy and robustness, which led to
highly precise multi-contrast segmentations in single subjects. This approach
can be extended to most 3D imaging data types and any region of interest
discernible in single scans or multi-subject templates.","['cs.CV', 'math.NA']"
A Data-driven Prior on Facet Orientation for Semantic Mesh Labeling,"Mesh labeling is the key problem of classifying the facets of a 3D mesh with
a label among a set of possible ones. State-of-the-art methods model mesh
labeling as a Markov Random Field over the facets. These algorithms map image
segmentations to the mesh by minimizing an energy function that comprises a
data term, a smoothness terms, and class-specific priors. The latter favor a
labeling with respect to another depending on the orientation of the facet
normals. In this paper we propose a novel energy term that acts as a prior, but
does not require any prior knowledge about the scene nor scene-specific
relationship among classes. It bootstraps from a coarse mapping of the 2D
segmentations on the mesh, and it favors the facets to be labeled according to
the statistics of the mesh normals in their neighborhood. We tested our
approach against five different datasets and, even if we do not inject prior
knowledge, our method adapts to the data and overcomes the state-of-the-art.",['cs.CV']
Feature Fusion through Multitask CNN for Large-scale Remote Sensing Image Segmentation,"In recent years, Fully Convolutional Networks (FCN) has been widely used in
various semantic segmentation tasks, including multi-modal remote sensing
imagery. How to fuse multi-modal data to improve the segmentation performance
has always been a research hotspot. In this paper, a novel end-toend fully
convolutional neural network is proposed for semantic segmentation of natural
color, infrared imagery and Digital Surface Models (DSM). It is based on a
modified DeepUNet and perform the segmentation in a multi-task way. The
channels are clustered into groups and processed on different task pipelines.
After a series of segmentation and fusion, their shared features and private
features are successfully merged together. Experiment results show that the
feature fusion network is efficient. And our approach achieves good performance
in ISPRS Semantic Labeling Contest (2D).",['cs.CV']
Skin Lesion Segmentation Using Atrous Convolution via DeepLab v3,"As melanoma diagnoses increase across the US, automated efforts to identify
malignant lesions become increasingly of interest to the research community.
Segmentation of dermoscopic images is the first step in this process, thus
accuracy is crucial. Although techniques utilizing convolutional neural
networks have been used in the past for lesion segmentation, we present a
solution employing the recently published DeepLab 3, an atrous convolution
method for image segmentation. Although the results produced by this run are
not ideal, with a mean Jaccard index of 0.498, we believe that with further
adjustments and modifications to the compatibility with the DeepLab code and
with training on more powerful processing units, this method may achieve better
results in future trials.",['cs.CV']
Clearing noisy annotations for computed tomography imaging,"One of the problems on the way to successful implementation of neural
networks is the quality of annotation. For instance, different annotators can
annotate images in a different way and very often their decisions do not match
exactly and in extreme cases are even mutually exclusive which results in noisy
annotations and, consequently, inaccurate predictions.
  To avoid that problem in the task of computed tomography (CT) imaging
segmentation we propose a clearing algorithm for annotations. It consists of 3
stages:
  - annotators scoring, which assigns a higher confidence level to better
annotators;
  - nodules scoring, which assigns a higher confidence level to nodules
confirmed by good annotators;
  - nodules merging, which aggregates annotations according to nodules
confidence.
  In general, the algorithm can be applied to many different tasks (namely,
binary and multi-class semantic segmentation, and also with trivial adjustments
to classification and regression) where there are several annotators labeling
each image.","['cs.CV', 'cs.LG', 'stat.ML']"
A post-processing method to improve the white matter hyperintensity segmentation accuracy for randomly-initialized U-net,"White matter hyperintensity (WMH) is commonly found in elder individuals and
appears to be associated with brain diseases. U-net is a convolutional network
that has been widely used for biomedical image segmentation. Recently, U-net
has been successfully applied to WMH segmentation. Random initialization is
usally used to initialize the model weights in the U-net. However, the model
may coverage to different local optima with different randomly initialized
weights. We find a combination of thresholding and averaging the outputs of
U-nets with different random initializations can largely improve the WMH
segmentation accuracy. Based on this observation, we propose a post-processing
technique concerning the way how averaging and thresholding are conducted.
Specifically, we first transfer the score maps from three U-nets to binary
masks via thresholding and then average those binary masks to obtain the final
WMH segmentation. Both quantitative analysis (via the Dice similarity
coefficient) and qualitative analysis (via visual examinations) reveal the
superior performance of the proposed method. This post-processing technique is
independent of the model used. As such, it can also be applied to situations
where other deep learning models are employed, especially when random
initialization is adopted and pre-training is unavailable.","['cs.CV', 'cs.LG', 'stat.ML']"
Segmentation of Drosophila Heart in Optical Coherence Microscopy Images Using Convolutional Neural Networks,"Convolutional neural networks are powerful tools for image segmentation and
classification. Here, we use this method to identify and mark the heart region
of Drosophila at different developmental stages in the cross-sectional images
acquired by a custom optical coherence microscopy (OCM) system. With our
well-trained convolutional neural network model, the heart regions through
multiple heartbeat cycles can be marked with an intersection over union (IOU)
of ~86%. Various morphological and dynamical cardiac parameters can be
quantified accurately with automatically segmented heart regions. This study
demonstrates an efficient heart segmentation method to analyze OCM images of
the beating heart in Drosophila.",['cs.CV']
Automatically Designing CNN Architectures for Medical Image Segmentation,"Deep neural network architectures have traditionally been designed and
explored with human expertise in a long-lasting trial-and-error process. This
process requires huge amount of time, expertise, and resources. To address this
tedious problem, we propose a novel algorithm to optimally find hyperparameters
of a deep network architecture automatically. We specifically focus on
designing neural architectures for medical image segmentation task. Our
proposed method is based on a policy gradient reinforcement learning for which
the reward function is assigned a segmentation evaluation utility (i.e., dice
index). We show the efficacy of the proposed method with its low computational
cost in comparison with the state-of-the-art medical image segmentation
networks. We also present a new architecture design, a densely connected
encoder-decoder CNN, as a strong baseline architecture to apply the proposed
hyperparameter search algorithm. We apply the proposed algorithm to each layer
of the baseline architectures. As an application, we train the proposed system
on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC)
MICCAI 2017. Starting from a baseline segmentation architecture, the resulting
network architecture obtains the state-of-the-art results in accuracy without
performing any trial-and-error based architecture design approaches or close
supervision of the hyperparameters changes.","['stat.ML', 'cs.CV', 'cs.LG']"
Conditional Random Fields as Recurrent Neural Networks for 3D Medical Imaging Segmentation,"The Conditional Random Field as a Recurrent Neural Network layer is a
recently proposed algorithm meant to be placed on top of an existing
Fully-Convolutional Neural Network to improve the quality of semantic
segmentation. In this paper, we test whether this algorithm, which was shown to
improve semantic segmentation for 2D RGB images, is able to improve
segmentation quality for 3D multi-modal medical images. We developed an
implementation of the algorithm which works for any number of spatial
dimensions, input/output image channels, and reference image channels. As far
as we know this is the first publicly available implementation of this sort. We
tested the algorithm with two distinct 3D medical imaging datasets, we
concluded that the performance differences observed were not statistically
significant. Finally, in the discussion section of the paper, we go into the
reasons as to why this technique transfers poorly from natural images to
medical images.",['cs.CV']
UNet++: A Nested U-Net Architecture for Medical Image Segmentation,"In this paper, we present UNet++, a new, more powerful architecture for
medical image segmentation. Our architecture is essentially a deeply-supervised
encoder-decoder network where the encoder and decoder sub-networks are
connected through a series of nested, dense skip pathways. The re-designed skip
pathways aim at reducing the semantic gap between the feature maps of the
encoder and decoder sub-networks. We argue that the optimizer would deal with
an easier learning task when the feature maps from the decoder and encoder
networks are semantically similar. We have evaluated UNet++ in comparison with
U-Net and wide U-Net architectures across multiple medical image segmentation
tasks: nodule segmentation in the low-dose CT scans of chest, nuclei
segmentation in the microscopy images, liver segmentation in abdominal CT
scans, and polyp segmentation in colonoscopy videos. Our experiments
demonstrate that UNet++ with deep supervision achieves an average IoU gain of
3.9 and 3.4 points over U-Net and wide U-Net, respectively.","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
Cross Pixel Optical Flow Similarity for Self-Supervised Learning,"We propose a novel method for learning convolutional neural image
representations without manual supervision. We use motion cues in the form of
optical flow, to supervise representations of static images. The obvious
approach of training a network to predict flow from a single image can be
needlessly difficult due to intrinsic ambiguities in this prediction task. We
instead propose a much simpler learning goal: embed pixels such that the
similarity between their embeddings matches that between their optical flow
vectors. At test time, the learned deep network can be used without access to
video or flow information and transferred to tasks such as image
classification, detection, and segmentation. Our method, which significantly
simplifies previous attempts at using motion for self-supervision, achieves
state-of-the-art results in self-supervision using motion cues, competitive
results for self-supervision in general, and is overall state of the art in
self-supervised pretraining for semantic image segmentation, as demonstrated on
standard benchmarks.","['cs.CV', 'cs.LG', 'cs.NE', '68T45']"
Near Real-time Hippocampus Segmentation Using Patch-based Canonical Neural Network,"Over the past decades, state-of-the-art medical image segmentation has
heavily rested on signal processing paradigms, most notably registration-based
label propagation and pair-wise patch comparison, which are generally slow
despite a high segmentation accuracy. In recent years, deep learning has
revolutionalized computer vision with many practices outperforming prior art,
in particular the convolutional neural network (CNN) studies on image
classification. Deep CNN has also started being applied to medical image
segmentation lately, but generally involves long training and demanding memory
requirements, achieving limited success. We propose a patch-based deep learning
framework based on a revisit to the classic neural network model with
substantial modernization, including the use of Rectified Linear Unit (ReLU)
activation, dropout layers, 2.5D tri-planar patch multi-pathway settings. In a
test application to hippocampus segmentation using 100 brain MR images from the
ADNI database, our approach significantly outperformed prior art in terms of
both segmentation accuracy and speed: scoring a median Dice score up to 90.98%
on a near real-time performance (<1s).",['cs.CV']
Learning to Segment Medical Images with Scribble-Supervision Alone,"Semantic segmentation of medical images is a crucial step for the
quantification of healthy anatomy and diseases alike. The majority of the
current state-of-the-art segmentation algorithms are based on deep neural
networks and rely on large datasets with full pixel-wise annotations. Producing
such annotations can often only be done by medical professionals and requires
large amounts of valuable time. Training a medical image segmentation network
with weak annotations remains a relatively unexplored topic. In this work we
investigate training strategies to learn the parameters of a pixel-wise
segmentation network from scribble annotations alone. We evaluate the
techniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentation
datasets. We find that the networks trained on scribbles suffer from a
remarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate)
with respect to a network trained on full annotations.",['cs.CV']
Sem-GAN: Semantically-Consistent Image-to-Image Translation,"Unpaired image-to-image translation is the problem of mapping an image in the
source domain to one in the target domain, without requiring corresponding
image pairs. To ensure the translated images are realistically plausible,
recent works, such as Cycle-GAN, demands this mapping to be invertible. While,
this requirement demonstrates promising results when the domains are unimodal,
its performance is unpredictable in a multi-modal scenario such as in an image
segmentation task. This is because, invertibility does not necessarily enforce
semantic correctness. To this end, we present a semantically-consistent GAN
framework, dubbed Sem-GAN, in which the semantics are defined by the class
identities of image segments in the source domain as produced by a semantic
segmentation algorithm. Our proposed framework includes consistency constraints
on the translation task that, together with the GAN loss and the
cycle-constraints, enforces that the images when translated will inherit the
appearances of the target domain, while (approximately) maintaining their
identities from the source domain. We present experiments on several
image-to-image translation tasks and demonstrate that Sem-GAN improves the
quality of the translated images significantly, sometimes by more than 20% on
the FCN score. Further, we show that semantic segmentation models, trained with
synthetic images translated via Sem-GAN, leads to significantly better
segmentation results than other variants.",['cs.CV']
"Deep Learning Hyperspectral Image Classification Using Multiple Class-based Denoising Autoencoders, Mixed Pixel Training Augmentation, and Morphological Operations","Herein, we present a system for hyperspectral image segmentation that
utilizes multiple class--based denoising autoencoders which are efficiently
trained. Moreover, we present a novel hyperspectral data augmentation method
for labelled HSI data using linear mixtures of pixels from each class, which
helps the system with edge pixels which are almost always mixed pixels.
Finally, we utilize a deep neural network and morphological hole-filling to
provide robust image classification. Results run on the Salinas dataset verify
the high performance of the proposed algorithm.","['cs.CV', 'cs.LG', 'stat.ML']"
Data-Driven Segmentation of Post-mortem Iris Images,"This paper presents a method for segmenting iris images obtained from the
deceased subjects, by training a deep convolutional neural network (DCNN)
designed for the purpose of semantic segmentation. Post-mortem iris recognition
has recently emerged as an alternative, or additional, method useful in
forensic analysis. At the same time it poses many new challenges from the
technological standpoint, one of them being the image segmentation stage, which
has proven difficult to be reliably executed by conventional iris recognition
methods. Our approach is based on the SegNet architecture, fine-tuned with
1,300 manually segmented post-mortem iris images taken from the
Warsaw-BioBase-Post-Mortem-Iris v1.0 database. The experiments presented in
this paper show that this data-driven solution is able to learn specific
deformations present in post-mortem samples, which are missing from alive
irises, and offers a considerable improvement over the state-of-the-art,
conventional segmentation algorithm (OSIRIS): the Intersection over Union (IoU)
metric was improved from 73.6% (for OSIRIS) to 83% (for DCNN-based presented in
this paper) averaged over subject-disjoint, multiple splits of the data into
train and test subsets. This paper offers the first known to us method of
automatic processing of post-mortem iris images. We offer source codes with the
trained DCNN that perform end-to-end segmentation of post-mortem iris images,
as described in this paper. Also, we offer binary masks corresponding to manual
segmentation of samples from Warsaw-BioBase-Post-Mortem-Iris v1.0 database to
facilitate development of alternative methods for post-mortem iris
segmentation.",['cs.CV']
"Efficient identification, localization and quantification of grapevine inflorescences in unprepared field images using Fully Convolutional Networks","Yield and its prediction is one of the most important tasks in grapevine
breeding purposes and vineyard management. Commonly, this trait is estimated
manually right before harvest by extrapolation, which mostly is
labor-intensive, destructive and inaccurate. In the present study an automated
image-based workflow was developed quantifying inflorescences and single
flowers in unprepared field images of grapevines, i.e. no artificial background
or light was applied. It is a novel approach for non-invasive, inexpensive and
objective phenotyping with high-throughput.
  First, image regions depicting inflorescences were identified and localized.
This was done by segmenting the images into the classes ""inflorescence"" and
""non-inflorescence"" using a Fully Convolutional Network (FCN). Efficient image
segmentation hereby is the most challenging step regarding the small geometry
and dense distribution of flowers (several hundred flowers per inflorescence),
similar color of all plant organs in the fore- and background as well as the
circumstance that only approximately 5% of an image show inflorescences. The
trained FCN achieved a mean Intersection Over Union (IOU) of 87.6% on the test
data set. Finally, individual flowers were extracted from the
""inflorescence""-areas using Circular Hough Transform. The flower extraction
achieved a recall of 80.3% and a precision of 70.7% using the segmentation
derived by the trained FCN model.
  Summarized, the presented approach is a promising strategy in order to
predict yield potential automatically in the earliest stage of grapevine
development which is applicable for objective monitoring and evaluations of
breeding material, genetic repositories or commercial vineyards.",['cs.CV']
Boosted Training of Convolutional Neural Networks for Multi-Class Segmentation,"Training deep neural networks on large and sparse datasets is still
challenging and can require large amounts of computation and memory. In this
work, we address the task of performing semantic segmentation on large
volumetric data sets, such as CT scans. Our contribution is threefold: 1) We
propose a boosted sampling scheme that uses a-posterior error maps, generated
throughout training, to focus sampling on difficult regions, resulting in a
more informative loss. This results in a significant training speed up and
improves learning performance for image segmentation. 2) We propose a novel
algorithm for boosting the SGD learning rate schedule by adaptively increasing
and lowering the learning rate, avoiding the need for extensive hyperparameter
tuning. 3) We show that our method is able to attain new state-of-the-art
results on the VISCERAL Anatomy benchmark.",['cs.CV']
Visual Explanations From Deep 3D Convolutional Neural Networks for Alzheimer's Disease Classification,"We develop three efficient approaches for generating visual explanations from
3D convolutional neural networks (3D-CNNs) for Alzheimer's disease
classification. One approach conducts sensitivity analysis on hierarchical 3D
image segmentation, and the other two visualize network activations on a
spatial map. Visual checks and a quantitative localization benchmark indicate
that all approaches identify important brain parts for Alzheimer's disease
diagnosis. Comparative analysis show that the sensitivity analysis based
approach has difficulty handling loosely distributed cerebral cortex, and
approaches based on visualization of activations are constrained by the
resolution of the convolutional layer. The complementarity of these methods
improves the understanding of 3D-CNNs in Alzheimer's disease classification
from different perspectives.","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']"
H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes,"Liver cancer is one of the leading causes of cancer death. To assist doctors
in hepatocellular carcinoma diagnosis and treatment planning, an accurate and
automatic liver and tumor segmentation method is highly demanded in the
clinical practice. Recently, fully convolutional neural networks (FCNs),
including 2D and 3D FCNs, serve as the back-bone in many volumetric image
segmentation. However, 2D convolutions can not fully leverage the spatial
information along the third dimension while 3D convolutions suffer from high
computational cost and GPU memory consumption. To address these issues, we
propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of
a 2D DenseUNet for efficiently extracting intra-slice features and a 3D
counterpart for hierarchically aggregating volumetric contexts under the spirit
of the auto-context algorithm for liver and tumor segmentation. We formulate
the learning process of H-DenseUNet in an end-to-end manner, where the
intra-slice representations and inter-slice features can be jointly optimized
through a hybrid feature fusion (HFF) layer. We extensively evaluated our
method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge
and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the
segmentation results of tumors and achieved very competitive performance for
liver segmentation even with a single model.",['cs.CV']
SynNet: Structure-Preserving Fully Convolutional Networks for Medical Image Synthesis,"Cross modal image syntheses is gaining significant interests for its ability
to estimate target images of a different modality from a given set of source
images,like estimating MR to MR, MR to CT, CT to PET etc, without the need for
an actual acquisition.Though they show potential for applications in radiation
therapy planning,image super resolution, atlas construction, image segmentation
etc.The synthesis results are not as accurate as the actual acquisition.In this
paper,we address the problem of multi modal image synthesis by proposing a
fully convolutional deep learning architecture called the SynNet.We extend the
proposed architecture for various input output configurations. And finally, we
propose a structure preserving custom loss function for cross-modal image
synthesis.We validate the proposed SynNet and its extended framework on BRATS
dataset with comparisons against three state-of-the art methods.And the results
of the proposed custom loss function is validated against the traditional loss
function used by the state-of-the-art methods for cross modal image synthesis.",['cs.CV']
Function Norms and Regularization in Deep Networks,"Deep neural networks (DNNs) have become increasingly important due to their
excellent empirical performance on a wide range of problems. However,
regularization is generally achieved by indirect means, largely due to the
complex set of functions defined by a network and the difficulty in measuring
function complexity. There exists no method in the literature for additive
regularization based on a norm of the function, as is classically considered in
statistical learning theory. In this work, we propose sampling-based
approximations to weighted function norms as regularizers for deep neural
networks. We provide, to the best of our knowledge, the first proof in the
literature of the NP-hardness of computing function norms of DNNs, motivating
the necessity of an approximate approach. We then derive a generalization bound
for functions trained with weighted norms and prove that a natural stochastic
optimization strategy minimizes the bound. Finally, we empirically validate the
improved performance of the proposed regularization strategies for both convex
function sets as well as DNNs on real-world classification and image
segmentation tasks demonstrating improved performance over weight decay,
dropout, and batch normalization. Source code will be released at the time of
publication.","['cs.LG', 'stat.ML']"
Combining Pyramid Pooling and Attention Mechanism for Pelvic MR Image Semantic Segmentaion,"One of the time-consuming routine work for a radiologist is to discern
anatomical structures from tomographic images. For assisting radiologists, this
paper develops an automatic segmentation method for pelvic magnetic resonance
(MR) images. The task has three major challenges 1) A pelvic organ can have
various sizes and shapes depending on the axial image, which requires local
contexts to segment correctly. 2) Different organs often have quite similar
appearance in MR images, which requires global context to segment. 3) The
number of available annotated images are very small to use the latest
segmentation algorithms. To address the challenges, we propose a novel
convolutional neural network called Attention-Pyramid network (APNet) that
effectively exploits both local and global contexts, in addition to a
data-augmentation technique that is particularly effective for MR images. In
order to evaluate our method, we construct fine-grained (50 pelvic organs) MR
image segmentation dataset, and experimentally confirm the superior performance
of our techniques over the state-of-the-art image segmentation methods.",['cs.CV']
Disparity Image Segmentation For ADAS,"We present a simple solution for segmenting grayscale images using existing
Connected Component Labeling (CCL) algorithms (which are generally applied to
binary images), which was efficient enough to be implemented in a constrained
(embedded automotive) architecture. Our solution customizes the region growing
and merging approach, and is primarily targeted for stereoscopic disparity
images where nearer objects carry more relevance. We provide results from a
standard OpenCV implementation for some basic cases and an image from the
Tsukuba stereo-pair dataset.","['cs.CV', '68U10, 68W99', 'I.4.6; I.4.5']"
Keypoint Transfer for Fast Whole-Body Segmentation,"We introduce an approach for image segmentation based on sparse
correspondences between keypoints in testing and training images. Keypoints
represent automatically identified distinctive image locations, where each
keypoint correspondence suggests a transformation between images. We use these
correspondences to transfer label maps of entire organs from the training
images to the test image. The keypoint transfer algorithm includes three steps:
(i) keypoint matching, (ii) voting-based keypoint labeling, and (iii)
keypoint-based probabilistic transfer of organ segmentations. We report
segmentation results for abdominal organs in whole-body CT and MRI, as well as
in contrast-enhanced CT and MRI. Our method offers a speed-up of about three
orders of magnitude in comparison to common multi-atlas segmentation, while
achieving an accuracy that compares favorably. Moreover, keypoint transfer does
not require the registration to an atlas or a training phase. Finally, the
method allows for the segmentation of scans with highly variable field-of-view.",['cs.CV']
Towards safe deep learning: accurately quantifying biomarker uncertainty in neural network predictions,"Automated medical image segmentation, specifically using deep learning, has
shown outstanding performance in semantic segmentation tasks. However, these
methods rarely quantify their uncertainty, which may lead to errors in
downstream analysis. In this work we propose to use Bayesian neural networks to
quantify uncertainty within the domain of semantic segmentation. We also
propose a method to convert voxel-wise segmentation uncertainty into volumetric
uncertainty, and calibrate the accuracy and reliability of confidence intervals
of derived measurements. When applied to a tumour volume estimation
application, we demonstrate that by using such modelling of uncertainty, deep
learning systems can be made to report volume estimates with well-calibrated
error-bars, making them safer for clinical use. We also show that the
uncertainty estimates extrapolate to unseen data, and that the confidence
intervals are robust in the presence of artificial noise. This could be used to
provide a form of quality control and quality assurance, and may permit further
adoption of deep learning tools in the clinic.",['cs.CV']
Histological images segmentation of mucous glands,"Mucous glands lesions analysis and assessing of malignant potential of colon
polyps are very important tasks of surgical pathology. However, differential
diagnosis of colon polyps often seems impossible by classical methods and it is
necessary to involve computer methods capable of assessing minimal differences
to extend the capabilities of the classical pathology examination. Accurate
segmentation of mucous glands from histology images is a crucial step to obtain
reliable morphometric criteria for quantitative diagnostic methods. We review
major trends in histological images segmentation and design a new convolutional
neural network for mucous gland segmentation.",['cs.CV']
Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model,"Myocardial contrast echocardiography (MCE) is an imaging technique that
assesses left ventricle function and myocardial perfusion for the detection of
coronary artery diseases. Automatic MCE perfusion quantification is challenging
and requires accurate segmentation of the myocardium from noisy and
time-varying images. Random forests (RF) have been successfully applied to many
medical image segmentation tasks. However, the pixel-wise RF classifier ignores
contextual relationships between label outputs of individual pixels. RF which
only utilizes local appearance features is also susceptible to data suffering
from large intensity variations. In this paper, we demonstrate how to overcome
the above limitations of classic RF by presenting a fully automatic
segmentation pipeline for myocardial segmentation in full-cycle 2D MCE data.
Specifically, a statistical shape model is used to provide shape prior
information that guide the RF segmentation in two ways. First, a novel shape
model (SM) feature is incorporated into the RF framework to generate a more
accurate RF probability map. Second, the shape model is fitted to the RF
probability map to refine and constrain the final segmentation to plausible
myocardial shapes. We further improve the performance by introducing a bounding
box detection algorithm as a preprocessing step in the segmentation pipeline.
Our approach on 2D image is further extended to 2D+t sequence which ensures
temporal consistency in the resultant sequence segmentations. When evaluated on
clinical MCE data, our proposed method achieves notable improvement in
segmentation accuracy and outperforms other state-of-the-art methods including
the classic RF and its variants, active shape model and image registration.",['cs.CV']
Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical Image Segmentations with Adversarial Loss,"Convolutional networks (ConvNets) have achieved great successes in various
challenging vision tasks. However, the performance of ConvNets would degrade
when encountering the domain shift. The domain adaptation is more significant
while challenging in the field of biomedical image analysis, where
cross-modality data have largely different distributions. Given that annotating
the medical data is especially expensive, the supervised transfer learning
approaches are not quite optimal. In this paper, we propose an unsupervised
domain adaptation framework with adversarial learning for cross-modality
biomedical image segmentations. Specifically, our model is based on a dilated
fully convolutional network for pixel-wise prediction. Moreover, we build a
plug-and-play domain adaptation module (DAM) to map the target input to
features which are aligned with source domain feature space. A domain critic
module (DCM) is set up for discriminating the feature space of both domains. We
optimize the DAM and DCM via an adversarial loss without using any target
domain label. Our proposed method is validated by adapting a ConvNet trained
with MRI images to unpaired CT data for cardiac structures segmentations, and
achieved very promising results.",['cs.CV']
Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data,"Learning inter-domain mappings from unpaired data can improve performance in
structured prediction tasks, such as image segmentation, by reducing the need
for paired data. CycleGAN was recently proposed for this problem, but
critically assumes the underlying inter-domain mapping is approximately
deterministic and one-to-one. This assumption renders the model ineffective for
tasks requiring flexible, many-to-many mappings. We propose a new model, called
Augmented CycleGAN, which learns many-to-many mappings between domains. We
examine Augmented CycleGAN qualitatively and quantitatively on several image
datasets.",['cs.LG']
Kid-Net: Convolution Networks for Kidney Vessels Segmentation from CT-Volumes,"Semantic image segmentation plays an important role in modeling
patient-specific anatomy. We propose a convolution neural network, called
Kid-Net, along with a training schema to segment kidney vessels: artery, vein
and collecting system. Such segmentation is vital during the surgical planning
phase in which medical decisions are made before surgical incision. Our main
contribution is developing a training schema that handles unbalanced data,
reduces false positives and enables high-resolution segmentation with a limited
memory budget. These objectives are attained using dynamic weighting, random
sampling and 3D patch segmentation. Manual medical image annotation is both
time-consuming and expensive. Kid-Net reduces kidney vessels segmentation time
from matter of hours to minutes. It is trained end-to-end using 3D patches from
volumetric CT-images. A complete segmentation for a 512x512x512 CT-volume is
obtained within a few minutes (1-2 mins) by stitching the output 3D patches
together. Feature down-sampling and up-sampling are utilized to achieve higher
classification and localization accuracies. Quantitative and qualitative
evaluation results on a challenging testing dataset show Kid-Net competence.",['cs.CV']
CompNet: Complementary Segmentation Network for Brain MRI Extraction,"Brain extraction is a fundamental step for most brain imaging studies. In
this paper, we investigate the problem of skull stripping and propose
complementary segmentation networks (CompNets) to accurately extract the brain
from T1-weighted MRI scans, for both normal and pathological brain images. The
proposed networks are designed in the framework of encoder-decoder networks and
have two pathways to learn features from both the brain tissue and its
complementary part located outside of the brain. The complementary pathway
extracts the features in the non-brain region and leads to a robust solution to
brain extraction from MRIs with pathologies, which do not exist in our training
dataset. We demonstrate the effectiveness of our networks by evaluating them on
the OASIS dataset, resulting in the state of the art performance under the
two-fold cross-validation setting. Moreover, the robustness of our networks is
verified by testing on images with introduced pathologies and by showing its
invariance to unseen brain pathologies. In addition, our complementary network
design is general and can be extended to address other image segmentation
problems with better generalization.",['cs.CV']
Real-time Prediction of Segmentation Quality,"Recent advances in deep learning based image segmentation methods have
enabled real-time performance with human-level accuracy. However, occasionally
even the best method fails due to low image quality, artifacts or unexpected
behaviour of black box algorithms. Being able to predict segmentation quality
in the absence of ground truth is of paramount importance in clinical practice,
but also in large-scale studies to avoid the inclusion of invalid data in
subsequent analysis.
  In this work, we propose two approaches of real-time automated quality
control for cardiovascular MR segmentations using deep learning. First, we
train a neural network on 12,880 samples to predict Dice Similarity
Coefficients (DSC) on a per-case basis. We report a mean average error (MAE) of
0.03 on 1,610 test samples and 97% binary classification accuracy for
separating low and high quality segmentations. Secondly, in the scenario where
no manually annotated data is available, we train a network to predict DSC
scores from estimated quality obtained via a reverse testing strategy. We
report an MAE=0.14 and 91% binary classification accuracy for this case.
Predictions are obtained in real-time which, when combined with real-time
segmentation methods, enables instant feedback on whether an acquired scan is
analysable while the patient is still in the scanner. This further enables new
applications of optimising image acquisition towards best possible analysis
results.",['cs.CV']
Task Driven Generative Modeling for Unsupervised Domain Adaptation: Application to X-ray Image Segmentation,"Automatic parsing of anatomical objects in X-ray images is critical to many
clinical applications in particular towards image-guided invention and workflow
automation. Existing deep network models require a large amount of labeled
data. However, obtaining accurate pixel-wise labeling in X-ray images relies
heavily on skilled clinicians due to the large overlaps of anatomy and the
complex texture patterns. On the other hand, organs in 3D CT scans preserve
clearer structures as well as sharper boundaries and thus can be easily
delineated. In this paper, we propose a novel model framework for learning
automatic X-ray image parsing from labeled CT scans. Specifically, a Dense
Image-to-Image network (DI2I) for multi-organ segmentation is first trained on
X-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT
volumes. Then we introduce a Task Driven Generative Adversarial Network
(TD-GAN) architecture to achieve simultaneous style transfer and parsing for
unseen real X-ray images. TD-GAN consists of a modified cycle-GAN substructure
for pixel-to-pixel translation between DRRs and X-ray images and an added
module leveraging the pre-trained DI2I to enforce segmentation consistency. The
TD-GAN framework is general and can be easily adapted to other learning tasks.
In the numerical experiments, we validate the proposed model on 815 DRRs and
153 topograms. While the vanilla DI2I without any adaptation fails completely
on segmenting the topograms, the proposed model does not require any topogram
labels and is able to provide a promising average dice of 85% which achieves
the same level accuracy of supervised training (88%).",['cs.CV']
NeuroNet: Fast and Robust Reproduction of Multiple Brain Image Segmentation Pipelines,"NeuroNet is a deep convolutional neural network mimicking multiple popular
and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM.
The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank
Imaging Study that have been automatically segmented into brain tissue and
cortical and sub-cortical structures using the standard neuroimaging pipelines.
Training a single model from these complementary and partially overlapping
label maps yields a new powerful ""all-in-one"", multi-output segmentation tool.
The processing time for a single subject is reduced by an order of magnitude
compared to running each individual software package. We demonstrate very good
reproducibility of the original outputs while increasing robustness to
variations in the input data. We believe NeuroNet could be an important tool in
large-scale population imaging studies and serve as a new standard in
neuroscience by reducing the risk of introducing bias when choosing a specific
software package.","['cs.CV', 'cs.LG']"
Retinal Optic Disc Segmentation using Conditional Generative Adversarial Network,"This paper proposed a retinal image segmentation method based on conditional
Generative Adversarial Network (cGAN) to segment optic disc. The proposed model
consists of two successive networks: generator and discriminator. The generator
learns to map information from the observing input (i.e., retinal fundus color
image), to the output (i.e., binary mask). Then, the discriminator learns as a
loss function to train this mapping by comparing the ground-truth and the
predicted output with observing the input image as a condition.Experiments were
performed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. The
proposed model outperformed state-of-the-art-methods by achieving around 0.96%
and 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an image
segmentation is performed in less than a second on recent GPU.",['cs.CV']
Language-Based Image Editing with Recurrent Attentive Models,"We investigate the problem of Language-Based Image Editing (LBIE). Given a
source image and a natural language description, we want to generate a target
image by editing the source image based on the description. We propose a
generic modeling framework for two sub-tasks of LBIE: language-based image
segmentation and image colorization. The framework uses recurrent attentive
models to fuse image and language features. Instead of using a fixed step size,
we introduce for each region of the image a termination gate to dynamically
determine after each inference step whether to continue extrapolating
additional information from the textual description. The effectiveness of the
framework is validated on three datasets. First, we introduce a synthetic
dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE
system. Second, we show that the framework leads to state-of-the-art
performance on image segmentation on the ReferIt dataset. Third, we present the
first language-based colorization result on the Oxford-102 Flowers dataset.","['cs.CV', 'cs.CL', 'cs.LG']"
Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks,"Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in
image segmentation for a plethora of applications. Architectural innovations
within F-CNNs have mainly focused on improving spatial encoding or network
connectivity to aid gradient flow. In this paper, we explore an alternate
direction of recalibrating the feature maps adaptively, to boost meaningful
features, while suppressing weak ones. We draw inspiration from the recently
proposed squeeze & excitation (SE) module for channel recalibration of feature
maps for image classification. Towards this end, we introduce three variants of
SE modules for image segmentation, (i) squeezing spatially and exciting
channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE)
and (iii) concurrent spatial and channel squeeze & excitation (scSE). We
effectively incorporate these SE modules within three different
state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent
improvement of performance across all architectures, while minimally effecting
model complexity. Evaluations are performed on two challenging applications:
whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset)
and organ segmentation on whole body contrast enhanced CT scans (Visceral
Dataset).",['cs.CV']
Contextual Hourglass Networks for Segmentation and Density Estimation,"Hourglass networks such as the U-Net and V-Net are popular neural
architectures for medical image segmentation and counting problems. Typical
instances of hourglass networks contain shortcut connections between mirroring
layers. These shortcut connections improve the performance and it is
hypothesized that this is due to mitigating effects on the vanishing gradient
problem and the ability of the model to combine feature maps from earlier and
later layers. We propose a method for not only combining feature maps of
mirroring layers but also feature maps of layers with different spatial
dimensions. For instance, the method enables the integration of the bottleneck
feature map with those of the reconstruction layers. The proposed approach is
applicable to any hourglass architecture. We evaluated the contextual hourglass
networks on image segmentation and object counting problems in the medical
domain. We achieve competitive results outperforming popular hourglass networks
by up to 17 percentage points.",['cs.CV']
Efficient semantic image segmentation with superpixel pooling,"In this work, we evaluate the use of superpixel pooling layers in deep
network architectures for semantic segmentation. Superpixel pooling is a
flexible and efficient replacement for other pooling strategies that
incorporates spatial prior information. We propose a simple and efficient
GPU-implementation of the layer and explore several designs for the integration
of the layer into existing network architectures. We provide experimental
results on the IBSR and Cityscapes dataset, demonstrating that superpixel
pooling can be leveraged to consistently increase network accuracy with minimal
computational overhead. Source code is available at
https://github.com/bermanmaxim/superpixPool","['cs.CV', 'cs.LG']"
On the Effect of Inter-observer Variability for a Reliable Estimation of Uncertainty of Medical Image Segmentation,"Uncertainty estimation methods are expected to improve the understanding and
quality of computer-assisted methods used in medical applications (e.g.,
neurosurgical interventions, radiotherapy planning), where automated medical
image segmentation is crucial. In supervised machine learning, a common
practice to generate ground truth label data is to merge observer annotations.
However, as many medical image tasks show a high inter-observer variability
resulting from factors such as image quality, different levels of user
expertise and domain knowledge, little is known as to how inter-observer
variability and commonly used fusion methods affect the estimation of
uncertainty of automated image segmentation. In this paper we analyze the
effect of common image label fusion techniques on uncertainty estimation, and
propose to learn the uncertainty among observers. The results highlight the
negative effect of fusion methods applied in deep learning, to obtain reliable
estimates of segmentation uncertainty. Additionally, we show that the learned
observers' uncertainty can be combined with current standard Monte Carlo
dropout Bayesian neural networks to characterize uncertainty of model's
parameters.",['cs.CV']
Performance Evaluation of Deep Learning Networks for Semantic Segmentation of Traffic Stereo-Pair Images,"Semantic image segmentation is one the most demanding task, especially for
analysis of traffic conditions for self-driving cars. Here the results of
application of several deep learning architectures (PSPNet and ICNet) for
semantic image segmentation of traffic stereo-pair images are presented. The
images from Cityscapes dataset and custom urban images were analyzed as to the
segmentation accuracy and image inference time. For the models pre-trained on
Cityscapes dataset, the inference time was equal in the limits of standard
deviation, but the segmentation accuracy was different for various cities and
stereo channels even. The distributions of accuracy (mean intersection over
union - mIoU) values for each city and channel are asymmetric, long-tailed, and
have many extreme outliers, especially for PSPNet network in comparison to
ICNet network. Some statistical properties of these distributions (skewness,
kurtosis) allow us to distinguish these two networks and open the question
about relations between architecture of deep learning networks and statistical
distribution of the predicted results (mIoU here). The results obtained
demonstrated the different sensitivity of these networks to: (1) the local
street view peculiarities in different cities that should be taken into account
during the targeted fine tuning the models before their practical applications,
(2) the right and left data channels in stereo-pairs. For both networks, the
difference in the predicted results (mIoU here) for the right and left data
channels in stereo-pairs is out of the limits of statistical error in relation
to mIoU values. It means that the traffic stereo pairs can be effectively used
not only for depth calculations (as it is usually used), but also as an
additional data channel that can provide much more information about scene
objects than simple duplication of the same street view images.","['cs.CV', 'cs.LG', 'stat.ML']"
CFCM: Segmentation via Coarse to Fine Context Memory,"Recent neural-network-based architectures for image segmentation make
extensive usage of feature forwarding mechanisms to integrate information from
multiple scales. Although yielding good results, even deeper architectures and
alternative methods for feature fusion at different resolutions have been
scarcely investigated for medical applications. In this work we propose to
implement segmentation via an encoder-decoder architecture which differs from
any other previously published method since (i) it employs a very deep
architecture based on residual learning and (ii) combines features via a
convolutional Long Short Term Memory (LSTM), instead of concatenation or
summation. The intuition is that the memory mechanism implemented by LSTMs can
better integrate features from different scales through a coarse-to-fine
strategy; hence the name Coarse-to-Fine Context Memory (CFCM). We demonstrate
the remarkable advantages of this approach on two datasets: the Montgomery
county lung segmentation dataset, and the EndoVis 2015 challenge dataset for
surgical instrument segmentation.",['cs.CV']
BoxNet: Deep Learning Based Biomedical Image Segmentation Using Boxes Only Annotation,"In recent years, deep learning (DL) methods have become powerful tools for
biomedical image segmentation. However, high annotation efforts and costs are
commonly needed to acquire sufficient biomedical training data for DL models.
To alleviate the burden of manual annotation, in this paper, we propose a new
weakly supervised DL approach for biomedical image segmentation using boxes
only annotation. First, we develop a method to combine graph search (GS) and DL
to generate fine object masks from box annotation, in which DL uses box
annotation to compute a rough segmentation for GS and then GS is applied to
locate the optimal object boundaries. During the mask generation process, we
carefully utilize information from box annotation to filter out potential
errors, and then use the generated masks to train an accurate DL segmentation
network. Extensive experiments on gland segmentation in histology images, lymph
node segmentation in ultrasound images, and fungus segmentation in electron
microscopy images show that our approach attains superior performance over the
best known state-of-the-art weakly supervised DL method and is able to achieve
(1) nearly the same accuracy compared to fully supervised DL methods with far
less annotation effort, (2) significantly better results with similar
annotation time, and (3) robust performance in various applications.",['cs.CV']
"Learn the new, keep the old: Extending pretrained models with new anatomy and images","Deep learning has been widely accepted as a promising solution for medical
image segmentation, given a sufficiently large representative dataset of images
with corresponding annotations. With ever increasing amounts of annotated
medical datasets, it is infeasible to train a learning method always with all
data from scratch. This is also doomed to hit computational limits, e.g.,
memory or runtime feasible for training. Incremental learning can be a
potential solution, where new information (images or anatomy) is introduced
iteratively. Nevertheless, for the preservation of the collective information,
it is essential to keep some ""important"" (i.e. representative) images and
annotations from the past, while adding new information. In this paper, we
introduce a framework for applying incremental learning for segmentation and
propose novel methods for selecting representative data therein. We
comparatively evaluate our methods in different scenarios using MR images and
validate the increased learning capacity with using our methods.","['cs.CV', 'stat.ML']"
Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation,"Deep learning (DL) based semantic segmentation methods have been providing
state-of-the-art performance in the last few years. More specifically, these
techniques have been successfully applied to medical image classification,
segmentation, and detection tasks. One deep learning technique, U-Net, has
become one of the most popular for these applications. In this paper, we
propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well
as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net
models, which are named RU-Net and R2U-Net respectively. The proposed models
utilize the power of U-Net, Residual Network, as well as RCNN. There are
several advantages of these proposed architectures for segmentation tasks.
First, a residual unit helps when training deep architecture. Second, feature
accumulation with recurrent residual convolutional layers ensures better
feature representation for segmentation tasks. Third, it allows us to design
better U-Net architecture with same number of network parameters with better
performance for medical image segmentation. The proposed models are tested on
three benchmark datasets such as blood vessel segmentation in retina images,
skin cancer segmentation, and lung lesion segmentation. The experimental
results show superior performance on segmentation tasks compared to equivalent
models including U-Net and residual U-Net (ResU-Net).",['cs.CV']
Peekaboo - Where are the Objects? Structure Adjusting Superpixels,"This paper addresses the search for a fast and meaningful image segmentation
in the context of $k$-means clustering. The proposed method builds on a
widely-used local version of Lloyd's algorithm, called Simple Linear Iterative
Clustering (SLIC). We propose an algorithm which extends SLIC to dynamically
adjust the local search, adopting superpixel resolution dynamically to
structure existent in the image, and thus provides for more meaningful
superpixels in the same linear runtime as standard SLIC. The proposed method is
evaluated against state-of-the-art techniques and improved boundary adherence
and undersegmentation error are observed, whilst still remaining among the
fastest algorithms which are tested.",['cs.CV']
3D Shape Segmentation via Shape Fully Convolutional Networks,"We desgin a novel fully convolutional network architecture for shapes,
denoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are represented
as graph structures in the SFCN architecture, based on novel graph convolution
and pooling operations, which are similar to convolution and pooling operations
used on images. Meanwhile, to build our SFCN architecture in the original image
segmentation fully convolutional network (FCN) architecture, we also design and
implement a generating operation} with bridging function. This ensures that the
convolution and pooling operation we have designed can be successfully applied
in the original FCN architecture. In this paper, we also present a new shape
segmentation approach based on SFCN. Furthermore, we allow more general and
challenging input, such as mixed datasets of different categories of shapes}
which can prove the ability of our generalisation. In our approach, SFCNs are
trained triangles-to-triangles by using three low-level geometric features as
input. Finally, the feature voting-based multi-label graph cuts is adopted to
optimise the segmentation results obtained by SFCN prediction. The experiment
results show that our method can effectively learn and predict mixed shape
datasets of either similar or different characteristics, and achieve excellent
segmentation results.",['cs.CV']
Semantic Binary Segmentation using Convolutional Networks without Decoders,"In this paper, we propose an efficient architecture for semantic image
segmentation using the depth-to-space (D2S) operation. Our D2S model is
comprised of a standard CNN encoder followed by a depth-to-space reordering of
the final convolutional feature maps. Our approach eliminates the decoder
portion of traditional encoder-decoder segmentation models and reduces the
amount of computation almost by half. As a participant of the DeepGlobe Road
Extraction competition, we evaluate our models on the corresponding road
segmentation dataset. Our highly efficient D2S models exhibit comparable
performance to standard segmentation models with much lower computational cost.",['cs.CV']
Complex Relations in a Deep Structured Prediction Model for Fine Image Segmentation,"Many deep learning architectures for semantic segmentation involve a Fully
Convolutional Neural Network (FCN) followed by a Conditional Random Field (CRF)
to carry out inference over an image. These models typically involve unary
potentials based on local appearance features computed by FCNs, and binary
potentials based on the displacement between pixels. We show that while current
methods succeed in segmenting whole objects, they perform poorly in situations
involving a large number of object parts. We therefore suggest incorporating
into the inference algorithm additional higher-order potentials inspired by the
way humans identify and localize parts. We incorporate two relations that were
shown to be useful to human object identification - containment and attachment
- into the energy term of the CRF and evaluate their performance on the Pascal
VOC Parts dataset. Our experimental results show that the segmentation of fine
parts is positively affected by the addition of these two relations, and that
the segmentation of fine parts can be further influenced by complex structural
features.",['cs.CV']
Convexity Shape Prior for Level Set based Image Segmentation Method,"We propose a geometric convexity shape prior preservation method for
variational level set based image segmentation methods. Our method is built
upon the fact that the level set of a convex signed distanced function must be
convex. This property enables us to transfer a complicated geometrical
convexity prior into a simple inequality constraint on the function. An active
set based Gauss-Seidel iteration is used to handle this constrained
minimization problem to get an efficient algorithm. We apply our method to
region and edge based level set segmentation models including Chan-Vese (CV)
model with guarantee that the segmented region will be convex. Experimental
results show the effectiveness and quality of the proposed model and algorithm.",['cs.CV']
Knowledge-based Fully Convolutional Network and Its Application in Segmentation of Lung CT Images,"A variety of deep neural networks have been applied in medical image
segmentation and achieve good performance. Unlike natural images, medical
images of the same imaging modality are characterized by the same pattern,
which indicates that same normal organs or tissues locate at similar positions
in the images. Thus, in this paper we try to incorporate the prior knowledge of
medical images into the structure of neural networks such that the prior
knowledge can be utilized for accurate segmentation. Based on this idea, we
propose a novel deep network called knowledge-based fully convolutional network
(KFCN) for medical image segmentation. The segmentation function and
corresponding error is analyzed. We show the existence of an asymptotically
stable region for KFCN which traditional FCN doesn't possess. Experiments
validate our knowledge assumption about the incorporation of prior knowledge
into the convolution kernels of KFCN and show that KFCN can achieve a
reasonable segmentation and a satisfactory accuracy.",['cs.CV']
Quickshift++: Provably Good Initializations for Sample-Based Mean Shift,"We provide initial seedings to the Quick Shift clustering algorithm, which
approximate the locally high-density regions of the data. Such seedings act as
more stable and expressive cluster-cores than the singleton modes found by
Quick Shift. We establish statistical consistency guarantees for this
modification. We then show strong clustering performance on real datasets as
well as promising applications to image segmentation.","['cs.LG', 'stat.ML']"
Consensus Based Medical Image Segmentation Using Semi-Supervised Learning And Graph Cuts,"Medical image segmentation requires consensus ground truth segmentations to
be derived from multiple expert annotations. A novel approach is proposed that
obtains consensus segmentations from experts using graph cuts (GC) and semi
supervised learning (SSL). Popular approaches use iterative Expectation
Maximization (EM) to estimate the final annotation and quantify annotator's
performance. Such techniques pose the risk of getting trapped in local minima.
We propose a self consistency (SC) score to quantify annotator consistency
using low level image features. SSL is used to predict missing annotations by
considering global features and local image consistency. The SC score also
serves as the penalty cost in a second order Markov random field (MRF) cost
function optimized using graph cuts to derive the final consensus label. Graph
cut obtains a global maximum without an iterative procedure. Experimental
results on synthetic images, real data of Crohn's disease patients and retinal
images show our final segmentation to be accurate and more consistent than
competing methods.",['cs.CV']
Attention U-Net: Learning Where to Look for the Pancreas,"We propose a novel attention gate (AG) model for medical imaging that
automatically learns to focus on target structures of varying shapes and sizes.
Models trained with AGs implicitly learn to suppress irrelevant regions in an
input image while highlighting salient features useful for a specific task.
This enables us to eliminate the necessity of using explicit external
tissue/organ localisation modules of cascaded convolutional neural networks
(CNNs). AGs can be easily integrated into standard CNN architectures such as
the U-Net model with minimal computational overhead while increasing the model
sensitivity and prediction accuracy. The proposed Attention U-Net architecture
is evaluated on two large CT abdominal datasets for multi-class image
segmentation. Experimental results show that AGs consistently improve the
prediction performance of U-Net across different datasets and training sizes
while preserving computational efficiency. The code for the proposed
architecture is publicly available.",['cs.CV']
Piecewise Flat Embedding for Image Segmentation,"We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat
Embedding (PFE) -- for image segmentation. Based on the theory of sparse signal
recovery, piecewise flat embedding with diverse channels attempts to recover a
piecewise constant image representation with sparse region boundaries and
sparse cluster value scattering. The resultant piecewise flat embedding
exhibits interesting properties such as suppressing slowly varying signals, and
offers an image representation with higher region identifiability which is
desirable for image segmentation or high-level semantic analysis tasks. We
formulate our embedding as a variant of the Laplacian Eigenmap embedding with
an $L_{1,p} (0<p\leq1)$ regularization term to promote sparse solutions. First,
we devise a two-stage numerical algorithm based on Bregman iterations to
compute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize
this algorithm through iterative reweighting to solve the general
$L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE
into two existing image segmentation frameworks, segmentation based on
clustering and hierarchical segmentation based on contour detection.
Experiments on four major benchmark datasets, BSDS500, MSRC, Stanford
Background Dataset, and PASCAL Context, show that segmentation algorithms
incorporating our embedding achieve significantly improved results.","['cs.CV', 'eess.IV', 'stat.ML']"
Convolutional CRFs for Semantic Segmentation,"For the challenging semantic image segmentation task the most efficient
models have traditionally combined the structured modelling capabilities of
Conditional Random Fields (CRFs) with the feature extraction power of CNNs. In
more recent works however, CRF post-processing has fallen out of favour. We
argue that this is mainly due to the slow training and inference speeds of
CRFs, as well as the difficulty of learning the internal CRF parameters. To
overcome both issues we propose to add the assumption of conditional
independence to the framework of fully-connected CRFs. This allows us to
reformulate the inference in terms of convolutions, which can be implemented
highly efficiently on GPUs. Doing so speeds up inference and training by a
factor of more then 100. All parameters of the convolutional CRFs can easily be
optimized using backpropagation. To facilitating further CRF research we make
our implementation publicly available. Please visit:
https://github.com/MarvinTeichmann/ConvCRF",['cs.CV']
Facade Segmentation in the Wild,"Urban facade segmentation from automatically acquired imagery, in contrast to
traditional image segmentation, poses several unique challenges. 360-degree
photospheres captured from vehicles are an effective way to capture a large
number of images, but this data presents difficult-to-model warping and
stitching artifacts. In addition, each pixel can belong to multiple facade
elements, and different facade elements (e.g., window, balcony, sill, etc.) are
correlated and vary wildly in their characteristics. In this paper, we propose
three network architectures of varying complexity to achieve multilabel
semantic segmentation of facade images while exploiting their unique
characteristics. Specifically, we propose a MULTIFACSEGNET architecture to
assign multiple labels to each pixel, a SEPARABLE architecture as a low-rank
formulation that encourages extraction of rectangular elements, and a
COMPATIBILITY network that simultaneously seeks segmentation across facade
element types allowing the network to 'see' intermediate output probabilities
of the various facade element classes. Our results on benchmark datasets show
significant improvements over existing facade segmentation approaches for the
typical facade elements. For example, on one commonly used dataset, the
accuracy scores for window(the most important architectural element) increases
from 0.91 to 0.97 percent compared to the best competing method, and comparable
improvements on other element types.",['cs.CV']
Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning,"Due to low tissue contrast, irregular object appearance, and unpredictable
location variation, segmenting the objects from different medical imaging
modalities (e.g., CT, MR) is considered as an important yet challenging task.
In this paper, we present a novel method for interactive medical image
segmentation with the following merits. (1) Our design is fundamentally
different from previous pure patch-based and image-based segmentation methods.
We observe that during delineation, the physician repeatedly check the
inside-outside intensity changing to determine the boundary, which indicates
that comparison in an inside-outside manner is extremely important. Thus, we
innovatively model our segmentation task as learning the representation of the
bi-directional sequential patches, starting from (or ending in) the given
central point of the object. This can be realized by our proposed ConvRNN
network embedded with a gated memory propagation unit. (2) Unlike previous
interactive methods (requiring bounding box or seed points), we only ask the
physician to merely click on the rough central point of the object before
segmentation, which could simultaneously enhance the performance and reduce the
segmentation time. (3) We utilize our method in a multi-level framework for
better performance. We systematically evaluate our method in three different
segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12
challenge, showing promising results compared with state-of-the-art methods.
The code is available here:
\href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.",['cs.CV']
On the iterative refinement of densely connected representation levels for semantic segmentation,"State-of-the-art semantic segmentation approaches increase the receptive
field of their models by using either a downsampling path composed of
poolings/strided convolutions or successive dilated convolutions. However, it
is not clear which operation leads to best results. In this paper, we
systematically study the differences introduced by distinct receptive field
enlargement methods and their impact on the performance of a novel
architecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a
densely connected backbone composed of residual networks. Following standard
image segmentation architectures, receptive field enlargement operations that
change the representation level are interleaved among residual networks. This
allows the model to exploit the benefits of both residual and dense
connectivity patterns, namely: gradient flow, iterative refinement of
representations, multi-scale feature combination and deep supervision. In order
to highlight the potential of our model, we test it on the challenging CamVid
urban scene understanding benchmark and make the following observations: 1)
downsampling operations outperform dilations when the model is trained from
scratch, 2) dilations are useful during the finetuning step of the model, 3)
coarser representations require less refinement steps, and 4) ResNets (by model
construction) are good regularizers, since they can reduce the model capacity
when needed. Finally, we compare our architecture to alternative methods and
report state-of-the-art result on the Camvid dataset, with at least twice fewer
parameters.",['cs.CV']
Hybrid Forests for Left Ventricle Segmentation using only the first slice label,"Machine learning models produce state-of-the-art results in many MRI images
segmentation. However, most of these models are trained on very large datasets
which come from experts manual labeling. This labeling process is very time
consuming and costs experts work. Therefore finding a way to reduce this cost
is on high demand. In this paper, we propose a segmentation method which
exploits MRI images sequential structure to nearly drop out this labeling task.
Only the first slice needs to be manually labeled to train the model which then
infers the next slice's segmentation. Inference result is another datum used to
train the model again. The updated model then infers the third slice and the
same process is carried out until the last slice. The proposed model is an
combination of two Random Forest algorithms: the classical one and a recent one
namely Mondrian Forests. We applied our method on human left ventricle
segmentation and results are very promising. This method can also be used to
generate labels.",['cs.CV']
Semi-parametric Image Synthesis,"We present a semi-parametric approach to photographic image synthesis from
semantic layouts. The approach combines the complementary strengths of
parametric and nonparametric techniques. The nonparametric component is a
memory bank of image segments constructed from a training set of images. Given
a novel semantic layout at test time, the memory bank is used to retrieve
photographic references that are provided as source material to a deep network.
The synthesis is performed by a deep network that draws on the provided
photographic material. Experiments on multiple semantic segmentation datasets
show that the presented approach yields considerably more realistic images than
recent purely parametric techniques. The results are shown in the supplementary
video at https://youtu.be/U4Q98lenGLQ","['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG']"
Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs,"This paper introduces a novel algorithm for transductive inference in
higher-order MRFs, where the unary energies are parameterized by a variable
classifier. The considered task is posed as a joint optimization problem in the
continuous classifier parameters and the discrete label variables. In contrast
to prior approaches such as convex relaxations, we propose an advantageous
decoupling of the objective function into discrete and continuous subproblems
and a novel, efficient optimization method related to ADMM. This approach
preserves integrality of the discrete label variables and guarantees global
convergence to a critical point. We demonstrate the advantages of our approach
in several experiments including video object segmentation on the DAVIS data
set and interactive image segmentation.",['cs.LG']
Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation,"Many imaging tasks require global information about all pixels in an image.
Conventional bottom-up classification networks globalize information by
decreasing resolution; features are pooled and downsampled into a single
output. But for semantic segmentation and object detection tasks, a network
must provide higher-resolution pixel-level outputs. To globalize information
while preserving resolution, many researchers propose the inclusion of
sophisticated auxiliary blocks, but these come at the cost of a considerable
increase in network size and computational cost. This paper proposes stacked
u-nets (SUNets), which iteratively combine features from different resolution
scales while maintaining resolution. SUNets leverage the information
globalization power of u-nets in a deeper network architectures that is capable
of handling the complexity of natural images. SUNets perform extremely well on
semantic segmentation tasks using a small number of parameters.","['cs.CV', 'cs.NE']"
Segmentation of Scanning Tunneling Microscopy Images Using Variational Methods and Empirical Wavelets,"In the fields of nanoscience and nanotechnology, it is important to be able
to functionalize surfaces chemically for a wide variety of applications.
Scanning tunneling microscopes (STMs) are important instruments in this area
used to measure the surface structure and chemistry with better than molecular
resolution. Self-assembly is frequently used to create monolayers that redefine
the surface chemistry in just a single-molecule-thick layer. Indeed, STM images
reveal rich information about the structure of self-assembled monolayers since
they convey chemical and physical properties of the studied material.
  In order to assist in and to enhance the analysis of STM and other images, we
propose and demonstrate an image-processing framework that produces two image
segmentations: one is based on intensities (apparent heights in STM images) and
the other is based on textural patterns. The proposed framework begins with a
cartoon+texture decomposition, which separates an image into its cartoon and
texture components. Afterward, the cartoon image is segmented by a modified
multiphase version of the local Chan-Vese model, while the texture image is
segmented by a combination of 2D empirical wavelet transform and a clustering
algorithm. Overall, our proposed framework contains several new features,
specifically in presenting a new application of cartoon+texture decomposition
and of the empirical wavelet transforms and in developing a specialized
framework to segment STM images and other data. To demonstrate the potential of
our approach, we apply it to actual STM images of cyanide monolayers on
Au\{111\} and present their corresponding segmentation results.",['cs.CV']
Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation,"America has a massive railway system. As of 2006, U.S. freight railroads have
140,490 route- miles of standard gauge, but maintaining such a huge system and
eliminating any dangers, like reduced track stability and poor drainage, caused
by railway ballast degradation require huge amount of labor. The traditional
way to quantify the degradation of ballast is to use an index called Fouling
Index (FI) through ballast sampling and sieve analysis. However, determining
the FI values in lab is very time-consuming and laborious, but with the help of
recent development in the field of computer vision, a novel method for a
potential machine-vison based ballast inspection system can be employed that
can hopefully replace the traditional mechanical method. The new machine-vision
approach analyses the images of the in-service ballasts, and then utilizes
image segmentation algorithm to get ballast segments. By comparing the segment
results and their corresponding FI values, this novel method produces a
machine-vision-based index that has the best-fit relation with FI. The
implementation details of how this algorithm works are discussed in this
report.",['cs.CV']
Outline Objects using Deep Reinforcement Learning,"Image segmentation needs both local boundary position information and global
object context information. The performance of the recent state-of-the-art
method, fully convolutional networks, reaches a bottleneck due to the neural
network limit after balancing between the two types of information
simultaneously in an end-to-end training style. To overcome this problem, we
divide the semantic image segmentation into temporal subtasks. First, we find a
possible pixel position of some object boundary; then trace the boundary at
steps within a limited length until the whole object is outlined. We present
the first deep reinforcement learning approach to semantic image segmentation,
called DeepOutline, which outperforms other algorithms in Coco detection
leaderboard in the middle and large size person category in Coco val2017
dataset. Meanwhile, it provides an insight into a divide and conquer way by
reinforcement learning on computer vision problems.","['cs.CV', 'cs.AI']"
Derivate-based Component-Trees for Multi-Channel Image Segmentation,"We introduce the concept of derivate-based component-trees for images with an
arbitrary number of channels. The approach is a natural extension of the
classical component-tree devoted to gray-scale images. The similar structure
enables the translation of many gray-level image processing techniques based on
the component-tree to hyperspectral and color images. As an example
application, we present an image segmentation approach that extracts Maximally
Stable Homogeneous Regions (MSHR). The approach very similar to MSER but can be
applied to images with an arbitrary number of channels. As opposed to MSER, our
approach implicitly segments regions with are both lighter and darker than
their background for gray-scale images and can be used in OCR applications
where MSER will fail. We introduce a local flooding-based immersion for the
derivate-based component-tree construction which is linear in the number of
pixels. In the experiments, we show that the runtime scales favorably with an
increasing number of channels and may improve algorithms which build on MSER.",['cs.CV']
A Multilinear Tongue Model Derived from Speech Related MRI Data of the Human Vocal Tract,"We present a multilinear statistical model of the human tongue that captures
anatomical and tongue pose related shape variations separately. The model is
derived from 3D magnetic resonance imaging data of 11 speakers sustaining
speech related vocal tract configurations. The extraction is performed by using
a minimally supervised method that uses as basis an image segmentation approach
and a template fitting technique. Furthermore, it uses image denoising to deal
with possibly corrupt data, palate surface information reconstruction to handle
palatal tongue contacts, and a bootstrap strategy to refine the obtained
shapes. Our evaluation concludes that limiting the degrees of freedom for the
anatomical and speech related variations to 5 and 4, respectively, produces a
model that can reliably register unknown data while avoiding overfitting
effects. Furthermore, we show that it can be used to generate a plausible
tongue animation by tracking sparse motion capture data.",['cs.CV']
Locally Adaptive Learning Loss for Semantic Image Segmentation,"We propose a novel locally adaptive learning estimator for enhancing the
inter- and intra- discriminative capabilities of Deep Neural Networks, which
can be used as improved loss layer for semantic image segmentation tasks. Most
loss layers compute pixel-wise cost between feature maps and ground truths,
ignoring spatial layouts and interactions between neighboring pixels with same
object category, and thus networks cannot be effectively sensitive to
intra-class connections. Stride by stride, our method firstly conducts adaptive
pooling filter operating over predicted feature maps, aiming to merge predicted
distributions over a small group of neighboring pixels with same category, and
then it computes cost between the merged distribution vector and their category
label. Such design can make groups of neighboring predictions from same
category involved into estimations on predicting correctness with respect to
their category, and hence train networks to be more sensitive to regional
connections between adjacent pixels based on their categories. In the
experiments on Pascal VOC 2012 segmentation datasets, the consistently improved
results show that our proposed approach achieves better segmentation masks
against previous counterparts.","['cs.CV', 'cs.LG']"
Towards integrating spatial localization in convolutional neural networks for brain image segmentation,"Semantic segmentation is an established while rapidly evolving field in
medical imaging. In this paper we focus on the segmentation of brain Magnetic
Resonance Images (MRI) into cerebral structures using convolutional neural
networks (CNN). CNNs achieve good performance by finding effective high
dimensional image features describing the patch content only. In this work, we
propose different ways to introduce spatial constraints into the network to
further reduce prediction inconsistencies.
  A patch based CNN architecture was trained, making use of multiple scales to
gather contextual information. Spatial constraints were introduced within the
CNN through a distance to landmarks feature or through the integration of a
probability atlas. We demonstrate experimentally that using spatial information
helps to reduce segmentation inconsistencies.","['cs.CV', 'stat.ML']"
Unsupervised Segmentation of 3D Medical Images Based on Clustering and Deep Representation Learning,"This paper presents a novel unsupervised segmentation method for 3D medical
images. Convolutional neural networks (CNNs) have brought significant advances
in image segmentation. However, most of the recent methods rely on supervised
learning, which requires large amounts of manually annotated data. Thus, it is
challenging for these methods to cope with the growing amount of medical
images. This paper proposes a unified approach to unsupervised deep
representation learning and clustering for segmentation. Our proposed method
consists of two phases. In the first phase, we learn deep feature
representations of training patches from a target image using joint
unsupervised learning (JULE) that alternately clusters representations
generated by a CNN and updates the CNN parameters using cluster labels as
supervisory signals. We extend JULE to 3D medical images by utilizing 3D
convolutions throughout the CNN architecture. In the second phase, we apply
k-means to the deep representations from the trained CNN and then project
cluster labels to the target image in order to obtain the fully segmented
image. We evaluated our methods on three images of lung cancer specimens
scanned with micro-computed tomography (micro-CT). The automatic segmentation
of pathological regions in micro-CT could further contribute to the
pathological examination process. Hence, we aim to automatically divide each
image into the regions of invasive carcinoma, noninvasive carcinoma, and normal
tissue. Our experiments show the potential abilities of unsupervised deep
representation learning for medical image segmentation.",['cs.CV']
Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means,"This paper presents a novel method for unsupervised segmentation of pathology
images. Staging of lung cancer is a major factor of prognosis. Measuring the
maximum dimensions of the invasive component in a pathology images is an
essential task. Therefore, image segmentation methods for visualizing the
extent of invasive and noninvasive components on pathology images could support
pathological examination. However, it is challenging for most of the recent
segmentation methods that rely on supervised learning to cope with unlabeled
pathology images. In this paper, we propose a unified approach to unsupervised
representation learning and clustering for pathology image segmentation. Our
method consists of two phases. In the first phase, we learn feature
representations of training patches from a target image using the spherical
k-means. The purpose of this phase is to obtain cluster centroids which could
be used as filters for feature extraction. In the second phase, we apply
conventional k-means to the representations extracted by the centroids and then
project cluster labels to the target images. We evaluated our methods on
pathology images of lung cancer specimen. Our experiments showed that the
proposed method outperforms traditional k-means segmentation and the
multithreshold Otsu method both quantitatively and qualitatively with an
improved normalized mutual information (NMI) score of 0.626 compared to 0.168
and 0.167, respectively. Furthermore, we found that the centroids can be
applied to the segmentation of other slices from the same sample.",['cs.CV']
Assessment of Breast Cancer Histology using Densely Connected Convolutional Networks,"Breast cancer is the most frequently diagnosed cancer and leading cause of
cancer-related death among females worldwide. In this article, we investigate
the applicability of densely connected convolutional neural networks to the
problems of histology image classification and whole slide image segmentation
in the area of computer-aided diagnoses for breast cancer. To this end, we
study various approaches for transfer learning and apply them to the data set
from the 2018 grand challenge on breast cancer histology images (BACH).",['cs.CV']
Boundary-sensitive Network for Portrait Segmentation,"Compared to the general semantic segmentation problem, portrait segmentation
has higher precision requirement on boundary area. However, this problem has
not been well studied in previous works. In this paper, we propose a
boundary-sensitive deep neural network (BSN) for portrait segmentation. BSN
introduces three novel techniques. First, an individual boundary-sensitive
kernel is proposed by dilating the contour line and assigning the boundary
pixels with multi-class labels. Second, a global boundary-sensitive kernel is
employed as a position sensitive prior to further constrain the overall shape
of the segmentation map. Third, we train a boundary-sensitive attribute
classifier jointly with the segmentation network to reinforce the network with
semantic boundary shape information. We have evaluated BSN on the current
largest public portrait segmentation dataset, i.e, the PFCN dataset, as well as
the portrait images collected from other three popular image segmentation
datasets: COCO, COCO-Stuff, and PASCAL VOC. Our method achieves the superior
quantitative and qualitative performance over state-of-the-arts on all the
datasets, especially on the boundary area.","['cs.CV', 'eess.IV']"
The Lovsz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks,"The Jaccard index, also referred to as the intersection-over-union score, is
commonly employed in the evaluation of image segmentation results given its
perceptual qualities, scale invariance - which lends appropriate relevance to
small objects, and appropriate counting of false negatives, in comparison to
per-pixel losses. We present a method for direct optimization of the mean
intersection-over-union loss in neural networks, in the context of semantic
image segmentation, based on the convex Lov\'asz extension of submodular
losses. The loss is shown to perform better with respect to the Jaccard index
measure than the traditionally used cross-entropy loss. We show quantitative
and qualitative differences between optimizing the Jaccard index per image
versus optimizing the Jaccard index taken over an entire dataset. We evaluate
the impact of our method in a semantic segmentation pipeline and show
substantially improved intersection-over-union segmentation scores on the
Pascal VOC and Cityscapes datasets using state-of-the-art deep learning
segmentation architectures.",['cs.CV']
Image Segmentation using Sparse Subset Selection,"In this paper, we present a new image segmentation method based on the
concept of sparse subset selection. Starting with an over-segmentation, we
adopt local spectral histogram features to encode the visual information of the
small segments into high-dimensional vectors, called superpixel features. Then,
the superpixel features are fed into a novel convex model which efficiently
leverages the features to group the superpixels into a proper number of
coherent regions. Our model automatically determines the optimal number of
coherent regions and superpixels assignment to shape final segments. To solve
our model, we propose a numerical algorithm based on the alternating direction
method of multipliers (ADMM), whose iterations consist of two highly
parallelizable sub-problems. We show each sub-problem enjoys closed-form
solution which makes the ADMM iterations computationally very efficient.
Extensive experiments on benchmark image segmentation datasets demonstrate that
our proposed method in combination with an over-segmentation can provide high
quality and competitive results compared to the existing state-of-the-art
methods.",['cs.CV']
Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound,"Deep convolutional neural networks (CNNs), especially fully convolutional
networks, have been widely applied to automatic medical image segmentation
problems, e.g., multi-organ segmentation. Existing CNN-based segmentation
methods mainly focus on looking for increasingly powerful network
architectures, but pay less attention to data sampling strategies for training
networks more effectively. In this paper, we present a simple but effective
sample selection method for training multi-organ segmentation networks. Sample
selection exhibits an exploitation-exploration strategy, i.e., exploiting hard
samples and exploring less frequently visited samples. Based on the fact that
very hard samples might have annotation errors, we propose a new sample
selection policy, named Relaxed Upper Confident Bound (RUCB). Compared with
other sample selection policies, e.g., Upper Confident Bound (UCB), it exploits
a range of hard samples rather than being stuck with a small set of very hard
ones, which mitigates the influence of annotation errors during training. We
apply this new sample selection policy to training a multi-organ segmentation
network on a dataset containing 120 abdominal CT scans and show that it boosts
segmentation performance significantly.",['cs.CV']
Application of Superpixels to Segment Several Landmarks in Running Rodents,"Examining locomotion has improved our basic understanding of motor control
and aided in treating motor impairment. Mice and rats are the model system of
choice for basic neuroscience studies of human disease. High frame rates are
needed to quantify the kinematics of running rodents, due to their high stride
frequency. Manual tracking, especially for multiple body landmarks, becomes
extremely time-consuming. To overcome these limitations, we proposed the use of
superpixels based image segmentation as superpixels utilized both spatial and
color information for segmentation. We segmented some parts of body and tested
the success of segmentation as a function of color space and SLIC segment size.
We used a simple merging function to connect the segmented regions considered
as neighbor and having the same intensity value range. In addition, 28 features
were extracted, and t-SNE was used to demonstrate how much the methods are
capable to differentiate the regions. Finally, we compared the segmented
regions to a manually outlined region. The results showed for segmentation,
using the RGB image was slightly better compared to the hue channel. For merg-
ing and classification, however, the hue representation was better as it
captures the relevant color information in a single channel.",['cs.CV']
Image Segmentation Using Subspace Representation and Sparse Decomposition,"Image foreground extraction is a classical problem in image processing and
vision, with a large range of applications. In this dissertation, we focus on
the extraction of text and graphics in mixed-content images, and design novel
approaches for various aspects of this problem.
  We first propose a sparse decomposition framework, which models the
background by a subspace containing smooth basis vectors, and foreground as a
sparse and connected component. We then formulate an optimization framework to
solve this problem, by adding suitable regularizations to the cost function to
promote the desired characteristics of each component. We present two
techniques to solve the proposed optimization problem, one based on alternating
direction method of multipliers (ADMM), and the other one based on robust
regression. Promising results are obtained for screen content image
segmentation using the proposed algorithm.
  We then propose a robust subspace learning algorithm for the representation
of the background component using training images that could contain both
background and foreground components, as well as noise. With the learnt
subspace for the background, we can further improve the segmentation results,
compared to using a fixed subspace. Lastly, we investigate a different class of
signal/image decomposition problem, where only one signal component is active
at each signal element. In this case, besides estimating each component, we
need to find their supports, which can be specified by a binary mask. We
propose a mixed-integer programming problem, that jointly estimates the two
components and their supports through an alternating optimization scheme. We
show the application of this algorithm on various problems, including image
segmentation, video motion segmentation, and also separation of text from
textured images.",['cs.CV']
A Multi-Layer Approach to Superpixel-based Higher-order Conditional Random Field for Semantic Image Segmentation,"Superpixel-based Higher-order Conditional random fields (SP-HO-CRFs) are
known for their effectiveness in enforcing both short and long spatial
contiguity for pixelwise labelling in computer vision. However, their
higher-order potentials are usually too complex to learn and often incur a high
computational cost in performing inference. We propose an new approximation
approach to SP-HO-CRFs that resolves these problems. Our approach is a
multi-layer CRF framework that inherits the simplicity from pairwise CRFs by
formulating both the higher-order and pairwise cues into the same pairwise
potentials in the first layer. Essentially, this approach provides accuracy
enhancement on the basis of pairwise CRFs without training by reusing their
pre-trained parameters and/or weights. The proposed multi-layer approach
performs especially well in delineating the boundary details (boarders) of
object categories such as ""trees"" and ""bushes"". Multiple sets of experiments
conducted on dataset MSRC-21 and PASCAL VOC 2012 validate the effectiveness and
efficiency of the proposed methods.",['cs.CV']
Compassionately Conservative Balanced Cuts for Image Segmentation,"The Normalized Cut (NCut) objective function, widely used in data clustering
and image segmentation, quantifies the cost of graph partitioning in a way that
biases clusters or segments that are balanced towards having lower values than
unbalanced partitionings. However, this bias is so strong that it avoids any
singleton partitions, even when vertices are very weakly connected to the rest
of the graph. Motivated by the B\""uhler-Hein family of balanced cut costs, we
propose the family of Compassionately Conservative Balanced (CCB) Cut costs,
which are indexed by a parameter that can be used to strike a compromise
between the desire to avoid too many singleton partitions and the notion that
all partitions should be balanced. We show that CCB-Cut minimization can be
relaxed into an orthogonally constrained $\ell_{\tau}$-minimization problem
that coincides with the problem of computing Piecewise Flat Embeddings (PFE)
for one particular index value, and we present an algorithm for solving the
relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh
quotients (IRRQ). Using images from the BSDS500 database, we show that image
segmentation based on CCB-Cut minimization provides better accuracy with
respect to ground truth and greater variability in region size than NCut-based
image segmentation.",['cs.CV']
Deep learning and its application to medical image segmentation,"One of the most common tasks in medical imaging is semantic segmentation.
Achieving this segmentation automatically has been an active area of research,
but the task has been proven very challenging due to the large variation of
anatomy across different patients. However, recent advances in deep learning
have made it possible to significantly improve the performance of image
recognition and semantic segmentation methods in the field of computer vision.
Due to the data driven approaches of hierarchical feature learning in deep
learning frameworks, these advances can be translated to medical images without
much difficulty. Several variations of deep convolutional neural networks have
been successfully applied to medical images. Especially fully convolutional
architectures have been proven efficient for segmentation of 3D medical images.
In this article, we describe how to build a 3D fully convolutional network
(FCN) that can process 3D images in order to produce automatic semantic
segmentations. The model is trained and evaluated on a clinical computed
tomography (CT) dataset and shows state-of-the-art performance in multi-organ
segmentation.",['cs.CV']
"-Net (Omega-Net): Fully Automatic, Multi-View Cardiac MR Detection, Orientation, and Segmentation with Deep Neural Networks","Pixelwise segmentation of the left ventricular (LV) myocardium and the four
cardiac chambers in 2-D steady state free precession (SSFP) cine sequences is
an essential preprocessing step for a wide range of analyses. Variability in
contrast, appearance, orientation, and placement of the heart between patients,
clinical views, scanners, and protocols makes fully automatic semantic
segmentation a notoriously difficult problem. Here, we present ${\Omega}$-Net
(Omega-Net): a novel convolutional neural network (CNN) architecture for
simultaneous localization, transformation into a canonical orientation, and
semantic segmentation. First, an initial segmentation is performed on the input
image, second, the features learned during this initial segmentation are used
to predict the parameters needed to transform the input image into a canonical
orientation, and third, a final segmentation is performed on the transformed
image. In this work, ${\Omega}$-Nets of varying depths were trained to detect
five foreground classes in any of three clinical views (short axis, SA,
four-chamber, 4C, two-chamber, 2C), without prior knowledge of the view being
segmented. The architecture was trained on a cohort of patients with
hypertrophic cardiomyopathy and healthy control subjects. Network performance
as measured by weighted foreground intersection-over-union (IoU) was
substantially improved in the best-performing ${\Omega}$- Net compared with
U-Net segmentation without localization or orientation. In addition,
{\Omega}-Net was retrained from scratch on the 2017 MICCAI ACDC dataset, and
achieves state-of-the-art results on the LV and RV bloodpools, and performed
slightly worse in segmentation of the LV myocardium. We conclude this
architecture represents a substantive advancement over prior approaches, with
implications for biomedical image segmentation more generally.",['cs.CV']
An application of cascaded 3D fully convolutional networks for medical image segmentation,"Recent advances in 3D fully convolutional networks (FCN) have made it
feasible to produce dense voxel-wise predictions of volumetric images. In this
work, we show that a multi-class 3D FCN trained on manually labeled CT scans of
several anatomical structures (ranging from the large organs to thin vessels)
can achieve competitive segmentation results, while avoiding the need for
handcrafting features or training class-specific models.
  To this end, we propose a two-stage, coarse-to-fine approach that will first
use a 3D FCN to roughly define a candidate region, which will then be used as
input to a second 3D FCN. This reduces the number of voxels the second FCN has
to classify to ~10% and allows it to focus on more detailed segmentation of the
organs and vessels.
  We utilize training and validation sets consisting of 331 clinical CT images
and test our models on a completely unseen data collection acquired at a
different hospital that includes 150 CT scans, targeting three anatomical
organs (liver, spleen, and pancreas). In challenging organs such as the
pancreas, our cascaded approach improves the mean Dice score from 68.5 to
82.2%, achieving the highest reported average score on this dataset. We compare
with a 2D FCN method on a separate dataset of 240 CT scans with 18 classes and
achieve a significantly higher performance in small organs and vessels.
Furthermore, we explore fine-tuning our models to different datasets.
  Our experiments illustrate the promise and robustness of current 3D FCN based
semantic segmentation of medical images, achieving state-of-the-art results.
Our code and trained models are available for download:
https://github.com/holgerroth/3Dunet_abdomen_cascade.",['cs.CV']
Adaptive strategy for superpixel-based region-growing image segmentation,"This work presents a region-growing image segmentation approach based on
superpixel decomposition. From an initial contour-constrained over-segmentation
of the input image, the image segmentation is achieved by iteratively merging
similar superpixels into regions. This approach raises two key issues: (1) how
to compute the similarity between superpixels in order to perform accurate
merging and (2) in which order those superpixels must be merged together. In
this perspective, we firstly introduce a robust adaptive multi-scale superpixel
similarity in which region comparisons are made both at content and common
border level. Secondly, we propose a global merging strategy to efficiently
guide the region merging process. Such strategy uses an adpative merging
criterion to ensure that best region aggregations are given highest priorities.
This allows to reach a final segmentation into consistent regions with strong
boundary adherence. We perform experiments on the BSDS500 image dataset to
highlight to which extent our method compares favorably against other
well-known image segmentation algorithms. The obtained results demonstrate the
promising potential of the proposed approach.",['cs.CV']
Recurrent Segmentation for Variable Computational Budgets,"State-of-the-art systems for semantic image segmentation use feed-forward
pipelines with fixed computational costs. Building an image segmentation system
that works across a range of computational budgets is challenging and
time-intensive as new architectures must be designed and trained for every
computational setting. To address this problem we develop a recurrent neural
network that successively improves prediction quality with each iteration.
Importantly, the RNN may be deployed across a range of computational budgets by
merely running the model for a variable number of iterations. We find that this
architecture is uniquely suited for efficiently segmenting videos. By
exploiting the segmentation of past frames, the RNN can perform video
segmentation at similar quality but reduced computational cost compared to
state-of-the-art image segmentation methods. When applied to static images in
the PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out a
speed-accuracy curve that saturates near the performance of state-of-the-art
segmentation methods.",['cs.CV']
Combining Multi-level Contexts of Superpixel using Convolutional Neural Networks to perform Natural Scene Labeling,"Modern deep learning algorithms have triggered various image segmentation
approaches. However most of them deal with pixel based segmentation. However,
superpixels provide a certain degree of contextual information while reducing
computation cost. In our approach, we have performed superpixel level semantic
segmentation considering 3 various levels as neighbours for semantic contexts.
Furthermore, we have enlisted a number of ensemble approaches like max-voting
and weighted-average. We have also used the Dempster-Shafer theory of
uncertainty to analyze confusion among various classes. Our method has proved
to be superior to a number of different modern approaches on the same dataset.",['cs.CV']
Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation,"With pervasive applications of medical imaging in health-care, biomedical
image segmentation plays a central role in quantitative analysis, clinical
diagno- sis, and medical intervention. Since manual anno- tation su ers limited
reproducibility, arduous e orts, and excessive time, automatic segmentation is
desired to process increasingly larger scale histopathological data. Recently,
deep neural networks (DNNs), par- ticularly fully convolutional networks
(FCNs), have been widely applied to biomedical image segmenta- tion, attaining
much improved performance. At the same time, quantization of DNNs has become an
ac- tive research topic, which aims to represent weights with less memory
(precision) to considerably reduce memory and computation requirements of DNNs
while maintaining acceptable accuracy. In this paper, we apply quantization
techniques to FCNs for accurate biomedical image segmentation. Unlike existing
litera- ture on quantization which primarily targets memory and computation
complexity reduction, we apply quan- tization as a method to reduce over tting
in FCNs for better accuracy. Speci cally, we focus on a state-of- the-art
segmentation framework, suggestive annotation [22], which judiciously extracts
representative annota- tion samples from the original training dataset, obtain-
ing an e ective small-sized balanced training dataset. We develop two new
quantization processes for this framework: (1) suggestive annotation with
quantiza- tion for highly representative training samples, and (2) network
training with quantization for high accuracy. Extensive experiments on the
MICCAI Gland dataset show that both quantization processes can improve the
segmentation performance, and our proposed method exceeds the current
state-of-the-art performance by up to 1%. In addition, our method has a
reduction of up to 6.4x on memory usage.",['cs.CV']
Combination of Hidden Markov Random Field and Conjugate Gradient for Brain Image Segmentation,"Image segmentation is the process of partitioning the image into significant
regions easier to analyze. Nowadays, segmentation has become a necessity in
many practical medical imaging methods as locating tumors and diseases. Hidden
Markov Random Field model is one of several techniques used in image
segmentation. It provides an elegant way to model the segmentation process.
This modeling leads to the minimization of an objective function. Conjugate
Gradient algorithm (CG) is one of the best known optimization techniques. This
paper proposes the use of the Conjugate Gradient algorithm (CG) for image
segmentation, based on the Hidden Markov Random Field. Since derivatives are
not available for this expression, finite differences are used in the CG
algorithm to approximate the first derivative. The approach is evaluated using
a number of publicly available images, where ground truth is known. The Dice
Coefficient is used as an objective criterion to measure the quality of
segmentation. The results show that the proposed CG approach compares favorably
with other variants of Hidden Markov Random Field segmentation algorithms.",['cs.CV']
"Driving Scene Perception Network: Real-time Joint Detection, Depth Estimation and Semantic Segmentation","As the demand for enabling high-level autonomous driving has increased in
recent years and visual perception is one of the critical features to enable
fully autonomous driving, in this paper, we introduce an efficient approach for
simultaneous object detection, depth estimation and pixel-level semantic
segmentation using a shared convolutional architecture. The proposed network
model, which we named Driving Scene Perception Network (DSPNet), uses
multi-level feature maps and multi-task learning to improve the accuracy and
efficiency of object detection, depth estimation and image segmentation tasks
from a single input image. Hence, the resulting network model uses less than
850 MiB of GPU memory and achieves 14.0 fps on NVIDIA GeForce GTX 1080 with a
1024x512 input image, and both precision and efficiency have been improved over
combination of single tasks.",['cs.CV']
IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions,"Image segmentation is still an open problem especially when intensities of
the interested objects are overlapped due to the presence of intensity
inhomogeneity (also known as bias field). To segment images with intensity
inhomogeneities, a bias correction embedded level set model is proposed where
Inhomogeneities are Estimated by Orthogonal Primary Functions (IEOPF). In the
proposed model, the smoothly varying bias is estimated by a linear combination
of a given set of orthogonal primary functions. An inhomogeneous intensity
clustering energy is then defined and membership functions of the clusters
described by the level set function are introduced to rewrite the energy as a
data term of the proposed model. Similar to popular level set methods, a
regularization term and an arc length term are also included to regularize and
smooth the level set function, respectively. The proposed model is then
extended to multichannel and multiphase patterns to segment colourful images
and images with multiple objects, respectively. It has been extensively tested
on both synthetic and real images that are widely used in the literature and
public BrainWeb and IBSR datasets. Experimental results and comparison with
state-of-the-art methods demonstrate that advantages of the proposed model in
terms of bias correction and segmentation accuracy.",['cs.CV']
Long-term Visual Localization using Semantically Segmented Images,"Robust cross-seasonal localization is one of the major challenges in
long-term visual navigation of autonomous vehicles. In this paper, we exploit
recent advances in semantic segmentation of images, i.e., where each pixel is
assigned a label related to the type of object it represents, to attack the
problem of long-term visual localization. We show that semantically labeled 3-D
point maps of the environment, together with semantically segmented images, can
be efficiently used for vehicle localization without the need for detailed
feature descriptors (SIFT, SURF, etc.). Thus, instead of depending on
hand-crafted feature descriptors, we rely on the training of an image
segmenter. The resulting map takes up much less storage space compared to a
traditional descriptor based map. A particle filter based semantic localization
solution is compared to one based on SIFT-features, and even with large
seasonal variations over the year we perform on par with the larger and more
descriptive SIFT-features, and are able to localize with an error below 1 m
most of the time.",['cs.CV']
Simultaneous Traffic Sign Detection and Boundary Estimation using Convolutional Neural Network,"We propose a novel traffic sign detection system that simultaneously
estimates the location and precise boundary of traffic signs using
convolutional neural network (CNN). Estimating the precise boundary of traffic
signs is important in navigation systems for intelligent vehicles where traffic
signs can be used as 3D landmarks for road environment. Previous traffic sign
detection systems, including recent methods based on CNN, only provide bounding
boxes of traffic signs as output, and thus requires additional processes such
as contour estimation or image segmentation to obtain the precise sign
boundary. In this work, the boundary estimation of traffic signs is formulated
as a 2D pose and shape class prediction problem, and this is effectively solved
by a single CNN. With the predicted 2D pose and the shape class of a target
traffic sign in an input image, we estimate the actual boundary of the target
sign by projecting the boundary of a corresponding template sign image into the
input image plane. By formulating the boundary estimation problem as a
CNN-based pose and shape prediction task, our method is end-to-end trainable,
and more robust to occlusion and small targets than other boundary estimation
methods that rely on contour estimation or image segmentation. The proposed
method with architectural optimization provides an accurate traffic sign
boundary estimation which is also efficient in compute, showing a detection
frame rate higher than 7 frames per second on low-power mobile platforms.",['cs.CV']
Left Ventricle Segmentation in Cardiac MR Images Using Fully Convolutional Network,"Medical image analysis, especially segmenting a specific organ, has an
important role in developing clinical decision support systems. In cardiac
magnetic resonance (MR) imaging, segmenting the left and right ventricles helps
physicians diagnose different heart abnormalities. There are challenges for
this task, including the intensity and shape similarity between left ventricle
and other organs, inaccurate boundaries and presence of noise in most of the
images. In this paper we propose an automated method for segmenting the left
ventricle in cardiac MR images. We first automatically extract the region of
interest, and then employ it as an input of a fully convolutional network. We
train the network accurately despite the small number of left ventricle pixels
in comparison with the whole image. Thresholding on the output map of the fully
convolutional network and selection of regions based on their roundness are
performed in our proposed post-processing phase. The Dice score of our method
reaches 87.24% by applying this algorithm on the York dataset of heart images.",['cs.CV']
Segmentation hirarchique faiblement supervise,"Image segmentation is the process of partitioning an image into a set of
meaningful regions according to some criteria. Hierarchical segmentation has
emerged as a major trend in this regard as it favors the emergence of important
regions at different scales. On the other hand, many methods allow us to have
prior information on the position of structures of interest in the images. In
this paper, we present a versatile hierarchical segmentation method that takes
into account any prior spatial information and outputs a hierarchical
segmentation that emphasizes the contours or regions of interest while
preserving the important structures in the image. An application of this method
to the weakly-supervised segmentation problem is presented.","['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']"
Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey,"Osteoarthritis (OA) is one of the major health issues among the elderly
population. MRI is the most popular technology to observe and evaluate the
progress of OA course. However, the extreme labor cost of MRI analysis makes
the process inefficient and expensive. Also, due to human error and subjective
nature, the inter- and intra-observer variability is rather high.
Computer-aided knee MRI segmentation is currently an active research field
because it can alleviate doctors and radiologists from the time consuming and
tedious job, and improve the diagnosis performance which has immense potential
for both clinic and scientific research. In the past decades, researchers have
investigated automatic/semi-automatic knee MRI segmentation methods
extensively. However, to the best of our knowledge, there is no comprehensive
survey paper in this field yet. In this survey paper, we classify the existing
methods by their principles and discuss the current research status and point
out the future research trend in-depth.",['cs.CV']
Generative ScatterNet Hybrid Deep Learning (G-SHDL) Network with Structural Priors for Semantic Image Segmentation,"This paper proposes a generative ScatterNet hybrid deep learning (G-SHDL)
network for semantic image segmentation. The proposed generative architecture
is able to train rapidly from relatively small labeled datasets using the
introduced structural priors. In addition, the number of filters in each layer
of the architecture is optimized resulting in a computationally efficient
architecture. The G-SHDL network produces state-of-the-art classification
performance against unsupervised and semi-supervised learning on two image
datasets. Advantages of the G-SHDL network over supervised methods are
demonstrated with experiments performed on training datasets of reduced size.",['cs.CV']
Image Inpainting for High-Resolution Textures using CNN Texture Synthesis,"Deep neural networks have been successfully applied to problems such as image
segmentation, image super-resolution, coloration and image inpainting. In this
work we propose the use of convolutional neural networks (CNN) for image
inpainting of large regions in high-resolution textures. Due to limited
computational resources processing high-resolution images with neural networks
is still an open problem. Existing methods separate inpainting of global
structure and the transfer of details, which leads to blurry results and loss
of global coherence in the detail transfer step. Based on advances in texture
synthesis using CNNs we propose patch-based image inpainting by a CNN that is
able to optimize for global as well as detail texture statistics. Our method is
capable of filling large inpainting regions, oftentimes exceeding the quality
of comparable methods for high-resolution images. For reference patch look-up
we propose to use the same summary statistics that are used in the inpainting
process.",['cs.CV']
"Automatic segmenting teeth in X-ray images: Trends, a novel data set, benchmarking and future perspectives","This review presents an in-depth study of the literature on segmentation
methods applied in dental imaging. Ten segmentation methods were studied and
categorized according to the type of the segmentation method (region-based,
threshold-based, cluster-based, boundary-based or watershed-based), type of
X-ray images used (intra-oral or extra-oral) and characteristics of the dataset
used to evaluate the methods in the state-of-the-art works. We found that the
literature has primarily focused on threshold-based segmentation methods (54%).
80% of the reviewed papers have used intra-oral X-ray images in their
experiments, demonstrating preference to perform segmentation on images of
already isolated parts of the teeth, rather than using extra-oral X-rays, which
show tooth structure of the mouth and bones of the face. To fill a scientific
gap in the field, a novel data set based on extra-oral X-ray images are
proposed here. A statistical comparison of the results found with the 10 image
segmentation methods over our proposed data set comprised of 1,500 images is
also carried out, providing a more comprehensive source of performance
assessment. Discussion on limitations of the methods conceived over the past
year as well as future perspectives on exploiting learning-based segmentation
methods to improve performance are also provided.",['cs.CV']
Improved Image Segmentation via Cost Minimization of Multiple Hypotheses,"Image segmentation is an important component of many image understanding
systems. It aims to group pixels in a spatially and perceptually coherent
manner. Typically, these algorithms have a collection of parameters that
control the degree of over-segmentation produced. It still remains a challenge
to properly select such parameters for human-like perceptual grouping. In this
work, we exploit the diversity of segments produced by different choices of
parameters. We scan the segmentation parameter space and generate a collection
of image segmentation hypotheses (from highly over-segmented to
under-segmented). These are fed into a cost minimization framework that
produces the final segmentation by selecting segments that: (1) better describe
the natural contours of the image, and (2) are more stable and persistent among
all the segmentation hypotheses. We compare our algorithm's performance with
state-of-the-art algorithms, showing that we can achieve improved results. We
also show that our framework is robust to the choice of segmentation kernel
that produces the initial set of hypotheses.",['cs.CV']
An Iterative Spanning Forest Framework for Superpixel Segmentation,"Superpixel segmentation has become an important research problem in image
processing. In this paper, we propose an Iterative Spanning Forest (ISF)
framework, based on sequences of Image Foresting Transforms, where one can
choose i) a seed sampling strategy, ii) a connectivity function, iii) an
adjacency relation, and iv) a seed pixel recomputation procedure to generate
improved sets of connected superpixels (supervoxels in 3D) per iteration. The
superpixels in ISF structurally correspond to spanning trees rooted at those
seeds. We present five ISF methods to illustrate different choices of its
components. These methods are compared with approaches from the
state-of-the-art in effectiveness and efficiency. The experiments involve 2D
and 3D datasets with distinct characteristics, and a high level application,
named sky image segmentation. The theoretical properties of ISF are
demonstrated in the supplementary material and the results show that some of
its methods are competitive with or superior to the best baselines in
effectiveness and efficiency.",['cs.CV']
Mix-and-Match Tuning for Self-Supervised Semantic Segmentation,"Deep convolutional networks for semantic image segmentation typically require
large-scale labeled data, e.g. ImageNet and MS COCO, for network pre-training.
To reduce annotation efforts, self-supervised semantic segmentation is recently
proposed to pre-train a network without any human-provided labels. The key of
this new form of learning is to design a proxy task (e.g. image colorization),
from which a discriminative loss can be formulated on unlabeled data. Many
proxy tasks, however, lack the critical supervision signals that could induce
discriminative representation for the target image segmentation task. Thus
self-supervision's performance is still far from that of supervised
pre-training. In this study, we overcome this limitation by incorporating a
""mix-and-match"" (M&M) tuning stage in the self-supervision pipeline. The
proposed approach is readily pluggable to many self-supervision methods and
does not use more annotated samples than the original process. Yet, it is
capable of boosting the performance of target image segmentation task to
surpass fully-supervised pre-trained counterpart. The improvement is made
possible by better harnessing the limited pixel-wise annotations in the target
dataset. Specifically, we first introduce the ""mix"" stage, which sparsely
samples and mixes patches from the target set to reflect rich and diverse local
patch statistics of target images. A ""match"" stage then forms a class-wise
connected graph, which can be used to derive a strong triplet-based
discriminative loss for fine-tuning the network. Our paradigm follows the
standard practice in existing self-supervised studies and no extra data or
label is required. With the proposed M&M approach, for the first time, a
self-supervision method can achieve comparable or even better performance
compared to its ImageNet pre-trained counterpart on both PASCAL VOC2012 dataset
and CityScapes dataset.","['cs.CV', 'cs.LG']"
Deep LOGISMOS: Deep Learning Graph-based 3D Segmentation of Pancreatic Tumors on CT scans,"This paper reports Deep LOGISMOS approach to 3D tumor segmentation by
incorporating boundary information derived from deep contextual learning to
LOGISMOS - layered optimal graph image segmentation of multiple objects and
surfaces. Accurate and reliable tumor segmentation is essential to tumor growth
analysis and treatment selection. A fully convolutional network (FCN), UNet, is
first trained using three adjacent 2D patches centered at the tumor, providing
contextual UNet segmentation and probability map for each 2D patch. The UNet
segmentation is then refined by Gaussian Mixture Model (GMM) and morphological
operations. The refined UNet segmentation is used to provide the initial shape
boundary to build a segmentation graph. The cost for each node of the graph is
determined by the UNet probability maps. Finally, a max-flow algorithm is
employed to find the globally optimal solution thus obtaining the final
segmentation. For evaluation, we applied the method to pancreatic tumor
segmentation on a dataset of 51 CT scans, among which 30 scans were used for
training and 21 for testing. With Deep LOGISMOS, DICE Similarity Coefficient
(DSC) and Relative Volume Difference (RVD) reached 83.2+-7.8% and 18.6+-17.4%
respectively, both are significantly improved (p<0.05) compared with contextual
UNet and/or LOGISMOS alone.",['cs.CV']
Self-Learning to Detect and Segment Cysts in Lung CT Images without Manual Annotation,"Image segmentation is a fundamental problem in medical image analysis. In
recent years, deep neural networks achieve impressive performances on many
medical image segmentation tasks by supervised learning on large manually
annotated data. However, expert annotations on big medical datasets are
tedious, expensive or sometimes unavailable. Weakly supervised learning could
reduce the effort for annotation but still required certain amounts of
expertise. Recently, deep learning shows a potential to produce more accurate
predictions than the original erroneous labels. Inspired by this, we introduce
a very weakly supervised learning method, for cystic lesion detection and
segmentation in lung CT images, without any manual annotation. Our method works
in a self-learning manner, where segmentation generated in previous steps
(first by unsupervised segmentation then by neural networks) is used as ground
truth for the next level of network learning. Experiments on a cystic lung
lesion dataset show that the deep learning could perform better than the
initial unsupervised annotation, and progressively improve itself after
self-learning.",['cs.CV']
TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation,"Pixel-wise image segmentation is demanding task in computer vision. Classical
U-Net architectures composed of encoders and decoders are very popular for
segmentation of medical images, satellite images etc. Typically, neural network
initialized with weights from a network pre-trained on a large data set like
ImageNet shows better performance than those trained from scratch on a small
dataset. In some practical applications, particularly in medicine and traffic
safety, the accuracy of the models is of utmost importance. In this paper, we
demonstrate how the U-Net type architecture can be improved by the use of the
pre-trained encoder. Our code and corresponding pre-trained weights are
publicly available at https://github.com/ternaus/TernausNet. We compare three
weight initialization schemes: LeCun uniform, the encoder with weights from
VGG11 and full network trained on the Carvana dataset. This network
architecture was a part of the winning solution (1st out of 735) in the Kaggle:
Carvana Image Masking Challenge.",['cs.CV']
Fully Convolutional Multi-scale Residual DenseNets for Cardiac Segmentation and Automated Cardiac Diagnosis using Ensemble of Classifiers,"Deep fully convolutional neural network (FCN) based architectures have shown
great potential in medical image segmentation. However, such architectures
usually have millions of parameters and inadequate number of training samples
leading to over-fitting and poor generalization. In this paper, we present a
novel highly parameter and memory efficient FCN based architecture for medical
image analysis. We propose a novel up-sampling path which incorporates long
skip and short-cut connections to overcome the feature map explosion in FCN
like architectures. In order to processes the input images at multiple scales
and view points simultaneously, we propose to incorporate Inception module's
parallel structures. We also propose a novel dual loss function whose weighting
scheme allows to combine advantages of cross-entropy and dice loss. We have
validated our proposed network architecture on two publicly available datasets,
namely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii)
Left Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017
challenge stands second place for segmentation and first place in automated
cardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011
challenge our approach attained 0.74 Jaccard index, which is so far the highest
published result in fully automated algorithms. From the segmentation we
extracted clinically relevant cardiac parameters and hand-crafted features
which reflected the clinical diagnostic analysis to train an ensemble system
for cardiac disease classification. Our approach combined both cardiac
segmentation and disease diagnosis into a fully automated framework which is
computational efficient and hence has the potential to be incorporated in
computer-aided diagnosis (CAD) tools for clinical application.",['cs.CV']
A Computer Vision Pipeline for Automated Determination of Cardiac Structure and Function and Detection of Disease by Two-Dimensional Echocardiography,"Automated cardiac image interpretation has the potential to transform
clinical practice in multiple ways including enabling low-cost serial
assessment of cardiac function in the primary care and rural setting. We
hypothesized that advances in computer vision could enable building a fully
automated, scalable analysis pipeline for echocardiogram (echo) interpretation.
Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN)
for view identification, image segmentation, and phasing of the cardiac cycle;
3) quantification of chamber volumes and left ventricular mass; 4) particle
tracking to compute longitudinal strain; and 5) targeted disease detection.
CNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmented
individual cardiac chambers. Cardiac structure measurements agreed with study
report values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left
ventricular diastolic volume index, 2918 studies). We computed automated
ejection fraction and longitudinal strain measurements (within 2 cohorts),
which agreed with commercial software-derived values [for ejection fraction,
MAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], and
demonstrated applicability to serial monitoring of breast cancer patients for
trastuzumab cardiotoxicity. Overall, we found that, compared to manual
measurements, automated measurements had superior performance across seven
internal consistency metrics with an average increase in the Spearman
correlation coefficient of 0.05 (p=0.02). Finally, we developed disease
detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis,
with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the
groundwork for using automated interpretation to support point-of-care handheld
cardiac ultrasound and large-scale analysis of the millions of echos archived
within healthcare systems.",['cs.CV']
A First Derivative Potts Model for Segmentation and Denoising Using ILP,"Unsupervised image segmentation and denoising are two fundamental tasks in
image processing. Usually, graph based models such as multicut are used for
segmentation and variational models are employed for denoising. Our approach
addresses both problems at the same time. We propose a novel ILP formulation of
the first derivative Potts model with the $\ell_1$ data term, where binary
variables are introduced to deal with the $\ell_0$ norm of the regularization
term. The ILP is then solved by a standard off-the-shelf MIP solver. Numerical
experiments are compared with the multicut problem.",['cs.CV']
Automatic Breast Ultrasound Image Segmentation: A Survey,"Breast cancer is one of the leading causes of cancer death among women
worldwide. In clinical routine, automatic breast ultrasound (BUS) image
segmentation is very challenging and essential for cancer diagnosis and
treatment planning. Many BUS segmentation approaches have been studied in the
last two decades, and have been proved to be effective on private datasets.
Currently, the advancement of BUS image segmentation seems to meet its
bottleneck. The improvement of the performance is increasingly challenging, and
only few new approaches were published in the last several years. It is the
time to look at the field by reviewing previous approaches comprehensively and
to investigate the future directions. In this paper, we study the basic ideas,
theories, pros and cons of the approaches, group them into categories, and
extensively review each category in depth by discussing the principles,
application issues, and advantages/disadvantages.","['cs.CV', 'cs.LG']"
A Benchmark for Breast Ultrasound Image Segmentation (BUSIS),"Breast ultrasound (BUS) image segmentation is challenging and critical for
BUS Computer-Aided Diagnosis (CAD) systems. Many BUS segmentation approaches
have been proposed in the last two decades, but the performances of most
approaches have been assessed using relatively small private datasets with
differ-ent quantitative metrics, which result in discrepancy in performance
comparison. Therefore, there is a pressing need for building a benchmark to
compare existing methods using a public dataset objectively, and to determine
the performance of the best breast tumor segmentation algorithm available today
and to investigate what segmentation strategies are valuable in clinical
practice and theoretical study. In this work, we will publish a B-mode BUS
image segmentation benchmark (BUSIS) with 562 images and compare the
performance of five state-of-the-art BUS segmentation methods quantitatively.",['cs.CV']
"A fully automated framework for lung tumour detection, segmentation and analysis","Early and correct diagnosis is a very important aspect of cancer treatment.
Detection of tumour in Computed Tomography scan is a tedious and tricky task
which requires expert knowledge and a lot of human working hours. As small
human error is present in any work he does, it is possible that a CT scan could
be misdiagnosed causing the patient to become terminal. This paper introduces a
novel fully automated framework which helps to detect and segment tumour, if
present in a lung CT scan series. It also provides useful analysis of the
detected tumour such as its approximate volume, centre location and more. The
framework provides a single click solution which analyses all CT images of a
single patient series in one go. It helps to reduce the work of manually going
through each CT slice and provides quicker and more accurate tumour diagnosis.
It makes use of customized image processing and image segmentation methods, to
detect and segment the prospective tumour region from the CT scan. It then uses
a trained ensemble classifier to correctly classify the segmented region as
being tumour or not. Tumour analysis further computed can then be used to
determine malignity of the tumour. With an accuracy of 98.14%, the implemented
framework can be used in various practical scenarios, capable of eliminating
need of any expert pathologist intervention.",['cs.CV']
Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions,"Semantic image segmentation is one of the most challenged tasks in computer
vision. In this paper, we propose a highly fused convolutional network, which
consists of three parts: feature downsampling, combined feature upsampling and
multiple predictions. We adopt a strategy of multiple steps of upsampling and
combined feature maps in pooling layers with its corresponding unpooling
layers. Then we bring out multiple pre-outputs, each pre-output is generated
from an unpooling layer by one-step upsampling. Finally, we concatenate these
pre-outputs to get the final output. As a result, our proposed network makes
highly use of the feature information by fusing and reusing feature maps. In
addition, when training our model, we add multiple soft cost functions on
pre-outputs and final outputs. In this way, we can reduce the loss reduction
when the loss is back propagated. We evaluate our model on three major
segmentation datasets: CamVid, PASCAL VOC and ADE20K. We achieve a
state-of-the-art performance on CamVid dataset, as well as considerable
improvements on PASCAL VOC dataset and ADE20K dataset",['cs.CV']
Automated image segmentation for detecting cell spreading for metastasizing assessments of cancer development,"The automated segmentation of cells in microscopic images is an open research
problem that has important implications for studies of the developmental and
cancer processes based on in vitro models. In this paper, we present the
approach for segmentation of the DIC images of cultured cells using G-neighbor
smoothing followed by Kauwahara filtering and local standard deviation approach
for boundary detection. NIH FIJI/ImageJ tools are used to create the ground
truth dataset. The results of this work indicate that detection of cell
boundaries using segmentation approach even in the case of realistic
measurement conditions is a challenging problem.",['cs.CV']
Interactive Video Object Segmentation in the Wild,"In this paper we present our system for human-in-the-loop video object
segmentation. The backbone of our system is a method for one-shot video object
segmentation. While fast, this method requires an accurate pixel-level
segmentation of one (or several) frames as input. As manually annotating such a
segmentation is impractical, we propose a deep interactive image segmentation
method, that can accurately segment objects with only a handful of clicks. On
the GrabCut dataset, our method obtains 90% IOU with just 3.8 clicks on
average, setting the new state of the art. Furthermore, as our method
iteratively refines an initial segmentation, it can effectively correct frames
where the video object segmentation fails, thus allowing users to quickly
obtain high quality results even on challenging sequences. Finally, we
investigate usage patterns and give insights in how many steps users take to
annotate frames, what kind of corrections they provide, etc., thus giving
important insights for further improving interactive video segmentation.",['cs.CV']
Integrating semi-supervised label propagation and random forests for multi-atlas based hippocampus segmentation,"A novel multi-atlas based image segmentation method is proposed by
integrating a semi-supervised label propagation method and a supervised random
forests method in a pattern recognition based label fusion framework. The
semi-supervised label propagation method takes into consideration local and
global image appearance of images to be segmented and segments the images by
propagating reliable segmentation results obtained by the supervised random
forests method. Particularly, the random forests method is used to train a
regression model based on image patches of atlas images for each voxel of the
images to be segmented. The regression model is used to obtain reliable
segmentation results to guide the label propagation for the segmentation. The
proposed method has been compared with state-of-the-art multi-atlas based image
segmentation methods for segmenting the hippocampus in MR images. The
experiment results have demonstrated that our method obtained superior
segmentation performance.",['cs.CV']
Camera-trap images segmentation using multi-layer robust principal component analysis,"The segmentation of animals from camera-trap images is a difficult task. To
illustrate, there are various challenges due to environmental conditions and
hardware limitation in these images. We proposed a multi-layer robust principal
component analysis (multi-layer RPCA) approach for background subtraction. Our
method computes sparse and low-rank images from a weighted sum of descriptors,
using color and texture features as case of study for camera-trap images
segmentation. The segmentation algorithm is composed of histogram equalization
or Gaussian filtering as pre-processing, and morphological filters with active
contour as post-processing. The parameters of our multi-layer RPCA were
optimized with an exhaustive search. The database consists of camera-trap
images from the Colombian forest taken by the Instituto de Investigaci\'on de
Recursos Biol\'ogicos Alexander von Humboldt. We analyzed the performance of
our method in inherent and therefore challenging situations of camera-trap
images. Furthermore, we compared our method with some state-of-the-art
algorithms of background subtraction, where our multi-layer RPCA outperformed
these other methods. Our multi-layer RPCA reached 76.17 and 69.97% of average
fine-grained F-measure for color and infrared sequences, respectively. To our
best knowledge, this paper is the first work proposing multi-layer RPCA and
using it for camera-trap images segmentation.",['cs.CV']
Conditional Random Field and Deep Feature Learning for Hyperspectral Image Segmentation,"Image segmentation is considered to be one of the critical tasks in
hyperspectral remote sensing image processing. Recently, convolutional neural
network (CNN) has established itself as a powerful model in segmentation and
classification by demonstrating excellent performances. The use of a graphical
model such as a conditional random field (CRF) contributes further in capturing
contextual information and thus improving the segmentation performance. In this
paper, we propose a method to segment hyperspectral images by considering both
spectral and spatial information via a combined framework consisting of CNN and
CRF. We use multiple spectral cubes to learn deep features using CNN, and then
formulate deep CRF with CNN-based unary and pairwise potential functions to
effectively extract the semantic correlations between patches consisting of
three-dimensional data cubes. Effective piecewise training is applied in order
to avoid the computationally expensive iterative CRF inference. Furthermore, we
introduce a deep deconvolution network that improves the segmentation masks. We
also introduce a new dataset and experimented our proposed method on it along
with several widely adopted benchmark datasets to evaluate the effectiveness of
our method. By comparing our results with those from several state-of-the-art
models, we show the promising potential of our method.",['cs.CV']
Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection,"We present an end-to-end trainable deep convolutional neural network (DCNN)
for semantic segmentation with built-in awareness of semantically meaningful
boundaries. Semantic segmentation is a fundamental remote sensing task, and
most state-of-the-art methods rely on DCNNs as their workhorse. A major reason
for their success is that deep networks learn to accumulate contextual
information over very large windows (receptive fields). However, this success
comes at a cost, since the associated loss of effecive spatial resolution
washes out high-frequency details and leads to blurry object boundaries. Here,
we propose to counter this effect by combining semantic segmentation with
semantically informed edge detection, thus making class-boundaries explicit in
the model, First, we construct a comparatively simple, memory-efficient model
by adding boundary detection to the Segnet encoder-decoder architecture.
Second, we also include boundary detection in FCN-type models and set up a
high-end classifier ensemble. We show that boundary detection significantly
improves semantic segmentation with CNNs. Our high-end ensemble achieves > 90%
overall accuracy on the ISPRS Vaihingen benchmark.",['cs.CV']
"Track, then Decide: Category-Agnostic Vision-based Multi-Object Tracking","The most common paradigm for vision-based multi-object tracking is
tracking-by-detection, due to the availability of reliable detectors for
several important object categories such as cars and pedestrians. However,
future mobile systems will need a capability to cope with rich human-made
environments, in which obtaining detectors for every possible object category
would be infeasible. In this paper, we propose a model-free multi-object
tracking approach that uses a category-agnostic image segmentation method to
track objects. We present an efficient segmentation mask-based tracker which
associates pixel-precise masks reported by the segmentation. Our approach can
utilize semantic information whenever it is available for classifying objects
at the track level, while retaining the capability to track generic unknown
objects in the absence of such information. We demonstrate experimentally that
our approach achieves performance comparable to state-of-the-art
tracking-by-detection methods for popular object categories such as cars and
pedestrians. Additionally, we show that the proposed method can discover and
robustly track a large variety of other objects.",['cs.CV']
Image Segmentation to Distinguish Between Overlapping Human Chromosomes,"In medicine, visualizing chromosomes is important for medical diagnostics,
drug development, and biomedical research. Unfortunately, chromosomes often
overlap and it is necessary to identify and distinguish between the overlapping
chromosomes. A segmentation solution that is fast and automated will enable
scaling of cost effective medicine and biomedical research. We apply neural
network-based image segmentation to the problem of distinguishing between
partially overlapping DNA chromosomes. A convolutional neural network is
customized for this problem. The results achieved intersection over union (IOU)
scores of 94.7% for the overlapping region and 88-94% on the non-overlapping
chromosome regions.","['cs.CV', 'cs.LG', 'q-bio.QM', 'stat.ML']"
Partial Labeled Gastric Tumor Segmentation via patch-based Reiterative Learning,"Gastric cancer is the second leading cause of cancer-related deaths
worldwide, and the major hurdle in biomedical image analysis is the
determination of the cancer extent. This assignment has high clinical relevance
and would generally require vast microscopic assessment by pathologists. Recent
advances in deep learning have produced inspiring results on biomedical image
segmentation, while its outcome is reliant on comprehensive annotation. This
requires plenty of labor costs, for the ground truth must be annotated
meticulously by pathologists. In this paper, a reiterative learning framework
was presented to train our network on partial annotated biomedical images, and
superior performance was achieved without any pre-trained or further manual
annotation. We eliminate the boundary error of patch-based model through our
overlapped region forecast algorithm. Through these advisable methods, a mean
intersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09%
on the partial labeled dataset was achieved, which made us win the 2017 China
Big Data & Artificial Intelligence Innovation and Entrepreneurship
Competitions.",['cs.CV']
Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation,"Precise 3D segmentation of infant brain tissues is an essential step towards
comprehensive volumetric studies and quantitative analysis of early brain
developement. However, computing such segmentations is very challenging,
especially for 6-month infant brain, due to the poor image quality, among other
difficulties inherent to infant brain MRI, e.g., the isointense contrast
between white and gray matter and the severe partial volume effect due to small
brain sizes. This study investigates the problem with an ensemble of semi-dense
fully convolutional neural networks (CNNs), which employs T1-weighted and
T2-weighted MR images as input. We demonstrate that the ensemble agreement is
highly correlated with the segmentation errors. Therefore, our method provides
measures that can guide local user corrections. To the best of our knowledge,
this work is the first ensemble of 3D CNNs for suggesting annotations within
images. Furthermore, inspired by the very recent success of dense networks, we
propose a novel architecture, SemiDenseNet, which connects all convolutional
layers directly to the end of the network. Our architecture allows the
efficient propagation of gradients during training, while limiting the number
of parameters, requiring one order of magnitude less parameters than popular
medical image segmentation networks such as 3D U-Net. Another contribution of
our work is the study of the impact that early or late fusions of multiple
image modalities might have on the performances of deep architectures. We
report evaluations of our method on the public data of the MICCAI iSEG-2017
Challenge on 6-month infant brain MRI segmentation, and show very competitive
results among 21 teams, ranking first or second in most metrics.",['cs.CV']
Flower Categorization using Deep Convolutional Neural Networks,"We have developed a deep learning network for classification of different
flowers. For this, we have used Visual Geometry Group's 102 category flower
dataset having 8189 images of 102 different flowers from University of Oxford.
The method is basically divided into two parts; Image segmentation and
classification. We have compared the performance of two different Convolutional
Neural Network architectures GoogLeNet and AlexNet for classification purpose.
By keeping the hyper parameters same for both architectures, we have found that
the top 1 and top 5 accuracies of GoogLeNet are 47.15% and 69.17% respectively
whereas the top 1 and top 5 accuracies of AlexNet are 43.39% and 68.68%
respectively. These results are extremely good when compared to random
classification accuracy of 0.98%. This method for classification of flowers can
be implemented in real time applications and can be used to help botanists for
their research as well as camping enthusiasts.",['cs.CV']
An Efficient Evolutionary Based Method For Image Segmentation,"The goal of this paper is to present a new efficient image segmentation
method based on evolutionary computation which is a model inspired from human
behavior. Based on this model, a four layer process for image segmentation is
proposed using the split/merge approach. In the first layer, an image is split
into numerous regions using the watershed algorithm. In the second layer, a
co-evolutionary process is applied to form centers of finals segments by
merging similar primary regions. In the third layer, a meta-heuristic process
uses two operators to connect the residual regions to their corresponding
determined centers. In the final layer, an evolutionary algorithm is used to
combine the resulted similar and neighbor regions. Different layers of the
algorithm are totally independent, therefore for certain applications a
specific layer can be changed without constraint of changing other layers. Some
properties of this algorithm like the flexibility of its method, the ability to
use different feature vectors for segmentation (grayscale, color, texture,
etc), the ability to control uniformity and the number of final segments using
free parameters and also maintaining small regions, makes it possible to apply
the algorithm to different applications. Moreover, the independence of each
region from other regions in the second layer, and the independence of centers
in the third layer, makes parallel implementation possible. As a result the
algorithm speed will increase. The presented algorithm was tested on a standard
dataset (BSDS 300) of images, and the region boundaries were compared with
different people segmentation contours. Results show the efficiency of the
algorithm and its improvement to similar methods. As an instance, in 70% of
tested images, results are better than ACT algorithm, besides in 100% of tested
images, we had better results in comparison with VSP algorithm.",['cs.CV']
Rethinking Atrous Convolution for Semantic Image Segmentation,"In this work, we revisit atrous convolution, a powerful tool to explicitly
adjust filter's field-of-view as well as control the resolution of feature
responses computed by Deep Convolutional Neural Networks, in the application of
semantic image segmentation. To handle the problem of segmenting objects at
multiple scales, we design modules which employ atrous convolution in cascade
or in parallel to capture multi-scale context by adopting multiple atrous
rates. Furthermore, we propose to augment our previously proposed Atrous
Spatial Pyramid Pooling module, which probes convolutional features at multiple
scales, with image-level features encoding global context and further boost
performance. We also elaborate on implementation details and share our
experience on training our system. The proposed `DeepLabv3' system
significantly improves over our previous DeepLab versions without DenseCRF
post-processing and attains comparable performance with other state-of-art
models on the PASCAL VOC 2012 semantic image segmentation benchmark.",['cs.CV']
An Error Detection and Correction Framework for Connectomics,"We define and study error detection and correction tasks that are useful for
3D reconstruction of neurons from electron microscopic imagery, and for image
segmentation more generally. Both tasks take as input the raw image and a
binary mask representing a candidate object. For the error detection task, the
desired output is a map of split and merge errors in the object. For the error
correction task, the desired output is the true object. We call this object
mask pruning, because the candidate object mask is assumed to be a superset of
the true object. We train multiscale 3D convolutional networks to perform both
tasks. We find that the error-detecting net can achieve high accuracy. The
accuracy of the error-correcting net is enhanced if its input object mask is
""advice"" (union of erroneous objects) from the error-detecting net.",['cs.CV']
A semi-supervised fuzzy GrowCut algorithm to segment and classify regions of interest of mammographic images,"According to the World Health Organization, breast cancer is the most common
form of cancer in women. It is the second leading cause of death among women
round the world, becoming the most fatal form of cancer. Mammographic image
segmentation is a fundamental task to support image analysis and diagnosis,
taking into account shape analysis of mammary lesions and their borders.
However, mammogram segmentation is a very hard process, once it is highly
dependent on the types of mammary tissues. In this work we present a new
semi-supervised segmentation algorithm based on the modification of the GrowCut
algorithm to perform automatic mammographic image segmentation once a region of
interest is selected by a specialist. In our proposal, we used fuzzy Gaussian
membership functions to modify the evolution rule of the original GrowCut
algorithm, in order to estimate the uncertainty of a pixel being object or
background. The main impact of the proposed method is the significant reduction
of expert effort in the initialization of seed points of GrowCut to perform
accurate segmentation, once it removes the need of selection of background
seeds. We also constructed an automatic point selection process based on the
simulated annealing optimization method, avoiding the need of human
intervention. The proposed approach was qualitatively compared with other
state-of-the-art segmentation techniques, considering the shape of segmented
regions. In order to validate our proposal, we built an image classifier using
a classical multilayer perceptron. We used Zernike moments to extract segmented
image features. This analysis employed 685 mammograms from IRMA breast cancer
database, using fat and fibroid tissues. Results show that the proposed
technique could achieve a classification rate of 91.28\% for fat tissues,
evidencing the feasibility of our approach.","['cs.CV', 'cs.AI', 'cs.IR', 'cs.NE', 'eess.IV']"
Revisiting Graph Construction for Fast Image Segmentation,"In this paper, we propose a simple but effective method for fast image
segmentation. We re-examine the locality-preserving character of spectral
clustering by constructing a graph over image regions with both global and
local connections. Our novel approach to build graph connections relies on two
key observations: 1) local region pairs that co-occur frequently will have a
high probability to reside on a common object; 2) spatially distant regions in
a common object often exhibit similar visual saliency, which implies their
neighborship in a manifold. We present a novel energy function to efficiently
conduct graph partitioning. Based on multiple high quality partitions, we show
that the generated eigenvector histogram based representation can automatically
drive effective unary potentials for a hierarchical random field model to
produce multi-class segmentation. Sufficient experiments, on the BSDS500
benchmark, large-scale PASCAL VOC and COCO datasets, demonstrate the
competitive segmentation accuracy and significantly improved efficiency of our
proposed method compared with other state of the arts.",['cs.CV']
Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks,"Spleen volume estimation using automated image segmentation technique may be
used to detect splenomegaly (abnormally enlarged spleen) on Magnetic Resonance
Imaging (MRI) scans. In recent years, Deep Convolutional Neural Networks (DCNN)
segmentation methods have demonstrated advantages for abdominal organ
segmentation. However, variations in both size and shape of the spleen on MRI
images may result in large false positive and false negative labeling when
deploying DCNN based methods. In this paper, we propose the Splenomegaly
Segmentation Network (SSNet) to address spatial variations when segmenting
extraordinarily large spleens. SSNet was designed based on the framework of
image-to-image conditional generative adversarial networks (cGAN).
Specifically, the Global Convolutional Network (GCN) was used as the generator
to reduce false negatives, while the Markovian discriminator (PatchGAN) was
used to alleviate false positives. A cohort of clinically acquired 3D MRI scans
(both T1 weighted and T2 weighted) from patients with splenomegaly were used to
train and test the networks. The experimental results demonstrated that a mean
Dice coefficient of 0.9260 and a median Dice coefficient of 0.9262 using SSNet
on independently tested MRI volumes of patients with splenomegaly.",['cs.CV']
Real-time Semantic Image Segmentation via Spatial Sparsity,"We propose an approach to semantic (image) segmentation that reduces the
computational costs by a factor of 25 with limited impact on the quality of
results. Semantic segmentation has a number of practical applications, and for
most such applications the computational costs are critical. The method follows
a typical two-column network structure, where one column accepts an input
image, while the other accepts a half-resolution version of that image. By
identifying specific regions in the full-resolution image that can be safely
ignored, as well as carefully tailoring the network structure, we can process
approximately 15 highresolution Cityscapes images (1024x2048) per second using
a single GTX 980 video card, while achieving a mean intersection-over-union
score of 72.9% on the Cityscapes test set.",['cs.CV']
Automatic Spine Segmentation using Convolutional Neural Network via Redundant Generation of Class Labels for 3D Spine Modeling,"There has been a significant increase from 2010 to 2016 in the number of
people suffering from spine problems. The automatic image segmentation of the
spine obtained from a computed tomography (CT) image is important for
diagnosing spine conditions and for performing surgery with computer-assisted
surgery systems. The spine has a complex anatomy that consists of 33 vertebrae,
23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the
spinal surgeon is faced with the challenge of needing a robust algorithm to
segment and create a model of the spine. In this study, we developed an
automatic segmentation method to segment the spine, and we compared our
segmentation results with reference segmentations obtained by experts. We
developed a fully automatic approach for spine segmentation from CT based on a
hybrid method. This method combines the convolutional neural network (CNN) and
fully convolutional network (FCN), and utilizes class redundancy as a soft
constraint to greatly improve the segmentation results. The proposed method was
found to significantly enhance the accuracy of the segmentation results and the
system processing time. Our comparison was based on 12 measurements: the Dice
coefficient (94%), Jaccard index (93%), volumetric similarity (96%),
sensitivity (97%), specificity (99%), precision (over segmentation; 8.3 and
under segmentation 2.6), accuracy (99%), Matthews correlation coefficient
(0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and
global consistency error (0.02). We experimented with CT images from 32
patients, and the experimental results demonstrated the efficiency of the
proposed method.",['cs.CV']
Clickstream analysis for crowd-based object segmentation with confidence,"With the rapidly increasing interest in machine learning based solutions for
automatic image annotation, the availability of reference annotations for
algorithm training is one of the major bottlenecks in the field. Crowdsourcing
has evolved as a valuable option for low-cost and large-scale data annotation;
however, quality control remains a major issue which needs to be addressed. To
our knowledge, we are the first to analyze the annotation process to improve
crowd-sourced image segmentation. Our method involves training a regressor to
estimate the quality of a segmentation from the annotator's clickstream data.
The quality estimation can be used to identify spam and weight individual
annotations by their (estimated) quality when merging multiple segmentations of
one image. Using a total of 29,000 crowd annotations performed on publicly
available data of different object classes, we show that (1) our method is
highly accurate in estimating the segmentation quality based on clickstream
data, (2) outperforms state-of-the-art methods for merging multiple
annotations. As the regressor does not need to be trained on the object class
that it is applied to it can be regarded as a low-cost option for quality
control and confidence analysis in the context of crowd-based image annotation.",['cs.CV']
VisDA: The Visual Domain Adaptation Challenge,"We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a
large-scale testbed for unsupervised domain adaptation across visual domains.
Unsupervised domain adaptation aims to solve the real-world problem of domain
shift, where machine learning models trained on one domain must be transferred
and adapted to a novel visual domain without additional supervision. The
VisDA2017 challenge is focused on the simulation-to-reality shift and has two
associated tasks: image classification and image segmentation. The goal in both
tracks is to first train a model on simulated, synthetic data in the source
domain and then adapt it to perform well on real image data in the unlabeled
test domain. Our dataset is the largest one to date for cross-domain object
classification, with over 280K images across 12 categories in the combined
training, validation and testing domains. The image segmentation dataset is
also large-scale with over 30K images across 18 categories in the three
domains. We compare VisDA to existing cross-domain adaptation datasets and
provide a baseline performance analysis using various domain adaptation models
that are currently popular in the field.",['cs.CV']
Cost-Effective Active Learning for Melanoma Segmentation,"We propose a novel Active Learning framework capable to train effectively a
convolutional neural network for semantic segmentation of medical imaging, with
a limited amount of training labeled data. Our contribution is a practical
Cost-Effective Active Learning approach using dropout at test time as Monte
Carlo sampling to model the pixel-wise uncertainty and to analyze the image
information to improve the training performance. The source code of this
project is available at
https://marc-gorriz.github.io/CEAL-Medical-Image-Segmentation/ .",['cs.CV']
Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method,"This paper presents an efficient automatic color image segmentation method
using a seeded region growing and merging method based on square elemental
regions. Our segmentation method consists of the three steps: generating seed
regions, merging the regions, and applying a pixel-wise boundary determination
algorithm to the resultant polygonal regions. The major features of our method
are as follows: the use of square elemental regions instead of pixels as the
processing unit, a seed generation method based on enhanced gradient values, a
seed region growing method exploiting local gradient values, a region merging
method using a similarity measure including a homogeneity distance based on
Tsallis entropy, and a termination condition of region merging using an
estimated desired number of regions. Using square regions as the processing
unit substantially reduces the time complexity of the algorithm and makes the
performance stable. The experimental results show that our method exhibits
stable performance for a variety of natural images, including heavily textured
areas, and produces good segmentation results using the same parameter values.
The results of our method are fairly comparable to, and in some respects better
than, those of existing algorithms.",['cs.CV']
Efficient and Invariant Convolutional Neural Networks for Dense Prediction,"Convolutional neural networks have shown great success on feature extraction
from raw input data such as images. Although convolutional neural networks are
invariant to translations on the inputs, they are not invariant to other
transformations, including rotation and flip. Recent attempts have been made to
incorporate more invariance in image recognition applications, but they are not
applicable to dense prediction tasks, such as image segmentation. In this
paper, we propose a set of methods based on kernel rotation and flip to enable
rotation and flip invariance in convolutional neural networks. The kernel
rotation can be achieved on kernels of 3 $\times$ 3, while kernel flip can be
applied on kernels of any size. By rotating in eight or four angles, the
convolutional layers could produce the corresponding number of feature maps
based on eight or four different kernels. By using flip, the convolution layer
can produce three feature maps. By combining produced feature maps using
maxout, the resource requirement could be significantly reduced while still
retain the invariance properties. Experimental results demonstrate that the
proposed methods can achieve various invariance at reasonable resource
requirements in terms of both memory and time.",['cs.CV']
W-Net: A Deep Model for Fully Unsupervised Image Segmentation,"While significant attention has been recently focused on designing supervised
deep semantic segmentation algorithms for vision tasks, there are many domains
in which sufficient supervised pixel-level labels are difficult to obtain. In
this paper, we revisit the problem of purely unsupervised image segmentation
and propose a novel deep architecture for this problem. We borrow recent ideas
from supervised semantic segmentation methods, in particular by concatenating
two fully convolutional networks together into an autoencoder--one for encoding
and one for decoding. The encoding layer produces a k-way pixelwise prediction,
and both the reconstruction error of the autoencoder as well as the normalized
cut produced by the encoder are jointly minimized during training. When
combined with suitable postprocessing involving conditional random field
smoothing and hierarchical segmentation, our resulting algorithm achieves
impressive results on the benchmark Berkeley Segmentation Data Set,
outperforming a number of competing methods.",['cs.CV']
Curve-Structure Segmentation from Depth Maps: A CNN-based Approach and Its Application to Exploring Cultural Heritage Objects,"Motivated by the important archaeological application of exploring cultural
heritage objects, in this paper we study the challenging problem of
automatically segmenting curve structures that are very weakly stamped or
carved on an object surface in the form of a highly noisy depth map. Different
from most classical low-level image segmentation methods that are known to be
very sensitive to the noise and occlusions, we propose a new supervised
learning algorithm based on Convolutional Neural Network (CNN) to implicitly
learn and utilize more curve geometry and pattern information for addressing
this challenging problem. More specifically, we first propose a Fully
Convolutional Network (FCN) to estimate the skeleton of curve structures and at
each skeleton pixel, a scale value is estimated to reflect the local curve
width. Then we propose a dense prediction network to refine the estimated curve
skeletons. Based on the estimated scale values, we finally develop an adaptive
thresholding algorithm to achieve the final segmentation of curve structures.
In the experiment, we validate the performance of the proposed method on a
dataset of depth images scanned from unearthed pottery sherds dating to the
Woodland period of Southeastern North America.",['cs.CV']
WAYLA - Generating Images from Eye Movements,"We present a method for reconstructing images viewed by observers based only
on their eye movements. By exploring the relationships between gaze patterns
and image stimuli, the ""What Are You Looking At?"" (WAYLA) system learns to
synthesize photo-realistic images that are similar to the original pictures
being viewed. The WAYLA approach is based on the Conditional Generative
Adversarial Network (Conditional GAN) image-to-image translation technique of
Isola et al. We consider two specific applications - the first, of
reconstructing newspaper images from gaze heat maps, and the second, of
detailed reconstruction of images containing only text. The newspaper image
reconstruction process is divided into two image-to-image translation
operations, the first mapping gaze heat maps into image segmentations, and the
second mapping the generated segmentation into a newspaper image. We validate
the performance of our approach using various evaluation metrics, along with
human visual inspection. All results confirm the ability of our network to
perform image generation tasks using eye tracking data.",['cs.CV']
Robust Seed Mask Generation for Interactive Image Segmentation,"In interactive medical image segmentation, anatomical structures are
extracted from reconstructed volumetric images. The first iterations of user
interaction traditionally consist of drawing pictorial hints as an initial
estimate of the object to extract. Only after this time consuming first phase,
the efficient selective refinement of current segmentation results begins.
Erroneously labeled seeds, especially near the border of the object, are
challenging to detect and replace for a human and may substantially impact the
overall segmentation quality. We propose an automatic seeding pipeline as well
as a configuration based on saliency recognition, in order to skip the
time-consuming initial interaction phase during segmentation. A median Dice
score of 68.22% is reached before the first user interaction on the test data
set with an error rate in seeding of only 0.088%.",['cs.CV']
Fast Recurrent Fully Convolutional Networks for Direct Perception in Autonomous Driving,"Deep convolutional neural networks (CNNs) have been shown to perform
extremely well at a variety of tasks including subtasks of autonomous driving
such as image segmentation and object classification. However, networks
designed for these tasks typically require vast quantities of training data and
long training periods to converge. We investigate the design rationale behind
end-to-end driving network designs by proposing and comparing three small and
computationally inexpensive deep end-to-end neural network models that generate
driving control signals directly from input images. In contrast to prior work
that segments the autonomous driving task, our models take on a novel approach
to the autonomous driving problem by utilizing deep and thin Fully
Convolutional Nets (FCNs) with recurrent neural nets and low parameter counts
to tackle a complex end-to-end regression task predicting both steering and
acceleration commands. In addition, we include layers optimized for
classification to allow the networks to implicitly learn image semantics. We
show that the resulting networks use 3x fewer parameters than the most recent
comparable end-to-end driving network and 500x fewer parameters than the
AlexNet variations and converge both faster and to lower losses while
maintaining robustness against overfitting.",['cs.CV']
DLTK: State of the Art Reference Implementations for Deep Learning on Medical Images,"We present DLTK, a toolkit providing baseline implementations for efficient
experimentation with deep learning methods on biomedical images. It builds on
top of TensorFlow and its high modularity and easy-to-use examples allow for a
low-threshold access to state-of-the-art implementations for typical medical
imaging problems. A comparison of DLTK's reference implementations of popular
network architectures for image segmentation demonstrates new top performance
on the publicly available challenge data ""Multi-Atlas Labeling Beyond the
Cranial Vault"". The average test Dice similarity coefficient of $81.5$ exceeds
the previously best performing CNN ($75.7$) and the accuracy of the challenge
winning method ($79.0$).","['cs.CV', 'cs.LG']"
Segmenting Brain Tumors with Symmetry,"We explore encoding brain symmetry into a neural network for a brain tumor
segmentation task. A healthy human brain is symmetric at a high level of
abstraction, and the high-level asymmetric parts are more likely to be tumor
regions. Paying more attention to asymmetries has the potential to boost the
performance in brain tumor segmentation. We propose a method to encode brain
symmetry into existing neural networks and apply the method to a
state-of-the-art neural network for medical imaging segmentation. We evaluate
our symmetry-encoded network on the dataset from a brain tumor segmentation
challenge and verify that the new model extracts information in the training
images more efficiently than the original model.",['cs.CV']
3D Randomized Connection Network with Graph-based Label Inference,"In this paper, a novel 3D deep learning network is proposed for brain MR
image segmentation with randomized connection, which can decrease the
dependency between layers and increase the network capacity. The convolutional
LSTM and 3D convolution are employed as network units to capture the long-term
and short-term 3D properties respectively. To assemble these two kinds of
spatial-temporal information and refine the deep learning outcomes, we further
introduce an efficient graph-based node selection and label inference method.
Experiments have been carried out on two publicly available databases and
results demonstrate that the proposed method can obtain competitive
performances as compared with other state-of-the-art methods.",['cs.CV']
A deep learning model integrating FCNNs and CRFs for brain tumor segmentation,"Accurate and reliable brain tumor segmentation is a critical component in
cancer diagnosis, treatment planning, and treatment outcome evaluation. Build
upon successful deep learning techniques, a novel brain tumor segmentation
method is developed by integrating fully convolutional neural networks (FCNNs)
and Conditional Random Fields (CRFs) in a unified framework to obtain
segmentation results with appearance and spatial consistency. We train a deep
learning based segmentation model using 2D image patches and image slices in
following steps: 1) training FCNNs using image patches; 2) training CRFs as
Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs
fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices.
Particularly, we train 3 segmentation models using 2D image patches and slices
obtained in axial, coronal and sagittal views respectively, and combine them to
segment brain tumors using a voting based fusion strategy. Our method could
segment brain images slice-by-slice, much faster than those based on image
patches. We have evaluated our method based on imaging data provided by the
Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015
and BRATS 2016. The experimental results have demonstrated that our method
could build a segmentation model with Flair, T1c, and T2 scans and achieve
competitive performance as those built with Flair, T1, T1c, and T2 scans.",['cs.CV']
Feature Sensitive Label Fusion with Random Walker for Atlas-based Image Segmentation,"In this paper, a novel label fusion method is proposed for brain magnetic
resonance image segmentation. This label fusion method is formulated on a
graph, which embraces both label priors from atlases and anatomical priors from
target image. To represent a pixel in a comprehensive way, three kinds of
feature vectors are generated, including intensity, gradient and structural
signature. To select candidate atlas nodes for fusion, rather than exact
searching, randomized k-d tree with spatial constraint is introduced as an
efficient approximation for high-dimensional feature matching. Feature
Sensitive Label Prior (FSLP), which takes both the consistency and variety of
different features into consideration, is proposed to gather atlas priors. As
FSLP is a non-convex problem, one heuristic approach is further designed to
solve it efficiently. Moreover, based on the anatomical knowledge, parts of the
target pixels are also employed as graph seeds to assist the label fusion
process and an iterative strategy is utilized to gradually update the label
map. The comprehensive experiments carried out on two publicly available
databases give results to demonstrate that the proposed method can obtain
better segmentation quality.",['cs.CV']
Image Segmentation of Multi-Shaped Overlapping Objects,"In this work, we propose a new segmentation algorithm for images containing
convex objects present in multiple shapes with a high degree of overlap. The
proposed algorithm is carried out in two steps, first we identify the visible
contours, segment them using concave points and finally group the segments
belonging to the same object. The next step is to assign a shape identity to
these grouped contour segments. For images containing objects in multiple
shapes we begin first by identifying shape classes of the contours followed by
assigning a shape entity to these classes. We provide a comprehensive
experimentation of our algorithm on two crystal image datasets. One dataset
comprises of images containing objects in multiple shapes overlapping each
other and the other dataset contains standard images with objects present in a
single shape. We test our algorithm against two baselines, with our proposed
algorithm outperforming both the baselines.",['cs.CV']
Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes,"Image analysis using more than one modality (i.e. multi-modal) has been
increasingly applied in the field of biomedical imaging. One of the challenges
in performing the multimodal analysis is that there exist multiple schemes for
fusing the information from different modalities, where such schemes are
application-dependent and lack a unified framework to guide their designs. In
this work we firstly propose a conceptual architecture for the image fusion
schemes in supervised biomedical image analysis: fusing at the feature level,
fusing at the classifier level, and fusing at the decision-making level.
Further, motivated by the recent success in applying deep learning for natural
image analysis, we implement the three image fusion schemes above based on the
Convolutional Neural Network (CNN) with varied structures, and combined into a
single framework. The proposed image segmentation framework is capable of
analyzing the multi-modality images using different fusing schemes
simultaneously. The framework is applied to detect the presence of soft tissue
sarcoma from the combination of Magnetic Resonance Imaging (MRI), Computed
Tomography (CT) and Positron Emission Tomography (PET) images. It is found from
the results that while all the fusion schemes outperform the single-modality
schemes, fusing at the feature level can generally achieve the best performance
in terms of both accuracy and computational cost, but also suffers from the
decreased robustness in the presence of large errors in any image modalities.","['cs.CV', 'cs.LG']"
Segmentation-by-Detection: A Cascade Network for Volumetric Medical Image Segmentation,"We propose an attention mechanism for 3D medical image segmentation. The
method, named segmentation-by-detection, is a cascade of a detection module
followed by a segmentation module. The detection module enables a region of
interest to come to attention and produces a set of object region candidates
which are further used as an attention model. Rather than dealing with the
entire volume, the segmentation module distills the information from the
potential region. This scheme is an efficient solution for volumetric data as
it reduces the influence of the surrounding noise which is especially important
for medical data with low signal-to-noise ratio. Experimental results on 3D
ultrasound data of the femoral head shows superiority of the proposed method
when compared with a standard fully convolutional network like the U-Net.",['cs.CV']
The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,"State-of-the-art approaches for semantic image segmentation are built on
Convolutional Neural Networks (CNNs). The typical segmentation architecture is
composed of (a) a downsampling path responsible for extracting coarse semantic
features, followed by (b) an upsampling path trained to recover the input image
resolution at the output of the model and, optionally, (c) a post-processing
module (e.g. Conditional Random Fields) to refine the model predictions.
  Recently, a new CNN architecture, Densely Connected Convolutional Networks
(DenseNets), has shown excellent results on image classification tasks. The
idea of DenseNets is based on the observation that if each layer is directly
connected to every other layer in a feed-forward fashion then the network will
be more accurate and easier to train.
  In this paper, we extend DenseNets to deal with the problem of semantic
segmentation. We achieve state-of-the-art results on urban scene benchmark
datasets such as CamVid and Gatech, without any further post-processing module
nor pretraining. Moreover, due to smart construction of the model, our approach
has much less parameters than currently published best entries for these
datasets.
  Code to reproduce the experiments is available here :
https://github.com/SimJeg/FC-DenseNet/blob/master/train.py",['cs.CV']
A Connection between Feed-Forward Neural Networks and Probabilistic Graphical Models,"Two of the most popular modelling paradigms in computer vision are
feed-forward neural networks (FFNs) and probabilistic graphical models (GMs).
Various connections between the two have been studied in recent works, such as
e.g. expressing mean-field based inference in a GM as an FFN. This paper
establishes a new connection between FFNs and GMs. Our key observation is that
any FFN implements a certain approximation of a corresponding Bayesian network
(BN). We characterize various benefits of having this connection. In
particular, it results in a new learning algorithm for BNs. We validate the
proposed methods for a classification problem on CIFAR-10 dataset and for
binary image segmentation on Weizmann Horse dataset. We show that statistically
learned BNs improve performance, having at the same time essentially better
generalization capability, than their FFN counterparts.","['stat.ML', 'cs.CV', 'cs.LG']"
SEGMENT3D: A Web-based Application for Collaborative Segmentation of 3D images used in the Shoot Apical Meristem,"The quantitative analysis of 3D confocal microscopy images of the shoot
apical meristem helps understanding the growth process of some plants. Cell
segmentation in these images is crucial for computational plant analysis and
many automated methods have been proposed. However, variations in signal
intensity across the image mitigate the effectiveness of those approaches with
no easy way for user correction. We propose a web-based collaborative 3D image
segmentation application, SEGMENT3D, to leverage automatic segmentation
results. The image is divided into 3D tiles that can be either segmented
interactively from scratch or corrected from a pre-existing segmentation.
Individual segmentation results per tile are then automatically merged via
consensus analysis and then stitched to complete the segmentation for the
entire image stack. SEGMENT3D is a comprehensive application that can be
applied to other 3D imaging modalities and general objects. It also provides an
easy way to create supervised data to advance segmentation using machine
learning models.",['cs.CV']
Improved Workflow for Unsupervised Multiphase Image Segmentation,"Quantitative image analysis often depends on accurate classification of
pixels through a segmentation process. However, imaging artifacts such as the
partial volume effect and sensor noise complicate the classification process.
These effects increase the pixel intensity variance of each constituent class,
causing intensities from one class to overlap with another. This increased
variance makes threshold based segmentation methods insufficient due to
ambiguous overlap regions in the pixel intensity distributions. The class
ambiguity becomes even more complex for systems with more than two
constituents, such as unsaturated moist granular media. In this paper, we
propose an image processing workflow that improves segmentation accuracy for
multiphase systems. First, the ambiguous transition regions between classes are
identified and removed, which allows for global thresholding of single-class
regions. Then the transition regions are classified using a distance function,
and finally both segmentations are combined into one classified image. This
workflow includes three methodologies for identifying transition pixels and we
demonstrate on a variety of synthetic images that these approaches are able to
accurately separate the ambiguous transition pixels from the single-class
regions. For situations with typical amounts of image noise, misclassification
errors and area differences calculated between each class of the synthetic
images and the resultant segmented images range from 0.69-1.48% and 0.01-0.74%,
respectively, showing the segmentation accuracy of this approach. We
demonstrate that we are able to accurately segment x-ray microtomography images
of moist granular media using these computationally efficient methodologies.",['cs.CV']
A deep level set method for image segmentation,"This paper proposes a novel image segmentation approachthat integrates fully
convolutional networks (FCNs) with a level setmodel. Compared with a FCN, the
integrated method can incorporatesmoothing and prior information to achieve an
accurate segmentation.Furthermore, different than using the level set model as
a post-processingtool, we integrate it into the training phase to fine-tune the
FCN. Thisallows the use of unlabeled data during training in a
semi-supervisedsetting. Using two types of medical imaging data (liver CT and
left ven-tricle MRI data), we show that the integrated method achieves
goodperformance even when little training data is available, outperformingthe
FCN or the level set model alone.",['cs.CV']
Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks,"In this paper, we present a novel approach to perform deep neural networks
layer-wise weight initialization using Linear Discriminant Analysis (LDA).
Typically, the weights of a deep neural network are initialized with: random
values, greedy layer-wise pre-training (usually as Deep Belief Network or as
auto-encoder) or by re-using the layers from another network (transfer
learning). Hence, many training epochs are needed before meaningful weights are
learned, or a rather similar dataset is required for seeding a fine-tuning of
transfer learning. In this paper, we describe how to turn an LDA into either a
neural layer or a classification layer. We analyze the initialization technique
on historical documents. First, we show that an LDA-based initialization is
quick and leads to a very stable initialization. Furthermore, for the task of
layout analysis at pixel level, we investigate the effectiveness of LDA-based
initialization and show that it outperforms state-of-the-art random weight
initialization methods.",['cs.CV']
A Finite Element Computational Framework for Active Contours on Graphs,"In this paper we present a new framework for the solution of active contour
models on graphs. With the use of the Finite Element Method we generalize
active contour models on graphs and reduce the problem from a partial
differential equation to the solution of a sparse non-linear system.
Additionally, we extend the proposed framework to solve models where the curve
evolution is locally constrained around its current location. Based on the
previous extension, we propose a fast algorithm for the solution of a wide
range active contour models. Last, we present a supervised extension of
Geodesic Active Contours for image segmentation and provide experimental
evidence for the effectiveness of our framework.",['cs.CV']
Interactive Medical Image Segmentation using Deep Learning with Image-specific Fine-tuning,"Convolutional neural networks (CNNs) have achieved state-of-the-art
performance for automatic medical image segmentation. However, they have not
demonstrated sufficiently accurate and robust results for clinical use. In
addition, they are limited by the lack of image-specific adaptation and the
lack of generalizability to previously unseen object classes. To address these
problems, we propose a novel deep learning-based framework for interactive
segmentation by incorporating CNNs into a bounding box and scribble-based
segmentation pipeline. We propose image-specific fine-tuning to make a CNN
model adaptive to a specific test image, which can be either unsupervised
(without additional user interactions) or supervised (with additional
scribbles). We also propose a weighted loss function considering network and
interaction-based uncertainty for the fine-tuning. We applied this framework to
two applications: 2D segmentation of multiple organs from fetal MR slices,
where only two types of these organs were annotated for training; and 3D
segmentation of brain tumor core (excluding edema) and whole brain tumor
(including edema) from different MR sequences, where only tumor cores in one MR
sequence were annotated for training. Experimental results show that 1) our
model is more robust to segment previously unseen objects than state-of-the-art
CNNs; 2) image-specific fine-tuning with the proposed weighted loss function
significantly improves segmentation accuracy; and 3) our method leads to
accurate results with fewer user interactions and less user time than
traditional interactive segmentation methods.",['cs.CV']
An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR Image Segmentation,"Accurate segmentation of the heart is an important step towards evaluating
cardiac function. In this paper, we present a fully automated framework for
segmentation of the left (LV) and right (RV) ventricular cavities and the
myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and
3D convolutional neural network architectures for this task. We investigate the
suitability of various state-of-the art 2D and 3D convolutional neural network
architectures, as well as slight modifications thereof, for this task.
Experiments were performed on the ACDC 2017 challenge training dataset
comprising cardiac MR images of 100 patients, where manual reference
segmentations were made available for end-diastolic (ED) and end-systolic (ES)
frames. We find that processing the images in a slice-by-slice fashion using 2D
networks is beneficial due to a relatively large slice thickness. However, the
exact network architecture only plays a minor role. We report mean Dice
coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively
with an average evaluation time of 1.1 seconds per volume on a modern GPU.",['cs.CV']
Texture Fuzzy Segmentation using Skew Divergence Adaptive Affinity Functions,"Digital image segmentation is the process of assigning distinct labels to
different objects in a digital image, and the fuzzy segmentation algorithm has
been successfully used in the segmentation of images from a wide variety of
sources. However, the traditional fuzzy segmentation algorithm fails to segment
objects that are characterized by textures whose patterns cannot be
successfully described by simple statistics computed over a very restricted
area. In this paper, we propose an extension of the fuzzy segmentation
algorithm that uses adaptive textural affinity functions to perform the
segmentation of such objects on bidimensional images. The adaptive affinity
functions compute their appropriate neighborhood size as they compute the
texture descriptors surrounding the seed spels (spatial elements), according to
the characteristics of the texture being processed. The algorithm then segments
the image with an appropriate neighborhood for each object. We performed
experiments on mosaic images that were composed using images from the Brodatz
database, and compared our results with the ones produced by a recently
published texture segmentation algorithm, showing the applicability of our
method.","['cs.CV', 'cs.AI', 'cs.GR']"
FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm,"Efficient and real time segmentation of color images has a variety of
importance in many fields of computer vision such as image compression, medical
imaging, mapping and autonomous navigation. Being one of the most
computationally expensive operation, it is usually done through software imple-
mentation using high-performance processors. In robotic systems, however, with
the constrained platform dimensions and the need for portability, low power
consumption and simultaneously the need for real time image segmentation, we
envision hardware parallelism as the way forward to achieve higher
acceleration. Field-programmable gate arrays (FPGAs) are among the best suited
for this task as they provide high computing power in a small physical area.
They exceed the computing speed of software based implementations by breaking
the paradigm of sequential execution and accomplishing more per clock cycle
operations by enabling hardware level parallelization at an architectural
level. In this paper, we propose three novel architectures of a well known
Efficient Graph based Image Segmentation algorithm. These proposed
implementations optimizes time and power consumption when compared to software
implementations. The hybrid design proposed, has notable furtherance of
acceleration capabilities delivering atleast 2X speed gain over other implemen-
tations, which henceforth allows real time image segmentation that can be
deployed on Mobile Robotic systems.","['cs.CV', 'cs.DC']"
Learning Affinity via Spatial Propagation Networks,"In this paper, we propose spatial propagation networks for learning the
affinity matrix for vision tasks. We show that by constructing a row/column
linear propagation model, the spatially varying transformation matrix exactly
constitutes an affinity matrix that models dense, global pairwise relationships
of an image. Specifically, we develop a three-way connection for the linear
propagation model, which (a) formulates a sparse transformation matrix, where
all elements can be the output from a deep CNN, but (b) results in a dense
affinity matrix that effectively models any task-specific pairwise similarity
matrix. Instead of designing the similarity kernels according to image features
of two points, we can directly output all the similarities in a purely
data-driven manner. The spatial propagation network is a generic framework that
can be applied to many affinity-related tasks, including but not limited to
image matting, segmentation and colorization, to name a few. Essentially, the
model can learn semantically-aware affinity values for high-level vision tasks
due to the powerful learning capability of the deep neural network classifier.
We validate the framework on the task of refinement for image segmentation
boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic
segmentation tasks show that the spatial propagation network provides a
general, effective and efficient solution for generating high-quality
segmentation results.","['cs.CV', 'cs.LG']"
Fast Barcode Retrieval for Consensus Contouring,"Marking tumors and organs is a challenging task suffering from both inter-
and intra-observer variability. The literature quantifies observer variability
by generating consensus among multiple experts when they mark the same image.
Automatically building consensus contours to establish quality assurance for
image segmentation is presently absent in the clinical practice. As the
\emph{big data} becomes more and more available, techniques to access a large
number of existing segments of multiple experts becomes possible. Fast
algorithms are, hence, required to facilitate the search for similar cases. The
present work puts forward a potential framework that tested with small datasets
(both synthetic and real images) displays the reliability of finding similar
images. In this paper, the idea of content-based barcodes is used to retrieve
similar cases in order to build consensus contours in medical image
segmentation. This approach may be regarded as an extension of the conventional
atlas-based segmentation that generally works with rather small atlases due to
required computational expenses. The fast segment-retrieval process via
barcodes makes it possible to create and use large atlases, something that
directly contributes to the quality of the consensus building. Because the
accuracy of experts' contours must be measured, we first used 500 synthetic
prostate images with their gold markers and delineations by 20 simulated users.
The fast barcode-guided computed consensus delivered an average error of
$8\%\!\pm\!5\%$ compared against the gold standard segments. Furthermore, we
used magnetic resonance images of prostates from 15 patients delineated by 5
oncologists and selected the best delineations to serve as the gold-standard
segments. The proposed barcode atlas achieved a Jaccard overlap of
$87\%\!\pm\!9\%$ with the contours of the gold-standard segments.",['cs.CV']
Possibilistic Fuzzy Local Information C-Means for Sonar Image Segmentation,"Side-look synthetic aperture sonar (SAS) can produce very high quality images
of the sea-floor. When viewing this imagery, a human observer can often easily
identify various sea-floor textures such as sand ripple, hard-packed sand, sea
grass and rock. In this paper, we present the Possibilistic Fuzzy Local
Information C-Means (PFLICM) approach to segment SAS imagery into sea-floor
regions that exhibit these various natural textures. The proposed PFLICM method
incorporates fuzzy and possibilistic clustering methods and leverages (local)
spatial information to perform soft segmentation. Results are shown on several
SAS scenes and compared to alternative segmentation approaches.",['cs.CV']
Improving Dermoscopic Image Segmentation with Enhanced Convolutional-Deconvolutional Networks,"Automatic skin lesion segmentation on dermoscopic images is an essential step
in computer-aided diagnosis of melanoma. However, this task is challenging due
to significant variations of lesion appearances across different patients. This
challenge is further exacerbated when dealing with a large amount of image
data. In this paper, we extended our previous work by developing a deeper
network architecture with smaller kernels to enhance its discriminant capacity.
In addition, we explicitly included color information from multiple color
spaces to facilitate network training and thus to further improve the
segmentation performance. We extensively evaluated our method on the ISBI 2017
skin lesion segmentation challenge. By training with the 2000 challenge
training images, our method achieved an average Jaccard Index (JA) of 0.765 on
the 600 challenge testing images, which ranked itself in the first place in the
challenge",['cs.CV']
Neural networks for topology optimization,"In this research, we propose a deep learning based approach for speeding up
the topology optimization methods. The problem we seek to solve is the layout
problem. The main novelty of this work is to state the problem as an image
segmentation task. We leverage the power of deep learning methods as the
efficient pixel-wise image labeling technique to perform the topology
optimization. We introduce convolutional encoder-decoder architecture and the
overall approach of solving the above-described problem with high performance.
The conducted experiments demonstrate the significant acceleration of the
optimization process. The proposed approach has excellent generalization
properties. We demonstrate the ability of the application of the proposed model
to other problems. The successful results, as well as the drawbacks of the
current method, are discussed.","['cs.LG', 'math.NA']"
Learning to Label Affordances from Simulated and Real Data,"An autonomous robot should be able to evaluate the affordances that are
offered by a given situation. Here we address this problem by designing a
system that can densely predict affordances given only a single 2D RGB image.
This is achieved with a convolutional neural network (ResNet), which we combine
with refinement modules recently proposed for addressing semantic image
segmentation. We define a novel cost function, which is able to handle
(potentially multiple) affordances of objects and their parts in a pixel-wise
manner even in the case of incomplete data. We perform qualitative as well as
quantitative evaluations with simulated and real data assessing 15 different
affordances. In general, we find that affordances, which are well-enough
represented in the training data, are correctly recognized with a substantial
fraction of correctly assigned pixels. Furthermore, we show that our model
outperforms several baselines. Hence, this method can give clear action
guidelines for a robot.","['cs.CV', 'cs.RO']"
SegFlow: Joint Learning for Video Object Segmentation and Optical Flow,"This paper proposes an end-to-end trainable network, SegFlow, for
simultaneously predicting pixel-wise object segmentation and optical flow in
videos. The proposed SegFlow has two branches where useful information of
object segmentation and optical flow is propagated bidirectionally in a unified
framework. The segmentation branch is based on a fully convolutional network,
which has been proved effective in image segmentation task, and the optical
flow branch takes advantage of the FlowNet model. The unified framework is
trained iteratively offline to learn a generic notion, and fine-tuned online
for specific objects. Extensive experiments on both the video object
segmentation and optical flow datasets demonstrate that introducing optical
flow improves the performance of segmentation and vice versa, against the
state-of-the-art algorithms.",['cs.CV']
DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation,"Accurate medical image segmentation is essential for diagnosis, surgical
planning and many other applications. Convolutional Neural Networks (CNNs) have
become the state-of-the-art automatic segmentation methods. However, fully
automatic results may still need to be refined to become accurate and robust
enough for clinical use. We propose a deep learning-based interactive
segmentation method to improve the results obtained by an automatic CNN and to
reduce user interactions during refinement for higher accuracy. We use one CNN
to obtain an initial automatic segmentation, on which user interactions are
added to indicate mis-segmentations. Another CNN takes as input the user
interactions with the initial segmentation and gives a refined result. We
propose to combine user interactions with CNNs through geodesic distance
transforms, and propose a resolution-preserving network that gives a better
dense prediction. In addition, we integrate user interactions as hard
constraints into a back-propagatable Conditional Random Field. We validated the
proposed framework in the context of 2D placenta segmentation from fetal MRI
and 3D brain tumor segmentation from FLAIR images. Experimental results show
our method achieves a large improvement from automatic CNNs, and obtains
comparable and even higher accuracy with fewer user interventions and less time
compared with traditional interactive methods.",['cs.CV']
3D Densely Convolutional Networks for Volumetric Segmentation,"In the isointense stage, the accurate volumetric image segmentation is a
challenging task due to the low contrast between tissues. In this paper, we
propose a novel very deep network architecture based on a densely convolutional
network for volumetric brain segmentation. The proposed network architecture
provides a dense connection between layers that aims to improve the information
flow in the network. By concatenating features map of fine and coarse dense
blocks, it allows capturing multi-scale contextual information. Experimental
results demonstrate significant advantages of the proposed method over existing
methods, in terms of both segmentation accuracy and parameter efficiency in
MICCAI grand challenge on 6-month infant brain MRI segmentation.",['cs.CV']
UI-Net: Interactive Artificial Neural Networks for Iterative Image Segmentation Based on a User Model,"For complex segmentation tasks, fully automatic systems are inherently
limited in their achievable accuracy for extracting relevant objects.
Especially in cases where only few data sets need to be processed for a highly
accurate result, semi-automatic segmentation techniques exhibit a clear benefit
for the user. One area of application is medical image processing during an
intervention for a single patient. We propose a learning-based cooperative
segmentation approach which includes the computing entity as well as the user
into the task. Our system builds upon a state-of-the-art fully convolutional
artificial neural network (FCN) as well as an active user model for training.
During the segmentation process, a user of the trained system can iteratively
add additional hints in form of pictorial scribbles as seed points into the FCN
system to achieve an interactive and precise segmentation result. The
segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches
can yield superior results compared to networks without the user input channel
component, due to a consistent improvement in segmentation quality after each
interaction.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', '68T05, 68T45', 'I.2.6; I.4.6; I.5.5']"
One-Shot Learning for Semantic Segmentation,"Low-shot learning methods for image classification support learning from
sparse data. We extend these techniques to support dense semantic image
segmentation. Specifically, we train a network that, given a small set of
annotated images, produces parameters for a Fully Convolutional Network (FCN).
We use this FCN to perform dense pixel-level prediction on a test image for the
new semantic class. Our architecture shows a 25% relative meanIoU improvement
compared to the best baseline methods for one-shot segmentation on unseen
classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.",['cs.CV']
Exploring and Exploiting Diversity for Image Segmentation,"Semantic image segmentation is an important computer vision task that is
difficult because it consists of both recognition and segmentation. The task is
often cast as a structured output problem on an exponentially large
output-space, which is typically modeled by a discrete probabilistic model. The
best segmentation is found by inferring the Maximum a-Posteriori (MAP) solution
over the output distribution defined by the model. Due to limitations in
optimization, the model cannot be arbitrarily complex. This leads to a
trade-off: devise a more accurate model that incorporates rich high-order
interactions between image elements at the cost of inaccurate and possibly
intractable optimization OR leverage a tractable model which produces less
accurate MAP solutions but may contain high quality solutions as other modes of
its output distribution.
  This thesis investigates the latter and presents a two stage approach to
semantic segmentation. In the first stage a tractable segmentation model
outputs a set of high probability segmentations from the underlying
distribution that are not just minor perturbations of each other. Critically
the output of this stage is a diverse set of plausible solutions and not just a
single one. In the second stage, a discriminatively trained re-ranking model
selects the best segmentation from this set. The re-ranking stage can use much
more complex features than what could be tractably used in the segmentation
model, allowing a better exploration of the solution space than simply
returning the MAP solution. The formulation is agnostic to the underlying
segmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, which
makes it applicable to a wide range of models and inference methods. Evaluation
of the approach on a number of semantic image segmentation benchmark datasets
highlight its superiority over inferring the MAP solution.",['cs.CV']
